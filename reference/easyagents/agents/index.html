



<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
      
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-4.6.0">
    
    
      
        <title>Agents - easyagents</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/application.1b62728e.css">
      
        <link rel="stylesheet" href="../../../assets/stylesheets/application-palette.a8b3c06d.css">
      
      
        
        
        <meta name="theme-color" content="#4caf50">
      
    
    
      <script src="../../../assets/javascripts/modernizr.268332fc.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../../../assets/fonts/material-icons.css">
    
    
    
      
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="green" data-md-color-accent="lightgreen">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448" viewBox="0 0 416 448" id="__github"><path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#module-easyagentsagents" tabindex="1" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="../../.." title="easyagents" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            <span class="md-header-nav__topic">
              easyagents
            </span>
            <span class="md-header-nav__topic">
              
                Agents
              
            </span>
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
          
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  

<a href="https://github.com/christianhidber/easyagents/" title="Go to repository" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    easyagents
  </div>
</a>
          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
      <main class="md-main" role="main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="../../.." title="easyagents" class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </a>
    easyagents
  </label>
  
    <div class="md-nav__source">
      


  

<a href="https://github.com/christianhidber/easyagents/" title="Go to repository" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    easyagents
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href="../../.." title="Home" class="md-nav__link">
      Home
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../../CODE_OF_CONDUCT/" title="Code Of Conduct" class="md-nav__link">
      Code Of Conduct
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../../CONTRIBUTING/" title="Contributing" class="md-nav__link">
      Contributing
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../../documentation/Markdown/Release_Notes/" title="Release Notes" class="md-nav__link">
      Release Notes
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-5" type="checkbox" id="nav-5" checked>
    
    <label class="md-nav__link" for="nav-5">
      Reference
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-5">
        Reference
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-5-1" type="checkbox" id="nav-5-1" checked>
    
    <label class="md-nav__link" for="nav-5-1">
      Easyagents
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="2">
      <label class="md-nav__title" for="nav-5-1">
        Easyagents
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Agents
      </label>
    
    <a href="./" title="Agents" class="md-nav__link md-nav__link--active">
      Agents
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#variables" class="md-nav__link">
    Variables
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#functions" class="md-nav__link">
    Functions
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#activate_tensorforce" class="md-nav__link">
    activate_tensorforce
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_backends" class="md-nav__link">
    get_backends
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#load" class="md-nav__link">
    load
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#register_backend" class="md-nav__link">
    register_backend
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#classes" class="md-nav__link">
    Classes
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cemagent" class="md-nav__link">
    CemAgent
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods" class="md-nav__link">
    Methods
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#evaluate" class="md-nav__link">
    evaluate
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#play" class="md-nav__link">
    play
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#save" class="md-nav__link">
    save
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train" class="md-nav__link">
    train
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#doubledqnagent" class="md-nav__link">
    DoubleDqnAgent
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_1" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_1" class="md-nav__link">
    Methods
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#evaluate_1" class="md-nav__link">
    evaluate
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#play_1" class="md-nav__link">
    play
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#save_1" class="md-nav__link">
    save
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train_1" class="md-nav__link">
    train
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dqnagent" class="md-nav__link">
    DqnAgent
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_2" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#descendants" class="md-nav__link">
    Descendants
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_2" class="md-nav__link">
    Methods
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#evaluate_2" class="md-nav__link">
    evaluate
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#play_2" class="md-nav__link">
    play
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#save_2" class="md-nav__link">
    save
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train_2" class="md-nav__link">
    train
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#duelingdqnagent" class="md-nav__link">
    DuelingDqnAgent
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_3" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_3" class="md-nav__link">
    Methods
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#evaluate_3" class="md-nav__link">
    evaluate
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#play_3" class="md-nav__link">
    play
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#save_3" class="md-nav__link">
    save
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train_3" class="md-nav__link">
    train
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyagent" class="md-nav__link">
    EasyAgent
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_4" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#descendants_1" class="md-nav__link">
    Descendants
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_4" class="md-nav__link">
    Methods
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#evaluate_4" class="md-nav__link">
    evaluate
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#play_4" class="md-nav__link">
    play
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#save_4" class="md-nav__link">
    save
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train_4" class="md-nav__link">
    train
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ppoagent" class="md-nav__link">
    PpoAgent
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_5" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_5" class="md-nav__link">
    Methods
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#evaluate_5" class="md-nav__link">
    evaluate
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#play_5" class="md-nav__link">
    play
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#save_5" class="md-nav__link">
    save
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train_5" class="md-nav__link">
    train
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#randomagent" class="md-nav__link">
    RandomAgent
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_6" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_6" class="md-nav__link">
    Methods
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#evaluate_6" class="md-nav__link">
    evaluate
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#play_6" class="md-nav__link">
    play
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#save_6" class="md-nav__link">
    save
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train_6" class="md-nav__link">
    train
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforceagent" class="md-nav__link">
    ReinforceAgent
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_7" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_7" class="md-nav__link">
    Methods
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#evaluate_7" class="md-nav__link">
    evaluate
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#play_7" class="md-nav__link">
    play
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#save_7" class="md-nav__link">
    save
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train_7" class="md-nav__link">
    train
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sacagent" class="md-nav__link">
    SacAgent
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_8" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_8" class="md-nav__link">
    Methods
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#evaluate_8" class="md-nav__link">
    evaluate
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#play_8" class="md-nav__link">
    play
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#save_8" class="md-nav__link">
    save
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train_8" class="md-nav__link">
    train
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../core/" title="Core" class="md-nav__link">
      Core
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../env/" title="Env" class="md-nav__link">
      Env
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-5-1-4" type="checkbox" id="nav-5-1-4">
    
    <label class="md-nav__link" for="nav-5-1-4">
      Backends
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="3">
      <label class="md-nav__title" for="nav-5-1-4">
        Backends
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../backends/core/" title="Core" class="md-nav__link">
      Core
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../backends/tfagents/" title="Tfagents" class="md-nav__link">
      Tfagents
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-5-1-5" type="checkbox" id="nav-5-1-5">
    
    <label class="md-nav__link" for="nav-5-1-5">
      Callbacks
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="3">
      <label class="md-nav__title" for="nav-5-1-5">
        Callbacks
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../callbacks/duration/" title="Duration" class="md-nav__link">
      Duration
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../callbacks/log/" title="Log" class="md-nav__link">
      Log
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../callbacks/plot/" title="Plot" class="md-nav__link">
      Plot
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../callbacks/save/" title="Save" class="md-nav__link">
      Save
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
      </ul>
    </nav>
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#variables" class="md-nav__link">
    Variables
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#functions" class="md-nav__link">
    Functions
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#activate_tensorforce" class="md-nav__link">
    activate_tensorforce
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_backends" class="md-nav__link">
    get_backends
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#load" class="md-nav__link">
    load
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#register_backend" class="md-nav__link">
    register_backend
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#classes" class="md-nav__link">
    Classes
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cemagent" class="md-nav__link">
    CemAgent
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods" class="md-nav__link">
    Methods
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#evaluate" class="md-nav__link">
    evaluate
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#play" class="md-nav__link">
    play
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#save" class="md-nav__link">
    save
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train" class="md-nav__link">
    train
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#doubledqnagent" class="md-nav__link">
    DoubleDqnAgent
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_1" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_1" class="md-nav__link">
    Methods
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#evaluate_1" class="md-nav__link">
    evaluate
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#play_1" class="md-nav__link">
    play
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#save_1" class="md-nav__link">
    save
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train_1" class="md-nav__link">
    train
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dqnagent" class="md-nav__link">
    DqnAgent
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_2" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#descendants" class="md-nav__link">
    Descendants
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_2" class="md-nav__link">
    Methods
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#evaluate_2" class="md-nav__link">
    evaluate
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#play_2" class="md-nav__link">
    play
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#save_2" class="md-nav__link">
    save
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train_2" class="md-nav__link">
    train
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#duelingdqnagent" class="md-nav__link">
    DuelingDqnAgent
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_3" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_3" class="md-nav__link">
    Methods
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#evaluate_3" class="md-nav__link">
    evaluate
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#play_3" class="md-nav__link">
    play
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#save_3" class="md-nav__link">
    save
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train_3" class="md-nav__link">
    train
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyagent" class="md-nav__link">
    EasyAgent
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_4" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#descendants_1" class="md-nav__link">
    Descendants
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_4" class="md-nav__link">
    Methods
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#evaluate_4" class="md-nav__link">
    evaluate
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#play_4" class="md-nav__link">
    play
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#save_4" class="md-nav__link">
    save
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train_4" class="md-nav__link">
    train
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ppoagent" class="md-nav__link">
    PpoAgent
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_5" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_5" class="md-nav__link">
    Methods
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#evaluate_5" class="md-nav__link">
    evaluate
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#play_5" class="md-nav__link">
    play
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#save_5" class="md-nav__link">
    save
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train_5" class="md-nav__link">
    train
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#randomagent" class="md-nav__link">
    RandomAgent
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_6" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_6" class="md-nav__link">
    Methods
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#evaluate_6" class="md-nav__link">
    evaluate
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#play_6" class="md-nav__link">
    play
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#save_6" class="md-nav__link">
    save
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train_6" class="md-nav__link">
    train
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforceagent" class="md-nav__link">
    ReinforceAgent
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_7" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_7" class="md-nav__link">
    Methods
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#evaluate_7" class="md-nav__link">
    evaluate
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#play_7" class="md-nav__link">
    play
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#save_7" class="md-nav__link">
    save
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train_7" class="md-nav__link">
    train
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sacagent" class="md-nav__link">
    SacAgent
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_8" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_8" class="md-nav__link">
    Methods
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#evaluate_8" class="md-nav__link">
    evaluate
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#play_8" class="md-nav__link">
    play
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#save_8" class="md-nav__link">
    save
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train_8" class="md-nav__link">
    train
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/christianhidber/easyagents/edit/master/reference/easyagents/agents.md" title="Edit this page" class="md-icon md-content__icon">&#xE3C9;</a>
                
                
                <h1 id="module-easyagentsagents">Module easyagents.agents</h1>
<p>This module contains the public api of the EasyAgents reinforcement learning library.</p>
<p>It consist mainly of the class hierarchy of the available agents (algorithms), registrations and
the management of the available backends. In their implementation the agents forward their calls
to the chosen backend.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><span class="sd">&quot;&quot;&quot;This module contains the public api of the EasyAgents reinforcement learning library.</span>

<span class="sd">    It consist mainly of the class hierarchy of the available agents (algorithms), registrations and</span>

<span class="sd">    the management of the available backends. In their implementation the agents forward their calls</span>

<span class="sd">    to the chosen backend.</span>

<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABC</span>

<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">namedtuple</span>

<span class="kn">import</span> <span class="nn">json</span>

<span class="kn">import</span> <span class="nn">os</span>

<span class="kn">import</span> <span class="nn">statistics</span>

<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Type</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">from</span> <span class="nn">easyagents</span> <span class="kn">import</span> <span class="n">core</span>

<span class="kn">from</span> <span class="nn">easyagents.backends</span> <span class="kn">import</span> <span class="n">core</span> <span class="k">as</span> <span class="n">bcore</span>

<span class="kn">from</span> <span class="nn">easyagents.callbacks</span> <span class="kn">import</span> <span class="n">plot</span>

<span class="kn">import</span> <span class="nn">easyagents.backends.default</span>

<span class="kn">import</span> <span class="nn">easyagents.backends.tfagents</span>

<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>

<span class="c1"># import easyagents.backends.tforce</span>

<span class="n">_backends</span><span class="p">:</span> <span class="p">[</span><span class="n">bcore</span><span class="o">.</span><span class="n">BackendAgentFactory</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>

<span class="sd">&quot;&quot;&quot;The seed used for all agents and gym environments. If None no seed is set (default).&quot;&quot;&quot;</span>

<span class="n">seed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span>

<span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="n">directory</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>

         <span class="n">callbacks</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">):</span>

    <span class="sd">&quot;&quot;&quot;Loads an agent from directory.</span>

<span class="sd">    After a successful load play() may be called directly.</span>

<span class="sd">    The agent, model, backend, seed and play policy are restored according to the previously saved agent.</span>

<span class="sd">    Args:</span>

<span class="sd">        directory: the directory containing the previously saved policy.</span>

<span class="sd">        callbacks: list of callbacks called during save (eg log.Agent)</span>

<span class="sd">    Result:</span>

<span class="sd">        a new instance of EasyAgents</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">assert</span> <span class="n">directory</span>

    <span class="n">agent_json_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span> <span class="n">EasyAgent</span><span class="o">.</span><span class="n">_KEY_EASYAGENT_FILENAME</span><span class="p">)</span>

    <span class="n">policy_directory</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span> <span class="n">EasyAgent</span><span class="o">.</span><span class="n">_KEY_POLICY_DIRECTORY</span><span class="p">)</span>

    <span class="k">assert</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isdir</span><span class="p">(</span><span class="n">directory</span><span class="p">),</span> <span class="n">f</span><span class="s1">&#39;directory &quot;{directory}&quot; not found.&#39;</span>

    <span class="k">assert</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">agent_json_path</span><span class="p">),</span> <span class="s1">&#39;file &quot;{agent_json_path}&quot; not found.&#39;</span>

    <span class="k">assert</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isdir</span><span class="p">(</span><span class="n">policy_directory</span><span class="p">),</span> <span class="n">f</span><span class="s1">&#39;directory &quot;{policy_directory}&quot; not found.&#39;</span>

    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">agent_json_path</span><span class="p">)</span> <span class="k">as</span> <span class="n">jsonfile</span><span class="p">:</span>

        <span class="n">agent_dict</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">jsonfile</span><span class="p">)</span>

    <span class="n">result</span> <span class="o">=</span> <span class="n">EasyAgent</span><span class="o">.</span><span class="n">_from_dict</span><span class="p">(</span><span class="n">agent_dict</span><span class="p">)</span>

    <span class="n">callbacks</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">_to_callback_list</span><span class="p">(</span><span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span>

    <span class="n">result</span><span class="o">.</span><span class="n">_backend_agent</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">directory</span><span class="o">=</span><span class="n">policy_directory</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">result</span>

<span class="k">def</span> <span class="nf">register_backend</span><span class="p">(</span><span class="n">backend</span><span class="p">:</span> <span class="n">bcore</span><span class="o">.</span><span class="n">BackendAgentFactory</span><span class="p">):</span>

    <span class="sd">&quot;&quot;&quot;registers a backend as a factory for agent implementations.</span>

<span class="sd">    If another backend with the same name is already registered, the old backend is replaced by backend.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">assert</span> <span class="n">backend</span>

    <span class="n">old_backends</span> <span class="o">=</span> <span class="p">[</span><span class="n">b</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">_backends</span> <span class="k">if</span> <span class="n">b</span><span class="o">.</span><span class="n">backend_name</span> <span class="o">==</span> <span class="n">backend</span><span class="o">.</span><span class="n">backend_name</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">old_backend</span> <span class="ow">in</span> <span class="n">old_backends</span><span class="p">:</span>

        <span class="n">_backends</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">old_backend</span><span class="p">)</span>

    <span class="n">_backends</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">backend</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">activate_tensorforce</span><span class="p">():</span>

    <span class="sd">&quot;&quot;&quot;registers the tensorforce backend.</span>

<span class="sd">    Due to an incompatibility between tensorforce and tf-agents, both libraries may not run</span>

<span class="sd">    in the same python instance. Thus - for the time being - once this method is called,</span>

<span class="sd">    the tfagents backend may not be used anymore.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="kn">import</span> <span class="nn">easyagents.backends.tforce</span>

    <span class="k">global</span> <span class="n">_backends</span>

    <span class="k">assert</span>  <span class="n">easyagents</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">_tf_eager_execution_active</span> <span class="ow">is</span> <span class="bp">None</span> <span class="ow">or</span> \

            <span class="n">easyagents</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">_tf_eager_execution_active</span> <span class="o">==</span> <span class="bp">False</span><span class="p">,</span> \

            <span class="s2">&quot;tensorforce can not be activated, since tensorflow eager execution mode was already actived.&quot;</span>

    <span class="n">_backends</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">register_backend</span><span class="p">(</span><span class="n">easyagents</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">default</span><span class="o">.</span><span class="n">DefaultAgentFactory</span><span class="p">(</span><span class="n">register_tensorforce</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>

    <span class="n">register_backend</span><span class="p">(</span><span class="n">easyagents</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">tforce</span><span class="o">.</span><span class="n">TensorforceAgentFactory</span><span class="p">())</span>

<span class="k">def</span> <span class="nf">_activate_tfagents</span><span class="p">():</span>

    <span class="sd">&quot;&quot;&quot;registers the tfagents backend.</span>

<span class="sd">    Due to an incompatibility between tensorforce and tf-agents, both libraries may not run</span>

<span class="sd">    in the same python instance.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">global</span> <span class="n">_backends</span>

    <span class="k">assert</span>  <span class="n">easyagents</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">_tf_eager_execution_active</span> <span class="ow">is</span> <span class="bp">None</span> <span class="ow">or</span> \

            <span class="n">easyagents</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">_tf_eager_execution_active</span> <span class="o">==</span> <span class="bp">True</span><span class="p">,</span> \

            <span class="s2">&quot;tfagents can not be activated, since tensorflow eager execution mode was already disabled.&quot;</span>

    <span class="n">_backends</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">register_backend</span><span class="p">(</span><span class="n">easyagents</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">default</span><span class="o">.</span><span class="n">DefaultAgentFactory</span><span class="p">(</span><span class="n">register_tensorforce</span><span class="o">=</span><span class="bp">False</span><span class="p">))</span>

    <span class="n">register_backend</span><span class="p">(</span><span class="n">easyagents</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">tfagents</span><span class="o">.</span><span class="n">TfAgentAgentFactory</span><span class="p">())</span>

<span class="n">_activate_tfagents</span><span class="p">()</span>

<span class="k">class</span> <span class="nc">EasyAgent</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>

    <span class="sd">&quot;&quot;&quot;Abstract base class for all easy reinforcment learning agents.</span>

<span class="sd">    Besides forwarding train and play it implements persistence.&quot;&quot;&quot;</span>

    <span class="n">_KEY_BACKEND</span> <span class="o">=</span> <span class="s1">&#39;backend&#39;</span>

    <span class="n">_KEY_EASYAGENT_CLASS</span> <span class="o">=</span> <span class="s1">&#39;easyagent_class&#39;</span>

    <span class="n">_KEY_EASYAGENT_FILENAME</span> <span class="o">=</span> <span class="s1">&#39;easyagent.json&#39;</span>

    <span class="n">_KEY_MODEL_CONFIG</span> <span class="o">=</span> <span class="s1">&#39;model_config&#39;</span>

    <span class="n">_KEY_POLICY_DIRECTORY</span> <span class="o">=</span> <span class="s1">&#39;policy&#39;</span>

    <span class="n">_KEY_VERSION</span> <span class="o">=</span> <span class="s1">&#39;version&#39;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>

                 <span class="n">gym_env_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>

                 <span class="n">fc_layers</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="nb">int</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>

                 <span class="n">backend</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="bp">None</span><span class="p">):</span>

        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">            Args:</span>

<span class="sd">                gym_env_name: name of an OpenAI gym environment to be used for training and evaluation</span>

<span class="sd">                fc_layers: defines the neural network to be used, a sequence of fully connected</span>

<span class="sd">                    layers of the given size. Eg (75,40) yields a neural network consisting</span>

<span class="sd">                    out of 2 hidden layers, the first one containing 75 and the second layer</span>

<span class="sd">                    containing 40 neurons.</span>

<span class="sd">                backend=the backend to be used (eg &#39;tfagents&#39;), if None a default implementation is used.</span>

<span class="sd">                    call get_backends() to get a list of the available backends.</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">model_config</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">ModelConfig</span><span class="p">(</span><span class="n">gym_env_name</span><span class="o">=</span><span class="n">gym_env_name</span><span class="p">,</span> <span class="n">fc_layers</span><span class="o">=</span><span class="n">fc_layers</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_initialize</span><span class="p">(</span><span class="n">model_config</span><span class="o">=</span><span class="n">model_config</span><span class="p">,</span> <span class="n">backend_name</span><span class="o">=</span><span class="n">backend</span><span class="p">)</span>

        <span class="k">return</span>

    <span class="k">def</span> <span class="nf">_initialize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_config</span><span class="p">:</span> <span class="n">core</span><span class="o">.</span><span class="n">ModelConfig</span><span class="p">,</span> <span class="n">backend_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="bp">None</span><span class="p">):</span>

        <span class="k">if</span> <span class="n">backend_name</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>

            <span class="n">backend_name</span> <span class="o">=</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">default</span><span class="o">.</span><span class="n">DefaultAgentFactory</span><span class="o">.</span><span class="n">backend_name</span>

        <span class="n">backend</span><span class="p">:</span> <span class="n">bcore</span><span class="o">.</span><span class="n">BackendAgentFactory</span> <span class="o">=</span> <span class="n">_get_backend</span><span class="p">(</span><span class="n">backend_name</span><span class="p">)</span>

        <span class="k">assert</span> <span class="n">model_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">,</span> <span class="s2">&quot;model_config not set.&quot;</span>

        <span class="k">assert</span> <span class="n">backend</span><span class="p">,</span> <span class="n">f</span><span class="s1">&#39;Backend &quot;{backend_name}&quot; not found. The registered backends are {get_backends()}.&#39;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_model_config</span><span class="p">:</span> <span class="n">core</span><span class="o">.</span><span class="n">ModelConfig</span> <span class="o">=</span> <span class="n">model_config</span>

        <span class="n">backend_agent</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="n">create_agent</span><span class="p">(</span><span class="n">easyagent_type</span><span class="o">=</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">),</span> <span class="n">model_config</span><span class="o">=</span><span class="n">model_config</span><span class="p">)</span>

        <span class="k">assert</span> <span class="n">backend_agent</span><span class="p">,</span> <span class="n">f</span><span class="s1">&#39;Backend &quot;{backend_name}&quot; does not implement &quot;{type(self).__name__}&quot;. &#39;</span> <span class="o">+</span> \

                              <span class="n">f</span><span class="s1">&#39;Choose one of the following backend {get_backends(type(self))}.&#39;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_backend_agent</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">bcore</span><span class="o">.</span><span class="n">_BackendAgent</span><span class="p">]</span> <span class="o">=</span> <span class="n">backend_agent</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_backend_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">backend_name</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_backend_agent</span><span class="o">.</span><span class="n">_agent_context</span><span class="o">.</span><span class="n">_agent_saver</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">save</span>

        <span class="k">return</span>

    <span class="k">def</span> <span class="nf">_add_plot_callbacks</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">callbacks</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">],</span>

                            <span class="n">default_plots</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>

                            <span class="n">default_plot_callbacks</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">plot</span><span class="o">.</span><span class="n">_PlotCallback</span><span class="p">]</span>

                            <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">]:</span>

        <span class="sd">&quot;&quot;&quot;Adds the default callbacks and sorts all callbacks in the order</span>

<span class="sd">            _PreProcessCallbacks, AgentCallbacks, _PostProcessCallbacks.</span>

<span class="sd">        Args:</span>

<span class="sd">            callbacks: existing callbacks to prepare</span>

<span class="sd">            default_plots: if set or if None and callbacks does not contain plots then the default plots are added</span>

<span class="sd">            default_plot_callbacks: plot callbacks to add.</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">pre_process</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">plot</span><span class="o">.</span><span class="n">_PreProcess</span><span class="p">()]</span>

        <span class="n">agent</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="n">post_process</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">plot</span><span class="o">.</span><span class="n">_PostProcess</span><span class="p">()]</span>

        <span class="k">if</span> <span class="n">default_plots</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>

            <span class="n">default_plots</span> <span class="o">=</span> <span class="bp">True</span>

            <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">callbacks</span><span class="p">:</span>

                <span class="n">default_plots</span> <span class="o">=</span> <span class="n">default_plots</span> <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">plot</span><span class="o">.</span><span class="n">_PlotCallback</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">default_plots</span><span class="p">:</span>

            <span class="n">agent</span> <span class="o">=</span> <span class="n">default_plot_callbacks</span>

        <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">callbacks</span><span class="p">:</span>

            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">core</span><span class="o">.</span><span class="n">_PreProcessCallback</span><span class="p">):</span>

                <span class="n">pre_process</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>

            <span class="k">else</span><span class="p">:</span>

                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">core</span><span class="o">.</span><span class="n">_PostProcessCallback</span><span class="p">):</span>

                    <span class="n">post_process</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>

                <span class="k">else</span><span class="p">:</span>

                    <span class="n">agent</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>

        <span class="n">result</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">]</span> <span class="o">=</span> <span class="n">pre_process</span> <span class="o">+</span> <span class="n">agent</span> <span class="o">+</span> <span class="n">post_process</span>

        <span class="k">return</span> <span class="n">result</span>

    <span class="nd">@staticmethod</span>

    <span class="k">def</span> <span class="nf">_from_dict</span><span class="p">(</span><span class="n">param_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">object</span><span class="p">]):</span>

        <span class="sd">&quot;&quot;&quot;recreates a new agent instance according to the definition previously created by _to_dict.</span>

<span class="sd">        Returns:</span>

<span class="sd">            new agent instance (excluding any trained policy), the agent type is preserved.</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">assert</span> <span class="n">param_dict</span>

        <span class="n">mc</span><span class="p">:</span> <span class="n">core</span><span class="o">.</span><span class="n">ModelConfig</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">ModelConfig</span><span class="o">.</span><span class="n">_from_dict</span><span class="p">(</span><span class="n">param_dict</span><span class="p">[</span><span class="n">EasyAgent</span><span class="o">.</span><span class="n">_KEY_MODEL_CONFIG</span><span class="p">])</span>

        <span class="n">agent_class</span> <span class="o">=</span> <span class="nb">globals</span><span class="p">()[</span><span class="n">param_dict</span><span class="p">[</span><span class="n">EasyAgent</span><span class="o">.</span><span class="n">_KEY_EASYAGENT_CLASS</span><span class="p">]]</span>

        <span class="n">backend</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">param_dict</span><span class="p">[</span><span class="n">EasyAgent</span><span class="o">.</span><span class="n">_KEY_BACKEND</span><span class="p">]</span>

        <span class="n">result</span> <span class="o">=</span> <span class="n">agent_class</span><span class="p">(</span><span class="n">gym_env_name</span><span class="o">=</span><span class="n">mc</span><span class="o">.</span><span class="n">original_env_name</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">)</span>

        <span class="n">result</span><span class="o">.</span><span class="n">_initialize</span><span class="p">(</span><span class="n">model_config</span><span class="o">=</span><span class="n">mc</span><span class="p">,</span> <span class="n">backend_name</span><span class="o">=</span><span class="n">backend</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">result</span>

    <span class="k">def</span> <span class="nf">_to_callback_list</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">callbacks</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span>

        <span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">]:</span>

        <span class="sd">&quot;&quot;&quot;maps callbacks to an admissible callback list.</span>

<span class="sd">        if callbacks is None an empty list is returned.</span>

<span class="sd">        if callbacks is an AgentCallback a list containing only this callback is returned</span>

<span class="sd">        otherwise callbacks is returned</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">result</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">callbacks</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>

            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">callbacks</span><span class="p">,</span> <span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">):</span>

                <span class="n">result</span> <span class="o">=</span> <span class="p">[</span><span class="n">callbacks</span><span class="p">]</span>

            <span class="k">else</span><span class="p">:</span>

                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">callbacks</span><span class="p">,</span> <span class="nb">list</span><span class="p">),</span> <span class="s2">&quot;callback not an AgentCallback or a list thereof.&quot;</span>

                <span class="n">result</span> <span class="o">=</span> <span class="n">callbacks</span>

        <span class="k">return</span> <span class="n">result</span>

    <span class="k">def</span> <span class="nf">_to_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">object</span><span class="p">]:</span>

        <span class="sd">&quot;&quot;&quot;saves the agent definition to a dict.</span>

<span class="sd">            Returns:</span>

<span class="sd">                dict containing all parameters to recreate the agent (excluding a trained policy)</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">result</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">object</span><span class="p">]</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>

        <span class="n">result</span><span class="p">[</span><span class="n">EasyAgent</span><span class="o">.</span><span class="n">_KEY_VERSION</span><span class="p">]</span> <span class="o">=</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">__version__</span>

        <span class="n">result</span><span class="p">[</span><span class="n">EasyAgent</span><span class="o">.</span><span class="n">_KEY_EASYAGENT_CLASS</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>

        <span class="n">result</span><span class="p">[</span><span class="n">EasyAgent</span><span class="o">.</span><span class="n">_KEY_BACKEND</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backend_name</span>

        <span class="n">result</span><span class="p">[</span><span class="n">EasyAgent</span><span class="o">.</span><span class="n">_KEY_MODEL_CONFIG</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model_config</span><span class="o">.</span><span class="n">_to_dict</span><span class="p">()</span>

        <span class="n">result</span><span class="p">[</span><span class="n">EasyAgent</span><span class="o">.</span><span class="n">_KEY_POLICY_DIRECTORY</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;policy&#39;</span>

        <span class="k">return</span> <span class="n">result</span>

    <span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>

                 <span class="n">num_episodes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>

                 <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">):</span>

        <span class="sd">&quot;&quot;&quot;Plays num_episodes with the current policy and computes metrics on rewards.</span>

<span class="sd">        Args:</span>

<span class="sd">            num_episodes: number of episodes to play</span>

<span class="sd">            max_steps_per_episode: max steps per episode</span>

<span class="sd">        Returns:</span>

<span class="sd">            extensible score metrics</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">play_context</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">PlayContext</span><span class="p">()</span>

        <span class="n">play_context</span><span class="o">.</span><span class="n">max_steps_per_episode</span> <span class="o">=</span> <span class="n">max_steps_per_episode</span>

        <span class="n">play_context</span><span class="o">.</span><span class="n">num_episodes</span> <span class="o">=</span> <span class="n">num_episodes</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">play</span><span class="p">(</span><span class="n">play_context</span><span class="o">=</span><span class="n">play_context</span><span class="p">,</span> <span class="n">default_plots</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

        <span class="n">Metrics</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;Metrics&#39;</span><span class="p">,</span> <span class="s1">&#39;steps rewards&#39;</span><span class="p">)</span>

        <span class="n">Rewards</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;Rewards&#39;</span><span class="p">,</span> <span class="s1">&#39;mean std min max all&#39;</span><span class="p">)</span>

        <span class="n">all_rewards</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">play_context</span><span class="o">.</span><span class="n">sum_of_rewards</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>

        <span class="n">mean_reward</span><span class="p">,</span> <span class="n">std_reward</span><span class="p">,</span> <span class="n">min_reward</span><span class="p">,</span> <span class="n">max_reward</span> <span class="o">=</span> <span class="n">statistics</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">all_rewards</span><span class="p">),</span> <span class="n">statistics</span><span class="o">.</span><span class="n">stdev</span><span class="p">(</span>

            <span class="n">all_rewards</span><span class="p">),</span> <span class="nb">min</span><span class="p">(</span><span class="n">all_rewards</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">all_rewards</span><span class="p">)</span>

        <span class="n">rewards</span> <span class="o">=</span> <span class="n">Rewards</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mean_reward</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="n">std_reward</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="n">min_reward</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="n">max_reward</span><span class="p">,</span> <span class="nb">all</span><span class="o">=</span><span class="n">all_rewards</span><span class="p">)</span>

        <span class="n">Steps</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;Steps&#39;</span><span class="p">,</span> <span class="s1">&#39;mean std min max all&#39;</span><span class="p">)</span>

        <span class="n">all_num_steps</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">play_context</span><span class="o">.</span><span class="n">rewards</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>

            <span class="n">all_num_steps</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">play_context</span><span class="o">.</span><span class="n">rewards</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>

        <span class="n">mean_steps</span><span class="p">,</span> <span class="n">std_steps</span><span class="p">,</span> <span class="n">min_steps</span><span class="p">,</span> <span class="n">max_steps</span> <span class="o">=</span> <span class="n">statistics</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">all_num_steps</span><span class="p">),</span> <span class="n">statistics</span><span class="o">.</span><span class="n">stdev</span><span class="p">(</span>

            <span class="n">all_num_steps</span><span class="p">),</span> <span class="nb">min</span><span class="p">(</span><span class="n">all_num_steps</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">all_num_steps</span><span class="p">)</span>

        <span class="n">steps</span> <span class="o">=</span> <span class="n">Steps</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mean_steps</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="n">std_steps</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="n">min_steps</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="n">max_steps</span><span class="p">,</span> <span class="nb">all</span><span class="o">=</span><span class="n">all_num_steps</span><span class="p">)</span>

        <span class="n">metrics</span> <span class="o">=</span> <span class="n">Metrics</span><span class="p">(</span><span class="n">rewards</span><span class="o">=</span><span class="n">rewards</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="n">steps</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">metrics</span>

    <span class="k">def</span> <span class="nf">play</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>

             <span class="n">callbacks</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>

             <span class="n">num_episodes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>

             <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>

             <span class="n">play_context</span><span class="p">:</span> <span class="n">core</span><span class="o">.</span><span class="n">PlayContext</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>

             <span class="n">default_plots</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">None</span><span class="p">):</span>

        <span class="sd">&quot;&quot;&quot;Plays num_episodes with the current policy.</span>

<span class="sd">        Args:</span>

<span class="sd">            callbacks: list of callbacks called during each episode play</span>

<span class="sd">            num_episodes: number of episodes to play</span>

<span class="sd">            max_steps_per_episode: max steps per episode</span>

<span class="sd">            play_context: play configuration to be used. If set override all other play context arguments</span>

<span class="sd">            default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...)</span>

<span class="sd">        Returns:</span>

<span class="sd">            play_context containg the actions taken and the rewards received during training</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backend_agent</span><span class="o">.</span><span class="n">_agent_context</span><span class="o">.</span><span class="n">_is_policy_trained</span><span class="p">,</span> <span class="s2">&quot;No trained policy available. Call train() first.&quot;</span>

        <span class="k">if</span> <span class="n">play_context</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>

            <span class="n">play_context</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">PlayContext</span><span class="p">()</span>

            <span class="n">play_context</span><span class="o">.</span><span class="n">max_steps_per_episode</span> <span class="o">=</span> <span class="n">max_steps_per_episode</span>

            <span class="n">play_context</span><span class="o">.</span><span class="n">num_episodes</span> <span class="o">=</span> <span class="n">num_episodes</span>

        <span class="n">callbacks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_to_callback_list</span><span class="p">(</span><span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span>

        <span class="n">callbacks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_add_plot_callbacks</span><span class="p">(</span><span class="n">callbacks</span><span class="p">,</span> <span class="n">default_plots</span><span class="p">,</span> <span class="p">[</span><span class="n">plot</span><span class="o">.</span><span class="n">Steps</span><span class="p">(),</span> <span class="n">plot</span><span class="o">.</span><span class="n">Rewards</span><span class="p">()])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_backend_agent</span><span class="o">.</span><span class="n">play</span><span class="p">(</span><span class="n">play_context</span><span class="o">=</span><span class="n">play_context</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">play_context</span>

    <span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">directory</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>

             <span class="n">callbacks</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>

        <span class="sd">&quot;&quot;&quot;Saves the currently trained actor policy in directory.</span>

<span class="sd">        If save is called before a trained policy is created, eg by calling train, an exception is raised.</span>

<span class="sd">        Args:</span>

<span class="sd">             directory: the directory to save the policy weights to.</span>

<span class="sd">                if the directory does not exist yet, a new directory is created. if None the policy is saved</span>

<span class="sd">                in a temp directory.</span>

<span class="sd">             callbacks: list of callbacks called during save (eg log.Agent)</span>

<span class="sd">        Returns:</span>

<span class="sd">            the absolute path to the directory containing the saved policy.</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">directory</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>

            <span class="n">directory</span> <span class="o">=</span> <span class="n">bcore</span><span class="o">.</span><span class="n">_get_temp_path</span><span class="p">()</span>

        <span class="k">assert</span> <span class="n">directory</span>

        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backend_agent</span><span class="o">.</span><span class="n">_agent_context</span><span class="o">.</span><span class="n">_is_policy_trained</span><span class="p">,</span> <span class="s2">&quot;No trained policy available. Call train() first.&quot;</span>

        <span class="n">directory</span> <span class="o">=</span> <span class="n">bcore</span><span class="o">.</span><span class="n">_mkdir</span><span class="p">(</span><span class="n">directory</span><span class="p">)</span>

        <span class="n">agent_json_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span> <span class="n">EasyAgent</span><span class="o">.</span><span class="n">_KEY_EASYAGENT_FILENAME</span><span class="p">)</span>

        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">agent_json_path</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">jsonfile</span><span class="p">:</span>

            <span class="n">agent_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_to_dict</span><span class="p">()</span>

            <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">agent_dict</span><span class="p">,</span> <span class="n">jsonfile</span><span class="p">,</span> <span class="n">sort_keys</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="n">callbacks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_to_callback_list</span><span class="p">(</span><span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span>

        <span class="n">policy_directory</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span> <span class="n">EasyAgent</span><span class="o">.</span><span class="n">_KEY_POLICY_DIRECTORY</span><span class="p">)</span>

        <span class="n">policy_directory</span> <span class="o">=</span> <span class="n">bcore</span><span class="o">.</span><span class="n">_mkdir</span><span class="p">(</span><span class="n">policy_directory</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_backend_agent</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">directory</span><span class="o">=</span><span class="n">policy_directory</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">directory</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train_context</span><span class="p">:</span> <span class="n">core</span><span class="o">.</span><span class="n">TrainContext</span><span class="p">,</span>

              <span class="n">callbacks</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="bp">None</span><span class="p">],</span>

              <span class="n">default_plots</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]):</span>

        <span class="sd">&quot;&quot;&quot;Trains a new model using the gym environment passed during instantiation.</span>

<span class="sd">        Args:</span>

<span class="sd">            callbacks: list of callbacks called during the training and evaluation</span>

<span class="sd">            train_context: training configuration to be used (num_iterations,num_episodes_per_iteration,...)</span>

<span class="sd">            default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...).</span>

<span class="sd">                if None default callbacks are only added if the callbacks list is empty</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">assert</span> <span class="n">train_context</span><span class="p">,</span> <span class="s2">&quot;train_context not set.&quot;</span>

        <span class="n">callbacks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_to_callback_list</span><span class="p">(</span><span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span>

        <span class="n">callbacks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_add_plot_callbacks</span><span class="p">(</span><span class="n">callbacks</span><span class="p">,</span> <span class="n">default_plots</span><span class="p">,</span> <span class="p">[</span><span class="n">plot</span><span class="o">.</span><span class="n">Loss</span><span class="p">(),</span> <span class="n">plot</span><span class="o">.</span><span class="n">Steps</span><span class="p">(),</span> <span class="n">plot</span><span class="o">.</span><span class="n">Rewards</span><span class="p">()])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_backend_agent</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">train_context</span><span class="o">=</span><span class="n">train_context</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">get_backends</span><span class="p">(</span><span class="n">agent</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Type</span><span class="p">[</span><span class="n">EasyAgent</span><span class="p">]]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">):</span>

    <span class="sd">&quot;&quot;&quot;returns a list of all registered backends containing an implementation for the EasyAgent type agent.</span>

<span class="sd">    Args:</span>

<span class="sd">        agent: type deriving from EasyAgent for which the backend identifiers are returned.</span>

<span class="sd">    Returns:</span>

<span class="sd">        a list of admissible values for the &#39;backend&#39; argument of EazyAgents constructors or a list of all</span>

<span class="sd">        available backends if agent is None.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">result</span> <span class="o">=</span> <span class="p">[</span><span class="n">b</span><span class="o">.</span><span class="n">backend_name</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">_backends</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">agent</span><span class="p">:</span>

        <span class="n">result</span> <span class="o">=</span> <span class="p">[</span><span class="n">b</span><span class="o">.</span><span class="n">backend_name</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">_backends</span> <span class="k">if</span> <span class="n">agent</span> <span class="ow">in</span> <span class="n">b</span><span class="o">.</span><span class="n">get_algorithms</span><span class="p">()]</span>

    <span class="k">return</span> <span class="n">result</span>

<span class="k">def</span> <span class="nf">_get_backend</span><span class="p">(</span><span class="n">backend_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>

    <span class="sd">&quot;&quot;&quot;Yields the backend with the given name.</span>

<span class="sd">    Returns:</span>

<span class="sd">        the backend instance or None if no backend is found.&quot;&quot;&quot;</span>

    <span class="k">assert</span> <span class="n">backend_name</span>

    <span class="n">backends</span> <span class="o">=</span> <span class="p">[</span><span class="n">b</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">_backends</span> <span class="k">if</span> <span class="n">b</span><span class="o">.</span><span class="n">backend_name</span> <span class="o">==</span> <span class="n">backend_name</span><span class="p">]</span>

    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">backends</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">f</span><span class="s1">&#39;no backend found with name &quot;{backend_name}&quot;. Available backends = {get_backends()}&#39;</span>

    <span class="n">result</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="k">if</span> <span class="n">backends</span><span class="p">:</span>

        <span class="n">result</span> <span class="o">=</span> <span class="n">backends</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">result</span>

<span class="k">class</span> <span class="nc">CemAgent</span><span class="p">(</span><span class="n">EasyAgent</span><span class="p">):</span>

    <span class="sd">&quot;&quot;&quot;creates a new agent based on the cross-entropy-method algorithm.</span>

<span class="sd">       From https://learning.mpi-sws.org/mlss2016/slides/2016-MLSS-RL.pdf:</span>

<span class="sd">        Initialize µ ∈Rd,σ ∈Rd</span>

<span class="sd">        for iteration = 1,2,... num_iterations do</span>

<span class="sd">            Collect num_episodes_per_iteration samples of θi ∼ N(µ,diag(σ))</span>

<span class="sd">            Perform a noisy evaluation Ri ∼ θi</span>

<span class="sd">            Select the top elite_set_fraction of samples (e.g. p = 0.2), which we’ll call the elite set</span>

<span class="sd">            Fit a Gaussian distribution, with diagonal covariance, to the elite set, obtaining a new µ,σ.</span>

<span class="sd">        end for</span>

<span class="sd">        Return the ﬁnal µ.</span>

<span class="sd">        see https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.6579&amp;rep=rep1&amp;type=pdf</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gym_env_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">fc_layers</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">backend</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="bp">None</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">gym_env_name</span><span class="p">,</span> <span class="n">fc_layers</span><span class="p">,</span> <span class="n">backend</span><span class="p">)</span>

        <span class="k">assert</span> <span class="bp">False</span><span class="p">,</span> <span class="s2">&quot;CemAgent is currently not available (pending migration of keras-rl to tf2.0)&quot;</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>

              <span class="n">callbacks</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>

              <span class="n">num_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>

              <span class="n">num_episodes_per_iteration</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>

              <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span>

              <span class="n">elite_set_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>

              <span class="n">num_iterations_between_eval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>

              <span class="n">num_episodes_per_eval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>

              <span class="n">train_context</span><span class="p">:</span> <span class="n">core</span><span class="o">.</span><span class="n">CemTrainContext</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>

              <span class="n">default_plots</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">None</span><span class="p">):</span>

        <span class="sd">&quot;&quot;&quot;Trains a new model using the gym environment passed during instantiation.</span>

<span class="sd">        Args:</span>

<span class="sd">            callbacks: list of callbacks called during training and evaluation</span>

<span class="sd">            num_iterations: number of times the training is repeated (with additional data)</span>

<span class="sd">            num_episodes_per_iteration: number of episodes played in each iteration. for each episode a new</span>

<span class="sd">                policy is sampled from the current weight distribution.</span>

<span class="sd">            max_steps_per_episode: maximum number of steps per episode</span>

<span class="sd">            elite_set_fraction: the fraction of policies which are members of the elite set.</span>

<span class="sd">                These policies are used to fit a new weight distribution in each iteration.</span>

<span class="sd">            num_iterations_between_eval: number of training iterations before the current policy is evaluated.</span>

<span class="sd">                if 0 no evaluation is performed.</span>

<span class="sd">            num_episodes_per_eval: number of episodes played to estimate the average return and steps</span>

<span class="sd">            train_context: training configuration to be used. if set overrides all other training context arguments.</span>

<span class="sd">            default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...).</span>

<span class="sd">                if None default callbacks are only added if the callbacks list is empty</span>

<span class="sd">        Returns:</span>

<span class="sd">            train_context: the training configuration containing the loss and sum of rewards encountered</span>

<span class="sd">                during training</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">train_context</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>

            <span class="n">train_context</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">CemTrainContext</span><span class="p">()</span>

            <span class="n">train_context</span><span class="o">.</span><span class="n">num_iterations</span> <span class="o">=</span> <span class="n">num_iterations</span>

            <span class="n">train_context</span><span class="o">.</span><span class="n">max_steps_per_episode</span> <span class="o">=</span> <span class="n">max_steps_per_episode</span>

            <span class="n">train_context</span><span class="o">.</span><span class="n">elite_set_fraction</span> <span class="o">=</span> <span class="n">elite_set_fraction</span>

            <span class="n">train_context</span><span class="o">.</span><span class="n">num_iterations_between_eval</span> <span class="o">=</span> <span class="n">num_iterations_between_eval</span>

            <span class="n">train_context</span><span class="o">.</span><span class="n">num_episodes_per_eval</span> <span class="o">=</span> <span class="n">num_episodes_per_eval</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">train_context</span><span class="o">=</span><span class="n">train_context</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span> <span class="n">default_plots</span><span class="o">=</span><span class="n">default_plots</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">train_context</span>

<span class="k">class</span> <span class="nc">DqnAgent</span><span class="p">(</span><span class="n">EasyAgent</span><span class="p">):</span>

    <span class="sd">&quot;&quot;&quot;creates a new agent based on the Dqn algorithm.</span>

<span class="sd">    From wikipedia:</span>

<span class="sd">    The DeepMind system used a deep convolutional neural network, with layers of tiled convolutional filters to mimic</span>

<span class="sd">    the effects of receptive fields. Reinforcement learning is unstable or divergent when a nonlinear function</span>

<span class="sd">    approximator such as a neural network is used to represent Q.</span>

<span class="sd">    This instability comes from the correlations present in the sequence of observations, the fact that small updates</span>

<span class="sd">    to Q may significantly change the policy and the data distribution, and the correlations between Q and the</span>

<span class="sd">    target values.</span>

<span class="sd">    The technique used experience replay, a biologically inspired mechanism that uses a random sample of prior actions</span>

<span class="sd">    instead of the most recent action to proceed.[2] This removes correlations in the observation sequence and smooths</span>

<span class="sd">    changes in the data distribution. Iterative update adjusts Q towards target values that are only periodically</span>

<span class="sd">    updated, further reducing correlations with the target.[17]</span>

<span class="sd">    see also: https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>

              <span class="n">callbacks</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>

              <span class="n">num_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20000</span><span class="p">,</span>

              <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span>

              <span class="n">num_steps_per_iteration</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>

              <span class="n">num_steps_buffer_preload</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>

              <span class="n">num_steps_sampled_from_buffer</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>

              <span class="n">num_iterations_between_eval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>

              <span class="n">num_episodes_per_eval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>

              <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">,</span>

              <span class="n">train_context</span><span class="p">:</span> <span class="n">core</span><span class="o">.</span><span class="n">StepsTrainContext</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>

              <span class="n">default_plots</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">None</span><span class="p">):</span>

        <span class="sd">&quot;&quot;&quot;Trains a new model using the gym environment passed during instantiation.</span>

<span class="sd">        Args:</span>

<span class="sd">            callbacks: list of callbacks called during training and evaluation</span>

<span class="sd">            num_iterations: number of times the training is repeated (with additional data)</span>

<span class="sd">            max_steps_per_episode: maximum number of steps per episode</span>

<span class="sd">            num_steps_per_iteration: number of steps played per training iteration</span>

<span class="sd">            num_steps_buffer_preload: number of initial collect steps to preload the buffer</span>

<span class="sd">            num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training</span>

<span class="sd">            num_iterations_between_eval: number of training iterations before the current policy is evaluated.</span>

<span class="sd">                if 0 no evaluation is performed.</span>

<span class="sd">            num_episodes_per_eval: number of episodes played to estimate the average return and steps</span>

<span class="sd">            learning_rate: the learning rate used in the next iteration&#39;s policy training (0,1]</span>

<span class="sd">            train_context: training configuration to be used. if set overrides all other training context arguments.</span>

<span class="sd">            default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...).</span>

<span class="sd">                if None default callbacks are only added if the callbacks list is empty</span>

<span class="sd">        Returns:</span>

<span class="sd">            train_context: the training configuration containing the loss and sum of rewards encountered</span>

<span class="sd">                during training</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">train_context</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>

            <span class="n">train_context</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">StepsTrainContext</span><span class="p">()</span>

            <span class="n">train_context</span><span class="o">.</span><span class="n">num_iterations</span> <span class="o">=</span> <span class="n">num_iterations</span>

            <span class="n">train_context</span><span class="o">.</span><span class="n">max_steps_per_episode</span> <span class="o">=</span> <span class="n">max_steps_per_episode</span>

            <span class="n">train_context</span><span class="o">.</span><span class="n">num_steps_per_iteration</span> <span class="o">=</span> <span class="n">num_steps_per_iteration</span>

            <span class="n">train_context</span><span class="o">.</span><span class="n">num_steps_buffer_preload</span> <span class="o">=</span> <span class="n">num_steps_buffer_preload</span>

            <span class="n">train_context</span><span class="o">.</span><span class="n">num_steps_sampled_from_buffer</span> <span class="o">=</span> <span class="n">num_steps_sampled_from_buffer</span>

            <span class="n">train_context</span><span class="o">.</span><span class="n">num_iterations_between_eval</span> <span class="o">=</span> <span class="n">num_iterations_between_eval</span>

            <span class="n">train_context</span><span class="o">.</span><span class="n">num_episodes_per_eval</span> <span class="o">=</span> <span class="n">num_episodes_per_eval</span>

            <span class="n">train_context</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">train_context</span><span class="o">=</span><span class="n">train_context</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span> <span class="n">default_plots</span><span class="o">=</span><span class="n">default_plots</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">train_context</span>

<span class="k">class</span> <span class="nc">DoubleDqnAgent</span><span class="p">(</span><span class="n">DqnAgent</span><span class="p">):</span>

    <span class="sd">&quot;&quot;&quot;Agent based on the Double Dqn algorithm (https://arxiv.org/abs/1509.06461)&quot;&quot;&quot;</span>

<span class="k">class</span> <span class="nc">DuelingDqnAgent</span><span class="p">(</span><span class="n">DqnAgent</span><span class="p">):</span>

    <span class="sd">&quot;&quot;&quot;Agent based on the Dueling Dqn algorithm (https://arxiv.org/abs/1511.06581).&quot;&quot;&quot;</span>

<span class="k">class</span> <span class="nc">PpoAgent</span><span class="p">(</span><span class="n">EasyAgent</span><span class="p">):</span>

    <span class="sd">&quot;&quot;&quot;creates a new agent based on the PPO algorithm.</span>

<span class="sd">        PPO is an actor-critic algorithm using 2 neural networks. The actor network</span>

<span class="sd">        to predict the next action to be taken and the critic network to estimate</span>

<span class="sd">        the value of the game state we are currently in (the expected, discounted</span>

<span class="sd">        sum of future rewards when following the current actor network).</span>

<span class="sd">        see also: https://spinningup.openai.com/en/latest/algorithms/ppo.html</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>

              <span class="n">callbacks</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>

              <span class="n">num_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>

              <span class="n">num_episodes_per_iteration</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>

              <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span>

              <span class="n">num_epochs_per_iteration</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>

              <span class="n">num_iterations_between_eval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>

              <span class="n">num_episodes_per_eval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>

              <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">,</span>

              <span class="n">train_context</span><span class="p">:</span> <span class="n">core</span><span class="o">.</span><span class="n">PpoTrainContext</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>

              <span class="n">default_plots</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">None</span><span class="p">):</span>

        <span class="sd">&quot;&quot;&quot;Trains a new model using the gym environment passed during instantiation.</span>

<span class="sd">        Args:</span>

<span class="sd">            callbacks: list of callbacks called during training and evaluation</span>

<span class="sd">            num_iterations: number of times the training is repeated (with additional data)</span>

<span class="sd">            num_episodes_per_iteration: number of episodes played per training iteration</span>

<span class="sd">            max_steps_per_episode: maximum number of steps per episode</span>

<span class="sd">            num_epochs_per_iteration: number of times the data collected for the current iteration</span>

<span class="sd">                is used to retrain the current policy</span>

<span class="sd">            num_iterations_between_eval: number of training iterations before the current policy is evaluated.</span>

<span class="sd">                if 0 no evaluation is performed.</span>

<span class="sd">            num_episodes_per_eval: number of episodes played to estimate the average return and steps</span>

<span class="sd">            learning_rate: the learning rate used in the next iteration&#39;s policy training (0,1]</span>

<span class="sd">            train_context: training configuration to be used. if set overrides all other training context arguments.</span>

<span class="sd">            default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...).</span>

<span class="sd">                if None default callbacks are only added if the callbacks list is empty</span>

<span class="sd">        Returns:</span>

<span class="sd">            train_context: the training configuration containing the loss and sum of rewards encountered</span>

<span class="sd">                during training</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">train_context</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>

            <span class="n">train_context</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">PpoTrainContext</span><span class="p">()</span>

            <span class="n">train_context</span><span class="o">.</span><span class="n">num_iterations</span> <span class="o">=</span> <span class="n">num_iterations</span>

            <span class="n">train_context</span><span class="o">.</span><span class="n">num_episodes_per_iteration</span> <span class="o">=</span> <span class="n">num_episodes_per_iteration</span>

            <span class="n">train_context</span><span class="o">.</span><span class="n">max_steps_per_episode</span> <span class="o">=</span> <span class="n">max_steps_per_episode</span>

            <span class="n">train_context</span><span class="o">.</span><span class="n">num_epochs_per_iteration</span> <span class="o">=</span> <span class="n">num_epochs_per_iteration</span>

            <span class="n">train_context</span><span class="o">.</span><span class="n">num_iterations_between_eval</span> <span class="o">=</span> <span class="n">num_iterations_between_eval</span>

            <span class="n">train_context</span><span class="o">.</span><span class="n">num_episodes_per_eval</span> <span class="o">=</span> <span class="n">num_episodes_per_eval</span>

            <span class="n">train_context</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">train_context</span><span class="o">=</span><span class="n">train_context</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span> <span class="n">default_plots</span><span class="o">=</span><span class="n">default_plots</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">train_context</span>

<span class="k">class</span> <span class="nc">RandomAgent</span><span class="p">(</span><span class="n">EasyAgent</span><span class="p">):</span>

    <span class="sd">&quot;&quot;&quot;Agent which always chooses uniform random actions.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>

              <span class="n">callbacks</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>

              <span class="n">num_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>

              <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>

              <span class="n">num_episodes_per_eval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>

              <span class="n">train_context</span><span class="p">:</span> <span class="n">core</span><span class="o">.</span><span class="n">TrainContext</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>

              <span class="n">default_plots</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">None</span><span class="p">):</span>

        <span class="sd">&quot;&quot;&quot;Evaluates the environment using a uniform random policy.</span>

<span class="sd">        The evaluation is performed in batches of num_episodes_per_eval episodes.</span>

<span class="sd">        Args:</span>

<span class="sd">            callbacks: list of callbacks called during training and evaluation</span>

<span class="sd">            num_iterations: number of times a batch of num_episodes_per_eval episodes is evaluated.</span>

<span class="sd">            max_steps_per_episode: maximum number of steps per episode</span>

<span class="sd">            num_episodes_per_eval: number of episodes played to estimate the average return and steps</span>

<span class="sd">            train_context: training configuration to be used. if set overrides all other training context arguments.</span>

<span class="sd">            default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...)</span>

<span class="sd">        Returns:</span>

<span class="sd">            train_context: the training configuration containing the loss and sum of rewards encountered</span>

<span class="sd">                during training</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">train_context</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>

            <span class="n">train_context</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">TrainContext</span><span class="p">()</span>

            <span class="n">train_context</span><span class="o">.</span><span class="n">num_iterations</span> <span class="o">=</span> <span class="n">num_iterations</span>

            <span class="n">train_context</span><span class="o">.</span><span class="n">max_steps_per_episode</span> <span class="o">=</span> <span class="n">max_steps_per_episode</span>

            <span class="n">train_context</span><span class="o">.</span><span class="n">num_epochs_per_iteration</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="n">train_context</span><span class="o">.</span><span class="n">num_iterations_between_eval</span> <span class="o">=</span> <span class="mi">1</span>

            <span class="n">train_context</span><span class="o">.</span><span class="n">num_episodes_per_eval</span> <span class="o">=</span> <span class="n">num_episodes_per_eval</span>

            <span class="n">train_context</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">train_context</span><span class="o">=</span><span class="n">train_context</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span> <span class="n">default_plots</span><span class="o">=</span><span class="n">default_plots</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">train_context</span>

<span class="k">class</span> <span class="nc">ReinforceAgent</span><span class="p">(</span><span class="n">EasyAgent</span><span class="p">):</span>

    <span class="sd">&quot;&quot;&quot;creates a new agent based on the Reinforce algorithm.</span>

<span class="sd">        Reinforce is a vanilla policy gradient algorithm using a single actor network.</span>

<span class="sd">        see also: www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>

              <span class="n">callbacks</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>

              <span class="n">num_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>

              <span class="n">num_episodes_per_iteration</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>

              <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span>

              <span class="n">num_epochs_per_iteration</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>

              <span class="n">num_iterations_between_eval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>

              <span class="n">num_episodes_per_eval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>

              <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">,</span>

              <span class="n">train_context</span><span class="p">:</span> <span class="n">core</span><span class="o">.</span><span class="n">EpisodesTrainContext</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>

              <span class="n">default_plots</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">None</span><span class="p">):</span>

        <span class="sd">&quot;&quot;&quot;Trains a new model using the gym environment passed during instantiation.</span>

<span class="sd">        Args:</span>

<span class="sd">            callbacks: list of callbacks called during training and evaluation</span>

<span class="sd">            num_iterations: number of times the training is repeated (with additional data)</span>

<span class="sd">            num_episodes_per_iteration: number of episodes played per training iteration</span>

<span class="sd">            max_steps_per_episode: maximum number of steps per episode</span>

<span class="sd">            num_epochs_per_iteration: number of times the data collected for the current iteration</span>

<span class="sd">                is used to retrain the current policy</span>

<span class="sd">            num_iterations_between_eval: number of training iterations before the current policy is evaluated.</span>

<span class="sd">                if 0 no evaluation is performed.</span>

<span class="sd">            num_episodes_per_eval: number of episodes played to estimate the average return and steps</span>

<span class="sd">            learning_rate: the learning rate used in the next iteration&#39;s policy training (0,1]</span>

<span class="sd">            train_context: training configuration to be used. if set overrides all other training context arguments.</span>

<span class="sd">            default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...).</span>

<span class="sd">                if None default callbacks are only added if the callbacks list is empty</span>

<span class="sd">        Returns:</span>

<span class="sd">            train_context: the training configuration containing the loss and sum of rewards encountered</span>

<span class="sd">                during training</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">train_context</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>

            <span class="n">train_context</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">EpisodesTrainContext</span><span class="p">()</span>

            <span class="n">train_context</span><span class="o">.</span><span class="n">num_iterations</span> <span class="o">=</span> <span class="n">num_iterations</span>

            <span class="n">train_context</span><span class="o">.</span><span class="n">num_episodes_per_iteration</span> <span class="o">=</span> <span class="n">num_episodes_per_iteration</span>

            <span class="n">train_context</span><span class="o">.</span><span class="n">max_steps_per_episode</span> <span class="o">=</span> <span class="n">max_steps_per_episode</span>

            <span class="n">train_context</span><span class="o">.</span><span class="n">num_epochs_per_iteration</span> <span class="o">=</span> <span class="n">num_epochs_per_iteration</span>

            <span class="n">train_context</span><span class="o">.</span><span class="n">num_iterations_between_eval</span> <span class="o">=</span> <span class="n">num_iterations_between_eval</span>

            <span class="n">train_context</span><span class="o">.</span><span class="n">num_episodes_per_eval</span> <span class="o">=</span> <span class="n">num_episodes_per_eval</span>

            <span class="n">train_context</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">train_context</span><span class="o">=</span><span class="n">train_context</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span> <span class="n">default_plots</span><span class="o">=</span><span class="n">default_plots</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">train_context</span>

<span class="k">class</span> <span class="nc">SacAgent</span><span class="p">(</span><span class="n">DqnAgent</span><span class="p">):</span>

    <span class="sd">&quot;&quot;&quot;Agent based on the Soft-Actor-Critic algorithm (https://arxiv.org/abs/1812.05905).&quot;&quot;&quot;</span>
</pre></div>


</details>
<h2 id="variables">Variables</h2>
<div class="codehilite"><pre><span></span><span class="n">seed</span>
</pre></div>


<h2 id="functions">Functions</h2>
<h3 id="activate_tensorforce">activate_tensorforce</h3>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">activate_tensorforce</span><span class="p">(</span>

<span class="p">)</span>
</pre></div>


<p>registers the tensorforce backend.</p>
<p>Due to an incompatibility between tensorforce and tf-agents, both libraries may not run
in the same python instance. Thus - for the time being - once this method is called,
the tfagents backend may not be used anymore.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">activate_tensorforce</span><span class="p">():</span>

    <span class="sd">&quot;&quot;&quot;registers the tensorforce backend.</span>

<span class="sd">    Due to an incompatibility between tensorforce and tf-agents, both libraries may not run</span>

<span class="sd">    in the same python instance. Thus - for the time being - once this method is called,</span>

<span class="sd">    the tfagents backend may not be used anymore.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="kn">import</span> <span class="nn">easyagents.backends.tforce</span>

    <span class="k">global</span> <span class="n">_backends</span>

    <span class="k">assert</span>  <span class="n">easyagents</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">_tf_eager_execution_active</span> <span class="ow">is</span> <span class="bp">None</span> <span class="ow">or</span> \

            <span class="n">easyagents</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">_tf_eager_execution_active</span> <span class="o">==</span> <span class="bp">False</span><span class="p">,</span> \

            <span class="s2">&quot;tensorforce can not be activated, since tensorflow eager execution mode was already actived.&quot;</span>

    <span class="n">_backends</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">register_backend</span><span class="p">(</span><span class="n">easyagents</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">default</span><span class="o">.</span><span class="n">DefaultAgentFactory</span><span class="p">(</span><span class="n">register_tensorforce</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>

    <span class="n">register_backend</span><span class="p">(</span><span class="n">easyagents</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">tforce</span><span class="o">.</span><span class="n">TensorforceAgentFactory</span><span class="p">())</span>
</pre></div>


</details>
<h3 id="get_backends">get_backends</h3>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_backends</span><span class="p">(</span>
    <span class="n">agent</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Type</span><span class="p">[</span><span class="n">easyagents</span><span class="o">.</span><span class="n">agents</span><span class="o">.</span><span class="n">EasyAgent</span><span class="p">],</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span>
</pre></div>


<p>returns a list of all registered backends containing an implementation for the EasyAgent type agent.</p>
<p>Args:
    agent: type deriving from EasyAgent for which the backend identifiers are returned.</p>
<p>Returns:
    a list of admissible values for the 'backend' argument of EazyAgents constructors or a list of all
    available backends if agent is None.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><span class="n">def</span><span class="w"> </span><span class="n">get_backends</span><span class="p">(</span><span class="nl">agent</span><span class="p">:</span><span class="w"> </span><span class="n">Optional</span><span class="o">[</span><span class="n">Type[EasyAgent</span><span class="o">]</span><span class="err">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">None</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">    </span><span class="ss">&quot;&quot;&quot;returns a list of all registered backends containing an implementation for the EasyAgent type agent.</span>

<span class="ss">    Args:</span>

<span class="ss">        agent: type deriving from EasyAgent for which the backend identifiers are returned.</span>

<span class="ss">    Returns:</span>

<span class="ss">        a list of admissible values for the &#39;backend&#39; argument of EazyAgents constructors or a list of all</span>

<span class="ss">        available backends if agent is None.</span>

<span class="ss">    &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">    </span><span class="k">result</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">b.backend_name for b in _backends</span><span class="o">]</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nl">agent</span><span class="p">:</span><span class="w"></span>

<span class="w">        </span><span class="k">result</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">b.backend_name for b in _backends if agent in b.get_algorithms()</span><span class="o">]</span><span class="w"></span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="k">result</span><span class="w"></span>
</pre></div>


</details>
<h3 id="load">load</h3>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">load</span><span class="p">(</span>
    <span class="n">directory</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span>
</pre></div>


<p>Loads an agent from directory.</p>
<p>After a successful load play() may be called directly.
The agent, model, backend, seed and play policy are restored according to the previously saved agent.</p>
<p>Args:
    directory: the directory containing the previously saved policy.
    callbacks: list of callbacks called during save (eg log.Agent)</p>
<p>Result:
    a new instance of EasyAgents</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><span class="n">def</span> <span class="k">load</span><span class="p">(</span><span class="n">directory</span><span class="p">:</span> <span class="n">str</span><span class="p">,</span>

         <span class="n">callbacks</span><span class="p">:</span> <span class="k">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="k">None</span><span class="p">]</span> <span class="o">=</span> <span class="k">None</span><span class="p">):</span>

    <span class="ss">&quot;&quot;&quot;Loads an agent from directory.</span>

<span class="ss">    After a successful load play() may be called directly.</span>

<span class="ss">    The agent, model, backend, seed and play policy are restored according to the previously saved agent.</span>

<span class="ss">    Args:</span>

<span class="ss">        directory: the directory containing the previously saved policy.</span>

<span class="ss">        callbacks: list of callbacks called during save (eg log.Agent)</span>

<span class="ss">    Result:</span>

<span class="ss">        a new instance of EasyAgents</span>

<span class="ss">    &quot;&quot;&quot;</span>

    <span class="n">assert</span> <span class="n">directory</span>

    <span class="n">agent_json_path</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="k">join</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span> <span class="n">EasyAgent</span><span class="p">.</span><span class="n">_KEY_EASYAGENT_FILENAME</span><span class="p">)</span>

    <span class="n">policy_directory</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="k">join</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span> <span class="n">EasyAgent</span><span class="p">.</span><span class="n">_KEY_POLICY_DIRECTORY</span><span class="p">)</span>

    <span class="n">assert</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">isdir</span><span class="p">(</span><span class="n">directory</span><span class="p">),</span> <span class="n">f</span><span class="s1">&#39;directory &quot;{directory}&quot; not found.&#39;</span>

    <span class="n">assert</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">agent_json_path</span><span class="p">),</span> <span class="s1">&#39;file &quot;{agent_json_path}&quot; not found.&#39;</span>

    <span class="n">assert</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">isdir</span><span class="p">(</span><span class="n">policy_directory</span><span class="p">),</span> <span class="n">f</span><span class="s1">&#39;directory &quot;{policy_directory}&quot; not found.&#39;</span>

    <span class="k">with</span> <span class="k">open</span><span class="p">(</span><span class="n">agent_json_path</span><span class="p">)</span> <span class="k">as</span> <span class="n">jsonfile</span><span class="p">:</span>

        <span class="n">agent_dict</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="k">load</span><span class="p">(</span><span class="n">jsonfile</span><span class="p">)</span>

    <span class="k">result</span> <span class="o">=</span> <span class="n">EasyAgent</span><span class="p">.</span><span class="n">_from_dict</span><span class="p">(</span><span class="n">agent_dict</span><span class="p">)</span>

    <span class="n">callbacks</span> <span class="o">=</span> <span class="k">result</span><span class="p">.</span><span class="n">_to_callback_list</span><span class="p">(</span><span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span>

    <span class="k">result</span><span class="p">.</span><span class="n">_backend_agent</span><span class="p">.</span><span class="k">load</span><span class="p">(</span><span class="n">directory</span><span class="o">=</span><span class="n">policy_directory</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span>

    <span class="k">return</span> <span class="k">result</span>
</pre></div>


</details>
<h3 id="register_backend">register_backend</h3>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">register_backend</span><span class="p">(</span>
    <span class="n">backend</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">BackendAgentFactory</span>
<span class="p">)</span>
</pre></div>


<p>registers a backend as a factory for agent implementations.</p>
<p>If another backend with the same name is already registered, the old backend is replaced by backend.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><span class="n">def</span> <span class="n">register_backend</span><span class="p">(</span><span class="n">backend</span><span class="p">:</span> <span class="n">bcore</span><span class="p">.</span><span class="n">BackendAgentFactory</span><span class="p">):</span>

    <span class="ss">&quot;&quot;&quot;registers a backend as a factory for agent implementations.</span>

<span class="ss">    If another backend with the same name is already registered, the old backend is replaced by backend.</span>

<span class="ss">    &quot;&quot;&quot;</span>

    <span class="n">assert</span> <span class="n">backend</span>

    <span class="n">old_backends</span> <span class="o">=</span> <span class="p">[</span><span class="n">b</span> <span class="k">for</span> <span class="n">b</span> <span class="k">in</span> <span class="n">_backends</span> <span class="k">if</span> <span class="n">b</span><span class="p">.</span><span class="n">backend_name</span> <span class="o">==</span> <span class="n">backend</span><span class="p">.</span><span class="n">backend_name</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">old_backend</span> <span class="k">in</span> <span class="n">old_backends</span><span class="p">:</span>

        <span class="n">_backends</span><span class="p">.</span><span class="n">remove</span><span class="p">(</span><span class="n">old_backend</span><span class="p">)</span>

    <span class="n">_backends</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">backend</span><span class="p">)</span>
</pre></div>


</details>
<h2 id="classes">Classes</h2>
<h3 id="cemagent">CemAgent</h3>
<div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">CemAgent</span><span class="p">(</span>
    <span class="n">gym_env_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">fc_layers</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">backend</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span>
</pre></div>


<p>creates a new agent based on the cross-entropy-method algorithm.</p>
<p>From https://learning.mpi-sws.org/mlss2016/slides/2016-MLSS-RL.pdf:
 Initialize µ ∈Rd,σ ∈Rd
 for iteration = 1,2,... num_iterations do
     Collect num_episodes_per_iteration samples of θi ∼ N(µ,diag(σ))
     Perform a noisy evaluation Ri ∼ θi
     Select the top elite_set_fraction of samples (e.g. p = 0.2), which we’ll call the elite set
     Fit a Gaussian distribution, with diagonal covariance, to the elite set, obtaining a new µ,σ.
 end for
 Return the ﬁnal µ.</p>
<p>see https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.6579&amp;rep=rep1&amp;type=pdf</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><span class="k">class</span> <span class="n">CemAgent</span>(<span class="n">EasyAgent</span>):

    <span class="s">&quot;&quot;&quot;creates a new agent based on the cross-entropy-method algorithm.</span>

<span class="s">       From https://learning.mpi-sws.org/mlss2016/slides/2016-MLSS-RL.pdf:</span>

<span class="s">        Initialize µ ∈Rd,σ ∈Rd</span>

<span class="s">        for iteration = 1,2,... num_iterations do</span>

<span class="s">            Collect num_episodes_per_iteration samples of θi ∼ N(µ,diag(σ))</span>

<span class="s">            Perform a noisy evaluation Ri ∼ θi</span>

<span class="s">            Select the top elite_set_fraction of samples (e.g. p = 0.2), which we’ll call the elite set</span>

<span class="s">            Fit a Gaussian distribution, with diagonal covariance, to the elite set, obtaining a new µ,σ.</span>

<span class="s">        end for</span>

<span class="s">        Return the ﬁnal µ.</span>

<span class="s">        see https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.6579&amp;rep=rep1&amp;type=pdf</span>

<span class="s">    &quot;&quot;&quot;</span>

    <span class="n">def</span> <span class="n">__init__</span>(<span class="k">self</span>, <span class="n">gym_env_name:</span> <span class="n">str</span>, <span class="n">fc_layers:</span> <span class="n">Optional</span>[<span class="n">Tuple</span>[<span class="nb">int</span>, ...]] = <span class="n">None</span>, <span class="n">backend:</span> <span class="n">str</span> = <span class="n">None</span>):

        <span class="n">super</span>().<span class="n">__init__</span>(<span class="n">gym_env_name</span>, <span class="n">fc_layers</span>, <span class="n">backend</span>)

        <span class="n">assert</span> <span class="nb">False</span>, <span class="s">&quot;CemAgent is currently not available (pending migration of keras-rl to tf2.0)&quot;</span>

    <span class="n">def</span> <span class="n">train</span>(<span class="k">self</span>,

              <span class="n">callbacks:</span> <span class="n">Union</span>[<span class="nb">List</span>[<span class="n">core</span>.<span class="n">AgentCallback</span>], <span class="n">core</span>.<span class="n">AgentCallback</span>, <span class="n">None</span>] = <span class="n">None</span>,

              <span class="n">num_iterations:</span> <span class="nb">int</span> = <span class="mi">100</span>,

              <span class="n">num_episodes_per_iteration:</span> <span class="nb">int</span> = <span class="mi">50</span>,

              <span class="n">max_steps_per_episode:</span> <span class="nb">int</span> = <span class="mi">500</span>,

              <span class="n">elite_set_fraction:</span> <span class="n">float</span> = <span class="mf">0.1</span>,

              <span class="n">num_iterations_between_eval:</span> <span class="nb">int</span> = <span class="mi">5</span>,

              <span class="n">num_episodes_per_eval:</span> <span class="nb">int</span> = <span class="mi">10</span>,

              <span class="n">train_context:</span> <span class="n">core</span>.<span class="n">CemTrainContext</span> = <span class="n">None</span>,

              <span class="n">default_plots:</span> <span class="nb">bool</span> = <span class="n">None</span>):

        <span class="s">&quot;&quot;&quot;Trains a new model using the gym environment passed during instantiation.</span>

<span class="s">        Args:</span>

<span class="s">            callbacks: list of callbacks called during training and evaluation</span>

<span class="s">            num_iterations: number of times the training is repeated (with additional data)</span>

<span class="s">            num_episodes_per_iteration: number of episodes played in each iteration. for each episode a new</span>

<span class="s">                policy is sampled from the current weight distribution.</span>

<span class="s">            max_steps_per_episode: maximum number of steps per episode</span>

<span class="s">            elite_set_fraction: the fraction of policies which are members of the elite set.</span>

<span class="s">                These policies are used to fit a new weight distribution in each iteration.</span>

<span class="s">            num_iterations_between_eval: number of training iterations before the current policy is evaluated.</span>

<span class="s">                if 0 no evaluation is performed.</span>

<span class="s">            num_episodes_per_eval: number of episodes played to estimate the average return and steps</span>

<span class="s">            train_context: training configuration to be used. if set overrides all other training context arguments.</span>

<span class="s">            default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...).</span>

<span class="s">                if None default callbacks are only added if the callbacks list is empty</span>

<span class="s">        Returns:</span>

<span class="s">            train_context: the training configuration containing the loss and sum of rewards encountered</span>

<span class="s">                during training</span>

<span class="s">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">train_context</span> <span class="k">is</span> <span class="n">None:</span>

            <span class="n">train_context</span> = <span class="n">core</span>.<span class="n">CemTrainContext</span>()

            <span class="n">train_context</span>.<span class="n">num_iterations</span> = <span class="n">num_iterations</span>

            <span class="n">train_context</span>.<span class="n">max_steps_per_episode</span> = <span class="n">max_steps_per_episode</span>

            <span class="n">train_context</span>.<span class="n">elite_set_fraction</span> = <span class="n">elite_set_fraction</span>

            <span class="n">train_context</span>.<span class="n">num_iterations_between_eval</span> = <span class="n">num_iterations_between_eval</span>

            <span class="n">train_context</span>.<span class="n">num_episodes_per_eval</span> = <span class="n">num_episodes_per_eval</span>

        <span class="n">super</span>().<span class="n">train</span>(<span class="n">train_context</span>=<span class="n">train_context</span>, <span class="n">callbacks</span>=<span class="n">callbacks</span>, <span class="n">default_plots</span>=<span class="n">default_plots</span>)

        <span class="k">return</span> <span class="n">train_context</span>
</pre></div>


</details>
<hr />
<h4 id="ancestors-in-mro">Ancestors (in MRO)</h4>
<ul>
<li>easyagents.agents.EasyAgent</li>
<li>abc.ABC</li>
</ul>
<h4 id="methods">Methods</h4>
<h5 id="evaluate">evaluate</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">num_episodes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span>
<span class="p">)</span>
</pre></div>


<p>Plays num_episodes with the current policy and computes metrics on rewards.</p>
<p>Args:
    num_episodes: number of episodes to play
    max_steps_per_episode: max steps per episode</p>
<p>Returns:
    extensible score metrics</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">evaluate</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="nl">num_episodes</span><span class="p">:</span><span class="w"> </span><span class="nc">int</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">50</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="nl">max_steps_per_episode</span><span class="p">:</span><span class="w"> </span><span class="nc">int</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">50</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">        </span><span class="ss">&quot;&quot;&quot;Plays num_episodes with the current policy and computes metrics on rewards.</span>

<span class="ss">        Args:</span>

<span class="ss">            num_episodes: number of episodes to play</span>

<span class="ss">            max_steps_per_episode: max steps per episode</span>

<span class="ss">        Returns:</span>

<span class="ss">            extensible score metrics</span>

<span class="ss">        &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">        </span><span class="n">play_context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">core</span><span class="p">.</span><span class="n">PlayContext</span><span class="p">()</span><span class="w"></span>

<span class="w">        </span><span class="n">play_context</span><span class="p">.</span><span class="n">max_steps_per_episode</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">max_steps_per_episode</span><span class="w"></span>

<span class="w">        </span><span class="n">play_context</span><span class="p">.</span><span class="n">num_episodes</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">num_episodes</span><span class="w"></span>

<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">play</span><span class="p">(</span><span class="n">play_context</span><span class="o">=</span><span class="n">play_context</span><span class="p">,</span><span class="w"> </span><span class="n">default_plots</span><span class="o">=</span><span class="k">False</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">Metrics</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;Metrics&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;steps rewards&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">Rewards</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;Rewards&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;mean std min max all&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">all_rewards</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">list</span><span class="p">(</span><span class="n">play_context</span><span class="p">.</span><span class="n">sum_of_rewards</span><span class="p">.</span><span class="k">values</span><span class="p">())</span><span class="w"></span>

<span class="w">        </span><span class="n">mean_reward</span><span class="p">,</span><span class="w"> </span><span class="n">std_reward</span><span class="p">,</span><span class="w"> </span><span class="n">min_reward</span><span class="p">,</span><span class="w"> </span><span class="n">max_reward</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">statistics</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">all_rewards</span><span class="p">),</span><span class="w"> </span><span class="k">statistics</span><span class="p">.</span><span class="nf">stdev</span><span class="p">(</span><span class="w"></span>

<span class="w">            </span><span class="n">all_rewards</span><span class="p">),</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">all_rewards</span><span class="p">),</span><span class="w"> </span><span class="nf">max</span><span class="p">(</span><span class="n">all_rewards</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">rewards</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Rewards</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mean_reward</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">=</span><span class="n">std_reward</span><span class="p">,</span><span class="w"> </span><span class="nf">min</span><span class="o">=</span><span class="n">min_reward</span><span class="p">,</span><span class="w"> </span><span class="nf">max</span><span class="o">=</span><span class="n">max_reward</span><span class="p">,</span><span class="w"> </span><span class="ow">all</span><span class="o">=</span><span class="n">all_rewards</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">Steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;Steps&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;mean std min max all&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">all_num_steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">[]</span><span class="w"></span>

<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">play_context</span><span class="p">.</span><span class="n">rewards</span><span class="p">.</span><span class="n">keys</span><span class="p">()</span><span class="err">:</span><span class="w"></span>

<span class="w">            </span><span class="n">all_num_steps</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">play_context</span><span class="p">.</span><span class="n">rewards</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="p">))</span><span class="w"></span>

<span class="w">        </span><span class="n">mean_steps</span><span class="p">,</span><span class="w"> </span><span class="n">std_steps</span><span class="p">,</span><span class="w"> </span><span class="n">min_steps</span><span class="p">,</span><span class="w"> </span><span class="n">max_steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">statistics</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">all_num_steps</span><span class="p">),</span><span class="w"> </span><span class="k">statistics</span><span class="p">.</span><span class="nf">stdev</span><span class="p">(</span><span class="w"></span>

<span class="w">            </span><span class="n">all_num_steps</span><span class="p">),</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">all_num_steps</span><span class="p">),</span><span class="w"> </span><span class="nf">max</span><span class="p">(</span><span class="n">all_num_steps</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Steps</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mean_steps</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">=</span><span class="n">std_steps</span><span class="p">,</span><span class="w"> </span><span class="nf">min</span><span class="o">=</span><span class="n">min_steps</span><span class="p">,</span><span class="w"> </span><span class="nf">max</span><span class="o">=</span><span class="n">max_steps</span><span class="p">,</span><span class="w"> </span><span class="ow">all</span><span class="o">=</span><span class="n">all_num_steps</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">metrics</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Metrics</span><span class="p">(</span><span class="n">rewards</span><span class="o">=</span><span class="n">rewards</span><span class="p">,</span><span class="w"> </span><span class="n">steps</span><span class="o">=</span><span class="n">steps</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">metrics</span><span class="w"></span>
</pre></div>


</details>
<h5 id="play">play</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">play</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_episodes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="n">play_context</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">PlayContext</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">default_plots</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span>
</pre></div>


<p>Plays num_episodes with the current policy.</p>
<p>Args:
    callbacks: list of callbacks called during each episode play
    num_episodes: number of episodes to play
    max_steps_per_episode: max steps per episode
    play_context: play configuration to be used. If set override all other play context arguments
    default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...)</p>
<p>Returns:
    play_context containg the actions taken and the rewards received during training</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="n">def</span> <span class="n">play</span><span class="p">(</span><span class="k">self</span><span class="p">,</span>

             <span class="n">callbacks</span><span class="p">:</span> <span class="k">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="k">None</span><span class="p">]</span> <span class="o">=</span> <span class="k">None</span><span class="p">,</span>

             <span class="n">num_episodes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>

             <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>

             <span class="n">play_context</span><span class="p">:</span> <span class="n">core</span><span class="p">.</span><span class="n">PlayContext</span> <span class="o">=</span> <span class="k">None</span><span class="p">,</span>

             <span class="n">default_plots</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="k">None</span><span class="p">):</span>

        <span class="ss">&quot;&quot;&quot;Plays num_episodes with the current policy.</span>

<span class="ss">        Args:</span>

<span class="ss">            callbacks: list of callbacks called during each episode play</span>

<span class="ss">            num_episodes: number of episodes to play</span>

<span class="ss">            max_steps_per_episode: max steps per episode</span>

<span class="ss">            play_context: play configuration to be used. If set override all other play context arguments</span>

<span class="ss">            default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...)</span>

<span class="ss">        Returns:</span>

<span class="ss">            play_context containg the actions taken and the rewards received during training</span>

<span class="ss">        &quot;&quot;&quot;</span>

        <span class="n">assert</span> <span class="k">self</span><span class="p">.</span><span class="n">_backend_agent</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">_is_policy_trained</span><span class="p">,</span> <span class="ss">&quot;No trained policy available. Call train() first.&quot;</span>

        <span class="k">if</span> <span class="n">play_context</span> <span class="k">is</span> <span class="k">None</span><span class="p">:</span>

            <span class="n">play_context</span> <span class="o">=</span> <span class="n">core</span><span class="p">.</span><span class="n">PlayContext</span><span class="p">()</span>

            <span class="n">play_context</span><span class="p">.</span><span class="n">max_steps_per_episode</span> <span class="o">=</span> <span class="n">max_steps_per_episode</span>

            <span class="n">play_context</span><span class="p">.</span><span class="n">num_episodes</span> <span class="o">=</span> <span class="n">num_episodes</span>

        <span class="n">callbacks</span> <span class="o">=</span> <span class="k">self</span><span class="p">.</span><span class="n">_to_callback_list</span><span class="p">(</span><span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span>

        <span class="n">callbacks</span> <span class="o">=</span> <span class="k">self</span><span class="p">.</span><span class="n">_add_plot_callbacks</span><span class="p">(</span><span class="n">callbacks</span><span class="p">,</span> <span class="n">default_plots</span><span class="p">,</span> <span class="p">[</span><span class="n">plot</span><span class="p">.</span><span class="n">Steps</span><span class="p">(),</span> <span class="n">plot</span><span class="p">.</span><span class="n">Rewards</span><span class="p">()])</span>

        <span class="k">self</span><span class="p">.</span><span class="n">_backend_agent</span><span class="p">.</span><span class="n">play</span><span class="p">(</span><span class="n">play_context</span><span class="o">=</span><span class="n">play_context</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">play_context</span>
</pre></div>


</details>
<h5 id="save">save</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">save</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span>
</pre></div>


<p>Saves the currently trained actor policy in directory.</p>
<p>If save is called before a trained policy is created, eg by calling train, an exception is raised.</p>
<p>Args:
     directory: the directory to save the policy weights to.
        if the directory does not exist yet, a new directory is created. if None the policy is saved
        in a temp directory.
     callbacks: list of callbacks called during save (eg log.Agent)</p>
<p>Returns:
    the absolute path to the directory containing the saved policy.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="k">save</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="nl">directory</span><span class="p">:</span><span class="w"> </span><span class="n">Optional</span><span class="o">[</span><span class="n">str</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">None</span><span class="p">,</span><span class="w"></span>

<span class="w">             </span><span class="nl">callbacks</span><span class="p">:</span><span class="w"> </span><span class="ow">Union</span><span class="o">[</span><span class="n">List[core.AgentCallback</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="err">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">None</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="nf">str</span><span class="err">:</span><span class="w"></span>

<span class="w">        </span><span class="ss">&quot;&quot;&quot;Saves the currently trained actor policy in directory.</span>

<span class="ss">        If save is called before a trained policy is created, eg by calling train, an exception is raised.</span>

<span class="ss">        Args:</span>

<span class="ss">             directory: the directory to save the policy weights to.</span>

<span class="ss">                if the directory does not exist yet, a new directory is created. if None the policy is saved</span>

<span class="ss">                in a temp directory.</span>

<span class="ss">             callbacks: list of callbacks called during save (eg log.Agent)</span>

<span class="ss">        Returns:</span>

<span class="ss">            the absolute path to the directory containing the saved policy.</span>

<span class="ss">        &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">directory</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">None</span><span class="err">:</span><span class="w"></span>

<span class="w">            </span><span class="n">directory</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bcore</span><span class="p">.</span><span class="n">_get_temp_path</span><span class="p">()</span><span class="w"></span>

<span class="w">        </span><span class="n">assert</span><span class="w"> </span><span class="n">directory</span><span class="w"></span>

<span class="w">        </span><span class="n">assert</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_backend_agent</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">_is_policy_trained</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;No trained policy available. Call train() first.&quot;</span><span class="w"></span>

<span class="w">        </span><span class="n">directory</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bcore</span><span class="p">.</span><span class="n">_mkdir</span><span class="p">(</span><span class="n">directory</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">agent_json_path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">os</span><span class="p">.</span><span class="k">path</span><span class="p">.</span><span class="k">join</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span><span class="w"> </span><span class="n">EasyAgent</span><span class="p">.</span><span class="n">_KEY_EASYAGENT_FILENAME</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">with</span><span class="w"> </span><span class="k">open</span><span class="p">(</span><span class="n">agent_json_path</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;w&#39;</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nl">jsonfile</span><span class="p">:</span><span class="w"></span>

<span class="w">            </span><span class="n">agent_dict</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_to_dict</span><span class="p">()</span><span class="w"></span>

<span class="w">            </span><span class="n">json</span><span class="p">.</span><span class="k">dump</span><span class="p">(</span><span class="n">agent_dict</span><span class="p">,</span><span class="w"> </span><span class="n">jsonfile</span><span class="p">,</span><span class="w"> </span><span class="n">sort_keys</span><span class="o">=</span><span class="k">True</span><span class="p">,</span><span class="w"> </span><span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">callbacks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_to_callback_list</span><span class="p">(</span><span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">policy_directory</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">os</span><span class="p">.</span><span class="k">path</span><span class="p">.</span><span class="k">join</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span><span class="w"> </span><span class="n">EasyAgent</span><span class="p">.</span><span class="n">_KEY_POLICY_DIRECTORY</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">policy_directory</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bcore</span><span class="p">.</span><span class="n">_mkdir</span><span class="p">(</span><span class="n">policy_directory</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">_backend_agent</span><span class="p">.</span><span class="k">save</span><span class="p">(</span><span class="n">directory</span><span class="o">=</span><span class="n">policy_directory</span><span class="p">,</span><span class="w"> </span><span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">directory</span><span class="w"></span>
</pre></div>


</details>
<h5 id="train">train</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">num_episodes_per_iteration</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span>
    <span class="n">elite_set_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">num_iterations_between_eval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
    <span class="n">num_episodes_per_eval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">train_context</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">CemTrainContext</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">default_plots</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span>
</pre></div>


<p>Trains a new model using the gym environment passed during instantiation.</p>
<p>Args:
    callbacks: list of callbacks called during training and evaluation
    num_iterations: number of times the training is repeated (with additional data)
    num_episodes_per_iteration: number of episodes played in each iteration. for each episode a new
        policy is sampled from the current weight distribution.
    max_steps_per_episode: maximum number of steps per episode
    elite_set_fraction: the fraction of policies which are members of the elite set.
        These policies are used to fit a new weight distribution in each iteration.
    num_iterations_between_eval: number of training iterations before the current policy is evaluated.
        if 0 no evaluation is performed.
    num_episodes_per_eval: number of episodes played to estimate the average return and steps
    train_context: training configuration to be used. if set overrides all other training context arguments.
    default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...).
        if None default callbacks are only added if the callbacks list is empty</p>
<p>Returns:
    train_context: the training configuration containing the loss and sum of rewards encountered
        during training</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="n">def</span> <span class="n">train</span><span class="p">(</span><span class="k">self</span><span class="p">,</span>

              <span class="n">callbacks</span><span class="p">:</span> <span class="k">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="k">None</span><span class="p">]</span> <span class="o">=</span> <span class="k">None</span><span class="p">,</span>

              <span class="n">num_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>

              <span class="n">num_episodes_per_iteration</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>

              <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span>

              <span class="n">elite_set_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">0</span><span class="p">.</span><span class="mi">1</span><span class="p">,</span>

              <span class="n">num_iterations_between_eval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>

              <span class="n">num_episodes_per_eval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>

              <span class="n">train_context</span><span class="p">:</span> <span class="n">core</span><span class="p">.</span><span class="n">CemTrainContext</span> <span class="o">=</span> <span class="k">None</span><span class="p">,</span>

              <span class="n">default_plots</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="k">None</span><span class="p">):</span>

        <span class="ss">&quot;&quot;&quot;Trains a new model using the gym environment passed during instantiation.</span>

<span class="ss">        Args:</span>

<span class="ss">            callbacks: list of callbacks called during training and evaluation</span>

<span class="ss">            num_iterations: number of times the training is repeated (with additional data)</span>

<span class="ss">            num_episodes_per_iteration: number of episodes played in each iteration. for each episode a new</span>

<span class="ss">                policy is sampled from the current weight distribution.</span>

<span class="ss">            max_steps_per_episode: maximum number of steps per episode</span>

<span class="ss">            elite_set_fraction: the fraction of policies which are members of the elite set.</span>

<span class="ss">                These policies are used to fit a new weight distribution in each iteration.</span>

<span class="ss">            num_iterations_between_eval: number of training iterations before the current policy is evaluated.</span>

<span class="ss">                if 0 no evaluation is performed.</span>

<span class="ss">            num_episodes_per_eval: number of episodes played to estimate the average return and steps</span>

<span class="ss">            train_context: training configuration to be used. if set overrides all other training context arguments.</span>

<span class="ss">            default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...).</span>

<span class="ss">                if None default callbacks are only added if the callbacks list is empty</span>

<span class="ss">        Returns:</span>

<span class="ss">            train_context: the training configuration containing the loss and sum of rewards encountered</span>

<span class="ss">                during training</span>

<span class="ss">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">train_context</span> <span class="k">is</span> <span class="k">None</span><span class="p">:</span>

            <span class="n">train_context</span> <span class="o">=</span> <span class="n">core</span><span class="p">.</span><span class="n">CemTrainContext</span><span class="p">()</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">num_iterations</span> <span class="o">=</span> <span class="n">num_iterations</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">max_steps_per_episode</span> <span class="o">=</span> <span class="n">max_steps_per_episode</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">elite_set_fraction</span> <span class="o">=</span> <span class="n">elite_set_fraction</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">num_iterations_between_eval</span> <span class="o">=</span> <span class="n">num_iterations_between_eval</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">num_episodes_per_eval</span> <span class="o">=</span> <span class="n">num_episodes_per_eval</span>

        <span class="n">super</span><span class="p">().</span><span class="n">train</span><span class="p">(</span><span class="n">train_context</span><span class="o">=</span><span class="n">train_context</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span> <span class="n">default_plots</span><span class="o">=</span><span class="n">default_plots</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">train_context</span>
</pre></div>


</details>
<h3 id="doubledqnagent">DoubleDqnAgent</h3>
<div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">DoubleDqnAgent</span><span class="p">(</span>
    <span class="n">gym_env_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">fc_layers</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="nb">int</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">backend</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span>
</pre></div>


<p>Agent based on the Double Dqn algorithm (https://arxiv.org/abs/1509.06461)</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><span class="k">class</span> <span class="n">DoubleDqnAgent</span>(<span class="n">DqnAgent</span>):

    <span class="s">&quot;&quot;&quot;Agent based on the Double Dqn algorithm (https://arxiv.org/abs/1509.06461)&quot;&quot;&quot;</span>
</pre></div>


</details>
<hr />
<h4 id="ancestors-in-mro_1">Ancestors (in MRO)</h4>
<ul>
<li>easyagents.agents.DqnAgent</li>
<li>easyagents.agents.EasyAgent</li>
<li>abc.ABC</li>
</ul>
<h4 id="methods_1">Methods</h4>
<h5 id="evaluate_1">evaluate</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">num_episodes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span>
<span class="p">)</span>
</pre></div>


<p>Plays num_episodes with the current policy and computes metrics on rewards.</p>
<p>Args:
    num_episodes: number of episodes to play
    max_steps_per_episode: max steps per episode</p>
<p>Returns:
    extensible score metrics</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">evaluate</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="nl">num_episodes</span><span class="p">:</span><span class="w"> </span><span class="nc">int</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">50</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="nl">max_steps_per_episode</span><span class="p">:</span><span class="w"> </span><span class="nc">int</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">50</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">        </span><span class="ss">&quot;&quot;&quot;Plays num_episodes with the current policy and computes metrics on rewards.</span>

<span class="ss">        Args:</span>

<span class="ss">            num_episodes: number of episodes to play</span>

<span class="ss">            max_steps_per_episode: max steps per episode</span>

<span class="ss">        Returns:</span>

<span class="ss">            extensible score metrics</span>

<span class="ss">        &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">        </span><span class="n">play_context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">core</span><span class="p">.</span><span class="n">PlayContext</span><span class="p">()</span><span class="w"></span>

<span class="w">        </span><span class="n">play_context</span><span class="p">.</span><span class="n">max_steps_per_episode</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">max_steps_per_episode</span><span class="w"></span>

<span class="w">        </span><span class="n">play_context</span><span class="p">.</span><span class="n">num_episodes</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">num_episodes</span><span class="w"></span>

<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">play</span><span class="p">(</span><span class="n">play_context</span><span class="o">=</span><span class="n">play_context</span><span class="p">,</span><span class="w"> </span><span class="n">default_plots</span><span class="o">=</span><span class="k">False</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">Metrics</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;Metrics&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;steps rewards&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">Rewards</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;Rewards&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;mean std min max all&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">all_rewards</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">list</span><span class="p">(</span><span class="n">play_context</span><span class="p">.</span><span class="n">sum_of_rewards</span><span class="p">.</span><span class="k">values</span><span class="p">())</span><span class="w"></span>

<span class="w">        </span><span class="n">mean_reward</span><span class="p">,</span><span class="w"> </span><span class="n">std_reward</span><span class="p">,</span><span class="w"> </span><span class="n">min_reward</span><span class="p">,</span><span class="w"> </span><span class="n">max_reward</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">statistics</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">all_rewards</span><span class="p">),</span><span class="w"> </span><span class="k">statistics</span><span class="p">.</span><span class="nf">stdev</span><span class="p">(</span><span class="w"></span>

<span class="w">            </span><span class="n">all_rewards</span><span class="p">),</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">all_rewards</span><span class="p">),</span><span class="w"> </span><span class="nf">max</span><span class="p">(</span><span class="n">all_rewards</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">rewards</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Rewards</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mean_reward</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">=</span><span class="n">std_reward</span><span class="p">,</span><span class="w"> </span><span class="nf">min</span><span class="o">=</span><span class="n">min_reward</span><span class="p">,</span><span class="w"> </span><span class="nf">max</span><span class="o">=</span><span class="n">max_reward</span><span class="p">,</span><span class="w"> </span><span class="ow">all</span><span class="o">=</span><span class="n">all_rewards</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">Steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;Steps&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;mean std min max all&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">all_num_steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">[]</span><span class="w"></span>

<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">play_context</span><span class="p">.</span><span class="n">rewards</span><span class="p">.</span><span class="n">keys</span><span class="p">()</span><span class="err">:</span><span class="w"></span>

<span class="w">            </span><span class="n">all_num_steps</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">play_context</span><span class="p">.</span><span class="n">rewards</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="p">))</span><span class="w"></span>

<span class="w">        </span><span class="n">mean_steps</span><span class="p">,</span><span class="w"> </span><span class="n">std_steps</span><span class="p">,</span><span class="w"> </span><span class="n">min_steps</span><span class="p">,</span><span class="w"> </span><span class="n">max_steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">statistics</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">all_num_steps</span><span class="p">),</span><span class="w"> </span><span class="k">statistics</span><span class="p">.</span><span class="nf">stdev</span><span class="p">(</span><span class="w"></span>

<span class="w">            </span><span class="n">all_num_steps</span><span class="p">),</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">all_num_steps</span><span class="p">),</span><span class="w"> </span><span class="nf">max</span><span class="p">(</span><span class="n">all_num_steps</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Steps</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mean_steps</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">=</span><span class="n">std_steps</span><span class="p">,</span><span class="w"> </span><span class="nf">min</span><span class="o">=</span><span class="n">min_steps</span><span class="p">,</span><span class="w"> </span><span class="nf">max</span><span class="o">=</span><span class="n">max_steps</span><span class="p">,</span><span class="w"> </span><span class="ow">all</span><span class="o">=</span><span class="n">all_num_steps</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">metrics</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Metrics</span><span class="p">(</span><span class="n">rewards</span><span class="o">=</span><span class="n">rewards</span><span class="p">,</span><span class="w"> </span><span class="n">steps</span><span class="o">=</span><span class="n">steps</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">metrics</span><span class="w"></span>
</pre></div>


</details>
<h5 id="play_1">play</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">play</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_episodes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="n">play_context</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">PlayContext</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">default_plots</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span>
</pre></div>


<p>Plays num_episodes with the current policy.</p>
<p>Args:
    callbacks: list of callbacks called during each episode play
    num_episodes: number of episodes to play
    max_steps_per_episode: max steps per episode
    play_context: play configuration to be used. If set override all other play context arguments
    default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...)</p>
<p>Returns:
    play_context containg the actions taken and the rewards received during training</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="n">def</span> <span class="n">play</span><span class="p">(</span><span class="k">self</span><span class="p">,</span>

             <span class="n">callbacks</span><span class="p">:</span> <span class="k">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="k">None</span><span class="p">]</span> <span class="o">=</span> <span class="k">None</span><span class="p">,</span>

             <span class="n">num_episodes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>

             <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>

             <span class="n">play_context</span><span class="p">:</span> <span class="n">core</span><span class="p">.</span><span class="n">PlayContext</span> <span class="o">=</span> <span class="k">None</span><span class="p">,</span>

             <span class="n">default_plots</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="k">None</span><span class="p">):</span>

        <span class="ss">&quot;&quot;&quot;Plays num_episodes with the current policy.</span>

<span class="ss">        Args:</span>

<span class="ss">            callbacks: list of callbacks called during each episode play</span>

<span class="ss">            num_episodes: number of episodes to play</span>

<span class="ss">            max_steps_per_episode: max steps per episode</span>

<span class="ss">            play_context: play configuration to be used. If set override all other play context arguments</span>

<span class="ss">            default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...)</span>

<span class="ss">        Returns:</span>

<span class="ss">            play_context containg the actions taken and the rewards received during training</span>

<span class="ss">        &quot;&quot;&quot;</span>

        <span class="n">assert</span> <span class="k">self</span><span class="p">.</span><span class="n">_backend_agent</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">_is_policy_trained</span><span class="p">,</span> <span class="ss">&quot;No trained policy available. Call train() first.&quot;</span>

        <span class="k">if</span> <span class="n">play_context</span> <span class="k">is</span> <span class="k">None</span><span class="p">:</span>

            <span class="n">play_context</span> <span class="o">=</span> <span class="n">core</span><span class="p">.</span><span class="n">PlayContext</span><span class="p">()</span>

            <span class="n">play_context</span><span class="p">.</span><span class="n">max_steps_per_episode</span> <span class="o">=</span> <span class="n">max_steps_per_episode</span>

            <span class="n">play_context</span><span class="p">.</span><span class="n">num_episodes</span> <span class="o">=</span> <span class="n">num_episodes</span>

        <span class="n">callbacks</span> <span class="o">=</span> <span class="k">self</span><span class="p">.</span><span class="n">_to_callback_list</span><span class="p">(</span><span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span>

        <span class="n">callbacks</span> <span class="o">=</span> <span class="k">self</span><span class="p">.</span><span class="n">_add_plot_callbacks</span><span class="p">(</span><span class="n">callbacks</span><span class="p">,</span> <span class="n">default_plots</span><span class="p">,</span> <span class="p">[</span><span class="n">plot</span><span class="p">.</span><span class="n">Steps</span><span class="p">(),</span> <span class="n">plot</span><span class="p">.</span><span class="n">Rewards</span><span class="p">()])</span>

        <span class="k">self</span><span class="p">.</span><span class="n">_backend_agent</span><span class="p">.</span><span class="n">play</span><span class="p">(</span><span class="n">play_context</span><span class="o">=</span><span class="n">play_context</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">play_context</span>
</pre></div>


</details>
<h5 id="save_1">save</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">save</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span>
</pre></div>


<p>Saves the currently trained actor policy in directory.</p>
<p>If save is called before a trained policy is created, eg by calling train, an exception is raised.</p>
<p>Args:
     directory: the directory to save the policy weights to.
        if the directory does not exist yet, a new directory is created. if None the policy is saved
        in a temp directory.
     callbacks: list of callbacks called during save (eg log.Agent)</p>
<p>Returns:
    the absolute path to the directory containing the saved policy.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="k">save</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="nl">directory</span><span class="p">:</span><span class="w"> </span><span class="n">Optional</span><span class="o">[</span><span class="n">str</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">None</span><span class="p">,</span><span class="w"></span>

<span class="w">             </span><span class="nl">callbacks</span><span class="p">:</span><span class="w"> </span><span class="ow">Union</span><span class="o">[</span><span class="n">List[core.AgentCallback</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="err">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">None</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="nf">str</span><span class="err">:</span><span class="w"></span>

<span class="w">        </span><span class="ss">&quot;&quot;&quot;Saves the currently trained actor policy in directory.</span>

<span class="ss">        If save is called before a trained policy is created, eg by calling train, an exception is raised.</span>

<span class="ss">        Args:</span>

<span class="ss">             directory: the directory to save the policy weights to.</span>

<span class="ss">                if the directory does not exist yet, a new directory is created. if None the policy is saved</span>

<span class="ss">                in a temp directory.</span>

<span class="ss">             callbacks: list of callbacks called during save (eg log.Agent)</span>

<span class="ss">        Returns:</span>

<span class="ss">            the absolute path to the directory containing the saved policy.</span>

<span class="ss">        &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">directory</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">None</span><span class="err">:</span><span class="w"></span>

<span class="w">            </span><span class="n">directory</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bcore</span><span class="p">.</span><span class="n">_get_temp_path</span><span class="p">()</span><span class="w"></span>

<span class="w">        </span><span class="n">assert</span><span class="w"> </span><span class="n">directory</span><span class="w"></span>

<span class="w">        </span><span class="n">assert</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_backend_agent</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">_is_policy_trained</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;No trained policy available. Call train() first.&quot;</span><span class="w"></span>

<span class="w">        </span><span class="n">directory</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bcore</span><span class="p">.</span><span class="n">_mkdir</span><span class="p">(</span><span class="n">directory</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">agent_json_path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">os</span><span class="p">.</span><span class="k">path</span><span class="p">.</span><span class="k">join</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span><span class="w"> </span><span class="n">EasyAgent</span><span class="p">.</span><span class="n">_KEY_EASYAGENT_FILENAME</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">with</span><span class="w"> </span><span class="k">open</span><span class="p">(</span><span class="n">agent_json_path</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;w&#39;</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nl">jsonfile</span><span class="p">:</span><span class="w"></span>

<span class="w">            </span><span class="n">agent_dict</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_to_dict</span><span class="p">()</span><span class="w"></span>

<span class="w">            </span><span class="n">json</span><span class="p">.</span><span class="k">dump</span><span class="p">(</span><span class="n">agent_dict</span><span class="p">,</span><span class="w"> </span><span class="n">jsonfile</span><span class="p">,</span><span class="w"> </span><span class="n">sort_keys</span><span class="o">=</span><span class="k">True</span><span class="p">,</span><span class="w"> </span><span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">callbacks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_to_callback_list</span><span class="p">(</span><span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">policy_directory</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">os</span><span class="p">.</span><span class="k">path</span><span class="p">.</span><span class="k">join</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span><span class="w"> </span><span class="n">EasyAgent</span><span class="p">.</span><span class="n">_KEY_POLICY_DIRECTORY</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">policy_directory</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bcore</span><span class="p">.</span><span class="n">_mkdir</span><span class="p">(</span><span class="n">policy_directory</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">_backend_agent</span><span class="p">.</span><span class="k">save</span><span class="p">(</span><span class="n">directory</span><span class="o">=</span><span class="n">policy_directory</span><span class="p">,</span><span class="w"> </span><span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">directory</span><span class="w"></span>
</pre></div>


</details>
<h5 id="train_1">train</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20000</span><span class="p">,</span>
    <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span>
    <span class="n">num_steps_per_iteration</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">num_steps_buffer_preload</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">num_steps_sampled_from_buffer</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="n">num_iterations_between_eval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="n">num_episodes_per_eval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">,</span>
    <span class="n">train_context</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">StepsTrainContext</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">default_plots</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span>
</pre></div>


<p>Trains a new model using the gym environment passed during instantiation.</p>
<p>Args:
    callbacks: list of callbacks called during training and evaluation
    num_iterations: number of times the training is repeated (with additional data)
    max_steps_per_episode: maximum number of steps per episode
    num_steps_per_iteration: number of steps played per training iteration
    num_steps_buffer_preload: number of initial collect steps to preload the buffer
    num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training
    num_iterations_between_eval: number of training iterations before the current policy is evaluated.
        if 0 no evaluation is performed.
    num_episodes_per_eval: number of episodes played to estimate the average return and steps
    learning_rate: the learning rate used in the next iteration's policy training (0,1]
    train_context: training configuration to be used. if set overrides all other training context arguments.
    default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...).
        if None default callbacks are only added if the callbacks list is empty</p>
<p>Returns:
    train_context: the training configuration containing the loss and sum of rewards encountered
        during training</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="n">def</span> <span class="n">train</span><span class="p">(</span><span class="k">self</span><span class="p">,</span>

              <span class="n">callbacks</span><span class="p">:</span> <span class="k">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="k">None</span><span class="p">]</span> <span class="o">=</span> <span class="k">None</span><span class="p">,</span>

              <span class="n">num_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20000</span><span class="p">,</span>

              <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span>

              <span class="n">num_steps_per_iteration</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>

              <span class="n">num_steps_buffer_preload</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>

              <span class="n">num_steps_sampled_from_buffer</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>

              <span class="n">num_iterations_between_eval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>

              <span class="n">num_episodes_per_eval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>

              <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">0</span><span class="p">.</span><span class="mi">001</span><span class="p">,</span>

              <span class="n">train_context</span><span class="p">:</span> <span class="n">core</span><span class="p">.</span><span class="n">StepsTrainContext</span> <span class="o">=</span> <span class="k">None</span><span class="p">,</span>

              <span class="n">default_plots</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="k">None</span><span class="p">):</span>

        <span class="ss">&quot;&quot;&quot;Trains a new model using the gym environment passed during instantiation.</span>

<span class="ss">        Args:</span>

<span class="ss">            callbacks: list of callbacks called during training and evaluation</span>

<span class="ss">            num_iterations: number of times the training is repeated (with additional data)</span>

<span class="ss">            max_steps_per_episode: maximum number of steps per episode</span>

<span class="ss">            num_steps_per_iteration: number of steps played per training iteration</span>

<span class="ss">            num_steps_buffer_preload: number of initial collect steps to preload the buffer</span>

<span class="ss">            num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training</span>

<span class="ss">            num_iterations_between_eval: number of training iterations before the current policy is evaluated.</span>

<span class="ss">                if 0 no evaluation is performed.</span>

<span class="ss">            num_episodes_per_eval: number of episodes played to estimate the average return and steps</span>

<span class="ss">            learning_rate: the learning rate used in the next iteration&#39;s policy training (0,1]</span>

<span class="ss">            train_context: training configuration to be used. if set overrides all other training context arguments.</span>

<span class="ss">            default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...).</span>

<span class="ss">                if None default callbacks are only added if the callbacks list is empty</span>

<span class="ss">        Returns:</span>

<span class="ss">            train_context: the training configuration containing the loss and sum of rewards encountered</span>

<span class="ss">                during training</span>

<span class="ss">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">train_context</span> <span class="k">is</span> <span class="k">None</span><span class="p">:</span>

            <span class="n">train_context</span> <span class="o">=</span> <span class="n">core</span><span class="p">.</span><span class="n">StepsTrainContext</span><span class="p">()</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">num_iterations</span> <span class="o">=</span> <span class="n">num_iterations</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">max_steps_per_episode</span> <span class="o">=</span> <span class="n">max_steps_per_episode</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">num_steps_per_iteration</span> <span class="o">=</span> <span class="n">num_steps_per_iteration</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">num_steps_buffer_preload</span> <span class="o">=</span> <span class="n">num_steps_buffer_preload</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">num_steps_sampled_from_buffer</span> <span class="o">=</span> <span class="n">num_steps_sampled_from_buffer</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">num_iterations_between_eval</span> <span class="o">=</span> <span class="n">num_iterations_between_eval</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">num_episodes_per_eval</span> <span class="o">=</span> <span class="n">num_episodes_per_eval</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>

        <span class="n">super</span><span class="p">().</span><span class="n">train</span><span class="p">(</span><span class="n">train_context</span><span class="o">=</span><span class="n">train_context</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span> <span class="n">default_plots</span><span class="o">=</span><span class="n">default_plots</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">train_context</span>
</pre></div>


</details>
<h3 id="dqnagent">DqnAgent</h3>
<div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">DqnAgent</span><span class="p">(</span>
    <span class="n">gym_env_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">fc_layers</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="nb">int</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">backend</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span>
</pre></div>


<p>creates a new agent based on the Dqn algorithm.</p>
<p>From wikipedia:
The DeepMind system used a deep convolutional neural network, with layers of tiled convolutional filters to mimic
the effects of receptive fields. Reinforcement learning is unstable or divergent when a nonlinear function
approximator such as a neural network is used to represent Q.
This instability comes from the correlations present in the sequence of observations, the fact that small updates
to Q may significantly change the policy and the data distribution, and the correlations between Q and the
target values.</p>
<p>The technique used experience replay, a biologically inspired mechanism that uses a random sample of prior actions
instead of the most recent action to proceed.[2] This removes correlations in the observation sequence and smooths
changes in the data distribution. Iterative update adjusts Q towards target values that are only periodically
updated, further reducing correlations with the target.[17]</p>
<p>see also: https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><span class="k">class</span> <span class="n">DqnAgent</span>(<span class="n">EasyAgent</span>):

    <span class="s">&quot;&quot;&quot;creates a new agent based on the Dqn algorithm.</span>

<span class="s">    From wikipedia:</span>

<span class="s">    The DeepMind system used a deep convolutional neural network, with layers of tiled convolutional filters to mimic</span>

<span class="s">    the effects of receptive fields. Reinforcement learning is unstable or divergent when a nonlinear function</span>

<span class="s">    approximator such as a neural network is used to represent Q.</span>

<span class="s">    This instability comes from the correlations present in the sequence of observations, the fact that small updates</span>

<span class="s">    to Q may significantly change the policy and the data distribution, and the correlations between Q and the</span>

<span class="s">    target values.</span>

<span class="s">    The technique used experience replay, a biologically inspired mechanism that uses a random sample of prior actions</span>

<span class="s">    instead of the most recent action to proceed.[2] This removes correlations in the observation sequence and smooths</span>

<span class="s">    changes in the data distribution. Iterative update adjusts Q towards target values that are only periodically</span>

<span class="s">    updated, further reducing correlations with the target.[17]</span>

<span class="s">    see also: https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning</span>

<span class="s">    &quot;&quot;&quot;</span>

    <span class="n">def</span> <span class="n">train</span>(<span class="k">self</span>,

              <span class="n">callbacks:</span> <span class="n">Union</span>[<span class="nb">List</span>[<span class="n">core</span>.<span class="n">AgentCallback</span>], <span class="n">core</span>.<span class="n">AgentCallback</span>, <span class="n">None</span>] = <span class="n">None</span>,

              <span class="n">num_iterations:</span> <span class="nb">int</span> = <span class="mi">20000</span>,

              <span class="n">max_steps_per_episode:</span> <span class="nb">int</span> = <span class="mi">500</span>,

              <span class="n">num_steps_per_iteration:</span> <span class="nb">int</span> = <span class="mi">1</span>,

              <span class="n">num_steps_buffer_preload</span>=<span class="mi">1000</span>,

              <span class="n">num_steps_sampled_from_buffer</span>=<span class="mi">64</span>,

              <span class="n">num_iterations_between_eval:</span> <span class="nb">int</span> = <span class="mi">1000</span>,

              <span class="n">num_episodes_per_eval:</span> <span class="nb">int</span> = <span class="mi">10</span>,

              <span class="n">learning_rate:</span> <span class="n">float</span> = <span class="mf">0.001</span>,

              <span class="n">train_context:</span> <span class="n">core</span>.<span class="n">StepsTrainContext</span> = <span class="n">None</span>,

              <span class="n">default_plots:</span> <span class="nb">bool</span> = <span class="n">None</span>):

        <span class="s">&quot;&quot;&quot;Trains a new model using the gym environment passed during instantiation.</span>

<span class="s">        Args:</span>

<span class="s">            callbacks: list of callbacks called during training and evaluation</span>

<span class="s">            num_iterations: number of times the training is repeated (with additional data)</span>

<span class="s">            max_steps_per_episode: maximum number of steps per episode</span>

<span class="s">            num_steps_per_iteration: number of steps played per training iteration</span>

<span class="s">            num_steps_buffer_preload: number of initial collect steps to preload the buffer</span>

<span class="s">            num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training</span>

<span class="s">            num_iterations_between_eval: number of training iterations before the current policy is evaluated.</span>

<span class="s">                if 0 no evaluation is performed.</span>

<span class="s">            num_episodes_per_eval: number of episodes played to estimate the average return and steps</span>

<span class="s">            learning_rate: the learning rate used in the next iteration&#39;s policy training (0,1]</span>

<span class="s">            train_context: training configuration to be used. if set overrides all other training context arguments.</span>

<span class="s">            default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...).</span>

<span class="s">                if None default callbacks are only added if the callbacks list is empty</span>

<span class="s">        Returns:</span>

<span class="s">            train_context: the training configuration containing the loss and sum of rewards encountered</span>

<span class="s">                during training</span>

<span class="s">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">train_context</span> <span class="k">is</span> <span class="n">None:</span>

            <span class="n">train_context</span> = <span class="n">core</span>.<span class="n">StepsTrainContext</span>()

            <span class="n">train_context</span>.<span class="n">num_iterations</span> = <span class="n">num_iterations</span>

            <span class="n">train_context</span>.<span class="n">max_steps_per_episode</span> = <span class="n">max_steps_per_episode</span>

            <span class="n">train_context</span>.<span class="n">num_steps_per_iteration</span> = <span class="n">num_steps_per_iteration</span>

            <span class="n">train_context</span>.<span class="n">num_steps_buffer_preload</span> = <span class="n">num_steps_buffer_preload</span>

            <span class="n">train_context</span>.<span class="n">num_steps_sampled_from_buffer</span> = <span class="n">num_steps_sampled_from_buffer</span>

            <span class="n">train_context</span>.<span class="n">num_iterations_between_eval</span> = <span class="n">num_iterations_between_eval</span>

            <span class="n">train_context</span>.<span class="n">num_episodes_per_eval</span> = <span class="n">num_episodes_per_eval</span>

            <span class="n">train_context</span>.<span class="n">learning_rate</span> = <span class="n">learning_rate</span>

        <span class="n">super</span>().<span class="n">train</span>(<span class="n">train_context</span>=<span class="n">train_context</span>, <span class="n">callbacks</span>=<span class="n">callbacks</span>, <span class="n">default_plots</span>=<span class="n">default_plots</span>)

        <span class="k">return</span> <span class="n">train_context</span>
</pre></div>


</details>
<hr />
<h4 id="ancestors-in-mro_2">Ancestors (in MRO)</h4>
<ul>
<li>easyagents.agents.EasyAgent</li>
<li>abc.ABC</li>
</ul>
<h4 id="descendants">Descendants</h4>
<ul>
<li>easyagents.agents.DoubleDqnAgent</li>
<li>easyagents.agents.DuelingDqnAgent</li>
<li>easyagents.agents.SacAgent</li>
</ul>
<h4 id="methods_2">Methods</h4>
<h5 id="evaluate_2">evaluate</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">num_episodes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span>
<span class="p">)</span>
</pre></div>


<p>Plays num_episodes with the current policy and computes metrics on rewards.</p>
<p>Args:
    num_episodes: number of episodes to play
    max_steps_per_episode: max steps per episode</p>
<p>Returns:
    extensible score metrics</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">evaluate</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="nl">num_episodes</span><span class="p">:</span><span class="w"> </span><span class="nc">int</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">50</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="nl">max_steps_per_episode</span><span class="p">:</span><span class="w"> </span><span class="nc">int</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">50</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">        </span><span class="ss">&quot;&quot;&quot;Plays num_episodes with the current policy and computes metrics on rewards.</span>

<span class="ss">        Args:</span>

<span class="ss">            num_episodes: number of episodes to play</span>

<span class="ss">            max_steps_per_episode: max steps per episode</span>

<span class="ss">        Returns:</span>

<span class="ss">            extensible score metrics</span>

<span class="ss">        &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">        </span><span class="n">play_context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">core</span><span class="p">.</span><span class="n">PlayContext</span><span class="p">()</span><span class="w"></span>

<span class="w">        </span><span class="n">play_context</span><span class="p">.</span><span class="n">max_steps_per_episode</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">max_steps_per_episode</span><span class="w"></span>

<span class="w">        </span><span class="n">play_context</span><span class="p">.</span><span class="n">num_episodes</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">num_episodes</span><span class="w"></span>

<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">play</span><span class="p">(</span><span class="n">play_context</span><span class="o">=</span><span class="n">play_context</span><span class="p">,</span><span class="w"> </span><span class="n">default_plots</span><span class="o">=</span><span class="k">False</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">Metrics</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;Metrics&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;steps rewards&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">Rewards</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;Rewards&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;mean std min max all&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">all_rewards</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">list</span><span class="p">(</span><span class="n">play_context</span><span class="p">.</span><span class="n">sum_of_rewards</span><span class="p">.</span><span class="k">values</span><span class="p">())</span><span class="w"></span>

<span class="w">        </span><span class="n">mean_reward</span><span class="p">,</span><span class="w"> </span><span class="n">std_reward</span><span class="p">,</span><span class="w"> </span><span class="n">min_reward</span><span class="p">,</span><span class="w"> </span><span class="n">max_reward</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">statistics</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">all_rewards</span><span class="p">),</span><span class="w"> </span><span class="k">statistics</span><span class="p">.</span><span class="nf">stdev</span><span class="p">(</span><span class="w"></span>

<span class="w">            </span><span class="n">all_rewards</span><span class="p">),</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">all_rewards</span><span class="p">),</span><span class="w"> </span><span class="nf">max</span><span class="p">(</span><span class="n">all_rewards</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">rewards</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Rewards</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mean_reward</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">=</span><span class="n">std_reward</span><span class="p">,</span><span class="w"> </span><span class="nf">min</span><span class="o">=</span><span class="n">min_reward</span><span class="p">,</span><span class="w"> </span><span class="nf">max</span><span class="o">=</span><span class="n">max_reward</span><span class="p">,</span><span class="w"> </span><span class="ow">all</span><span class="o">=</span><span class="n">all_rewards</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">Steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;Steps&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;mean std min max all&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">all_num_steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">[]</span><span class="w"></span>

<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">play_context</span><span class="p">.</span><span class="n">rewards</span><span class="p">.</span><span class="n">keys</span><span class="p">()</span><span class="err">:</span><span class="w"></span>

<span class="w">            </span><span class="n">all_num_steps</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">play_context</span><span class="p">.</span><span class="n">rewards</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="p">))</span><span class="w"></span>

<span class="w">        </span><span class="n">mean_steps</span><span class="p">,</span><span class="w"> </span><span class="n">std_steps</span><span class="p">,</span><span class="w"> </span><span class="n">min_steps</span><span class="p">,</span><span class="w"> </span><span class="n">max_steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">statistics</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">all_num_steps</span><span class="p">),</span><span class="w"> </span><span class="k">statistics</span><span class="p">.</span><span class="nf">stdev</span><span class="p">(</span><span class="w"></span>

<span class="w">            </span><span class="n">all_num_steps</span><span class="p">),</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">all_num_steps</span><span class="p">),</span><span class="w"> </span><span class="nf">max</span><span class="p">(</span><span class="n">all_num_steps</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Steps</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mean_steps</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">=</span><span class="n">std_steps</span><span class="p">,</span><span class="w"> </span><span class="nf">min</span><span class="o">=</span><span class="n">min_steps</span><span class="p">,</span><span class="w"> </span><span class="nf">max</span><span class="o">=</span><span class="n">max_steps</span><span class="p">,</span><span class="w"> </span><span class="ow">all</span><span class="o">=</span><span class="n">all_num_steps</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">metrics</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Metrics</span><span class="p">(</span><span class="n">rewards</span><span class="o">=</span><span class="n">rewards</span><span class="p">,</span><span class="w"> </span><span class="n">steps</span><span class="o">=</span><span class="n">steps</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">metrics</span><span class="w"></span>
</pre></div>


</details>
<h5 id="play_2">play</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">play</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_episodes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="n">play_context</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">PlayContext</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">default_plots</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span>
</pre></div>


<p>Plays num_episodes with the current policy.</p>
<p>Args:
    callbacks: list of callbacks called during each episode play
    num_episodes: number of episodes to play
    max_steps_per_episode: max steps per episode
    play_context: play configuration to be used. If set override all other play context arguments
    default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...)</p>
<p>Returns:
    play_context containg the actions taken and the rewards received during training</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="n">def</span> <span class="n">play</span><span class="p">(</span><span class="k">self</span><span class="p">,</span>

             <span class="n">callbacks</span><span class="p">:</span> <span class="k">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="k">None</span><span class="p">]</span> <span class="o">=</span> <span class="k">None</span><span class="p">,</span>

             <span class="n">num_episodes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>

             <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>

             <span class="n">play_context</span><span class="p">:</span> <span class="n">core</span><span class="p">.</span><span class="n">PlayContext</span> <span class="o">=</span> <span class="k">None</span><span class="p">,</span>

             <span class="n">default_plots</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="k">None</span><span class="p">):</span>

        <span class="ss">&quot;&quot;&quot;Plays num_episodes with the current policy.</span>

<span class="ss">        Args:</span>

<span class="ss">            callbacks: list of callbacks called during each episode play</span>

<span class="ss">            num_episodes: number of episodes to play</span>

<span class="ss">            max_steps_per_episode: max steps per episode</span>

<span class="ss">            play_context: play configuration to be used. If set override all other play context arguments</span>

<span class="ss">            default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...)</span>

<span class="ss">        Returns:</span>

<span class="ss">            play_context containg the actions taken and the rewards received during training</span>

<span class="ss">        &quot;&quot;&quot;</span>

        <span class="n">assert</span> <span class="k">self</span><span class="p">.</span><span class="n">_backend_agent</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">_is_policy_trained</span><span class="p">,</span> <span class="ss">&quot;No trained policy available. Call train() first.&quot;</span>

        <span class="k">if</span> <span class="n">play_context</span> <span class="k">is</span> <span class="k">None</span><span class="p">:</span>

            <span class="n">play_context</span> <span class="o">=</span> <span class="n">core</span><span class="p">.</span><span class="n">PlayContext</span><span class="p">()</span>

            <span class="n">play_context</span><span class="p">.</span><span class="n">max_steps_per_episode</span> <span class="o">=</span> <span class="n">max_steps_per_episode</span>

            <span class="n">play_context</span><span class="p">.</span><span class="n">num_episodes</span> <span class="o">=</span> <span class="n">num_episodes</span>

        <span class="n">callbacks</span> <span class="o">=</span> <span class="k">self</span><span class="p">.</span><span class="n">_to_callback_list</span><span class="p">(</span><span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span>

        <span class="n">callbacks</span> <span class="o">=</span> <span class="k">self</span><span class="p">.</span><span class="n">_add_plot_callbacks</span><span class="p">(</span><span class="n">callbacks</span><span class="p">,</span> <span class="n">default_plots</span><span class="p">,</span> <span class="p">[</span><span class="n">plot</span><span class="p">.</span><span class="n">Steps</span><span class="p">(),</span> <span class="n">plot</span><span class="p">.</span><span class="n">Rewards</span><span class="p">()])</span>

        <span class="k">self</span><span class="p">.</span><span class="n">_backend_agent</span><span class="p">.</span><span class="n">play</span><span class="p">(</span><span class="n">play_context</span><span class="o">=</span><span class="n">play_context</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">play_context</span>
</pre></div>


</details>
<h5 id="save_2">save</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">save</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span>
</pre></div>


<p>Saves the currently trained actor policy in directory.</p>
<p>If save is called before a trained policy is created, eg by calling train, an exception is raised.</p>
<p>Args:
     directory: the directory to save the policy weights to.
        if the directory does not exist yet, a new directory is created. if None the policy is saved
        in a temp directory.
     callbacks: list of callbacks called during save (eg log.Agent)</p>
<p>Returns:
    the absolute path to the directory containing the saved policy.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="k">save</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="nl">directory</span><span class="p">:</span><span class="w"> </span><span class="n">Optional</span><span class="o">[</span><span class="n">str</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">None</span><span class="p">,</span><span class="w"></span>

<span class="w">             </span><span class="nl">callbacks</span><span class="p">:</span><span class="w"> </span><span class="ow">Union</span><span class="o">[</span><span class="n">List[core.AgentCallback</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="err">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">None</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="nf">str</span><span class="err">:</span><span class="w"></span>

<span class="w">        </span><span class="ss">&quot;&quot;&quot;Saves the currently trained actor policy in directory.</span>

<span class="ss">        If save is called before a trained policy is created, eg by calling train, an exception is raised.</span>

<span class="ss">        Args:</span>

<span class="ss">             directory: the directory to save the policy weights to.</span>

<span class="ss">                if the directory does not exist yet, a new directory is created. if None the policy is saved</span>

<span class="ss">                in a temp directory.</span>

<span class="ss">             callbacks: list of callbacks called during save (eg log.Agent)</span>

<span class="ss">        Returns:</span>

<span class="ss">            the absolute path to the directory containing the saved policy.</span>

<span class="ss">        &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">directory</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">None</span><span class="err">:</span><span class="w"></span>

<span class="w">            </span><span class="n">directory</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bcore</span><span class="p">.</span><span class="n">_get_temp_path</span><span class="p">()</span><span class="w"></span>

<span class="w">        </span><span class="n">assert</span><span class="w"> </span><span class="n">directory</span><span class="w"></span>

<span class="w">        </span><span class="n">assert</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_backend_agent</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">_is_policy_trained</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;No trained policy available. Call train() first.&quot;</span><span class="w"></span>

<span class="w">        </span><span class="n">directory</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bcore</span><span class="p">.</span><span class="n">_mkdir</span><span class="p">(</span><span class="n">directory</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">agent_json_path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">os</span><span class="p">.</span><span class="k">path</span><span class="p">.</span><span class="k">join</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span><span class="w"> </span><span class="n">EasyAgent</span><span class="p">.</span><span class="n">_KEY_EASYAGENT_FILENAME</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">with</span><span class="w"> </span><span class="k">open</span><span class="p">(</span><span class="n">agent_json_path</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;w&#39;</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nl">jsonfile</span><span class="p">:</span><span class="w"></span>

<span class="w">            </span><span class="n">agent_dict</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_to_dict</span><span class="p">()</span><span class="w"></span>

<span class="w">            </span><span class="n">json</span><span class="p">.</span><span class="k">dump</span><span class="p">(</span><span class="n">agent_dict</span><span class="p">,</span><span class="w"> </span><span class="n">jsonfile</span><span class="p">,</span><span class="w"> </span><span class="n">sort_keys</span><span class="o">=</span><span class="k">True</span><span class="p">,</span><span class="w"> </span><span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">callbacks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_to_callback_list</span><span class="p">(</span><span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">policy_directory</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">os</span><span class="p">.</span><span class="k">path</span><span class="p">.</span><span class="k">join</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span><span class="w"> </span><span class="n">EasyAgent</span><span class="p">.</span><span class="n">_KEY_POLICY_DIRECTORY</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">policy_directory</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bcore</span><span class="p">.</span><span class="n">_mkdir</span><span class="p">(</span><span class="n">policy_directory</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">_backend_agent</span><span class="p">.</span><span class="k">save</span><span class="p">(</span><span class="n">directory</span><span class="o">=</span><span class="n">policy_directory</span><span class="p">,</span><span class="w"> </span><span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">directory</span><span class="w"></span>
</pre></div>


</details>
<h5 id="train_2">train</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20000</span><span class="p">,</span>
    <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span>
    <span class="n">num_steps_per_iteration</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">num_steps_buffer_preload</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">num_steps_sampled_from_buffer</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="n">num_iterations_between_eval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="n">num_episodes_per_eval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">,</span>
    <span class="n">train_context</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">StepsTrainContext</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">default_plots</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span>
</pre></div>


<p>Trains a new model using the gym environment passed during instantiation.</p>
<p>Args:
    callbacks: list of callbacks called during training and evaluation
    num_iterations: number of times the training is repeated (with additional data)
    max_steps_per_episode: maximum number of steps per episode
    num_steps_per_iteration: number of steps played per training iteration
    num_steps_buffer_preload: number of initial collect steps to preload the buffer
    num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training
    num_iterations_between_eval: number of training iterations before the current policy is evaluated.
        if 0 no evaluation is performed.
    num_episodes_per_eval: number of episodes played to estimate the average return and steps
    learning_rate: the learning rate used in the next iteration's policy training (0,1]
    train_context: training configuration to be used. if set overrides all other training context arguments.
    default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...).
        if None default callbacks are only added if the callbacks list is empty</p>
<p>Returns:
    train_context: the training configuration containing the loss and sum of rewards encountered
        during training</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="n">def</span> <span class="n">train</span><span class="p">(</span><span class="k">self</span><span class="p">,</span>

              <span class="n">callbacks</span><span class="p">:</span> <span class="k">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="k">None</span><span class="p">]</span> <span class="o">=</span> <span class="k">None</span><span class="p">,</span>

              <span class="n">num_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20000</span><span class="p">,</span>

              <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span>

              <span class="n">num_steps_per_iteration</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>

              <span class="n">num_steps_buffer_preload</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>

              <span class="n">num_steps_sampled_from_buffer</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>

              <span class="n">num_iterations_between_eval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>

              <span class="n">num_episodes_per_eval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>

              <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">0</span><span class="p">.</span><span class="mi">001</span><span class="p">,</span>

              <span class="n">train_context</span><span class="p">:</span> <span class="n">core</span><span class="p">.</span><span class="n">StepsTrainContext</span> <span class="o">=</span> <span class="k">None</span><span class="p">,</span>

              <span class="n">default_plots</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="k">None</span><span class="p">):</span>

        <span class="ss">&quot;&quot;&quot;Trains a new model using the gym environment passed during instantiation.</span>

<span class="ss">        Args:</span>

<span class="ss">            callbacks: list of callbacks called during training and evaluation</span>

<span class="ss">            num_iterations: number of times the training is repeated (with additional data)</span>

<span class="ss">            max_steps_per_episode: maximum number of steps per episode</span>

<span class="ss">            num_steps_per_iteration: number of steps played per training iteration</span>

<span class="ss">            num_steps_buffer_preload: number of initial collect steps to preload the buffer</span>

<span class="ss">            num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training</span>

<span class="ss">            num_iterations_between_eval: number of training iterations before the current policy is evaluated.</span>

<span class="ss">                if 0 no evaluation is performed.</span>

<span class="ss">            num_episodes_per_eval: number of episodes played to estimate the average return and steps</span>

<span class="ss">            learning_rate: the learning rate used in the next iteration&#39;s policy training (0,1]</span>

<span class="ss">            train_context: training configuration to be used. if set overrides all other training context arguments.</span>

<span class="ss">            default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...).</span>

<span class="ss">                if None default callbacks are only added if the callbacks list is empty</span>

<span class="ss">        Returns:</span>

<span class="ss">            train_context: the training configuration containing the loss and sum of rewards encountered</span>

<span class="ss">                during training</span>

<span class="ss">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">train_context</span> <span class="k">is</span> <span class="k">None</span><span class="p">:</span>

            <span class="n">train_context</span> <span class="o">=</span> <span class="n">core</span><span class="p">.</span><span class="n">StepsTrainContext</span><span class="p">()</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">num_iterations</span> <span class="o">=</span> <span class="n">num_iterations</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">max_steps_per_episode</span> <span class="o">=</span> <span class="n">max_steps_per_episode</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">num_steps_per_iteration</span> <span class="o">=</span> <span class="n">num_steps_per_iteration</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">num_steps_buffer_preload</span> <span class="o">=</span> <span class="n">num_steps_buffer_preload</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">num_steps_sampled_from_buffer</span> <span class="o">=</span> <span class="n">num_steps_sampled_from_buffer</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">num_iterations_between_eval</span> <span class="o">=</span> <span class="n">num_iterations_between_eval</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">num_episodes_per_eval</span> <span class="o">=</span> <span class="n">num_episodes_per_eval</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>

        <span class="n">super</span><span class="p">().</span><span class="n">train</span><span class="p">(</span><span class="n">train_context</span><span class="o">=</span><span class="n">train_context</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span> <span class="n">default_plots</span><span class="o">=</span><span class="n">default_plots</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">train_context</span>
</pre></div>


</details>
<h3 id="duelingdqnagent">DuelingDqnAgent</h3>
<div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">DuelingDqnAgent</span><span class="p">(</span>
    <span class="n">gym_env_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">fc_layers</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="nb">int</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">backend</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span>
</pre></div>


<p>Agent based on the Dueling Dqn algorithm (https://arxiv.org/abs/1511.06581).</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><span class="k">class</span> <span class="n">DuelingDqnAgent</span>(<span class="n">DqnAgent</span>):

    <span class="s">&quot;&quot;&quot;Agent based on the Dueling Dqn algorithm (https://arxiv.org/abs/1511.06581).&quot;&quot;&quot;</span>
</pre></div>


</details>
<hr />
<h4 id="ancestors-in-mro_3">Ancestors (in MRO)</h4>
<ul>
<li>easyagents.agents.DqnAgent</li>
<li>easyagents.agents.EasyAgent</li>
<li>abc.ABC</li>
</ul>
<h4 id="methods_3">Methods</h4>
<h5 id="evaluate_3">evaluate</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">num_episodes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span>
<span class="p">)</span>
</pre></div>


<p>Plays num_episodes with the current policy and computes metrics on rewards.</p>
<p>Args:
    num_episodes: number of episodes to play
    max_steps_per_episode: max steps per episode</p>
<p>Returns:
    extensible score metrics</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">evaluate</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="nl">num_episodes</span><span class="p">:</span><span class="w"> </span><span class="nc">int</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">50</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="nl">max_steps_per_episode</span><span class="p">:</span><span class="w"> </span><span class="nc">int</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">50</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">        </span><span class="ss">&quot;&quot;&quot;Plays num_episodes with the current policy and computes metrics on rewards.</span>

<span class="ss">        Args:</span>

<span class="ss">            num_episodes: number of episodes to play</span>

<span class="ss">            max_steps_per_episode: max steps per episode</span>

<span class="ss">        Returns:</span>

<span class="ss">            extensible score metrics</span>

<span class="ss">        &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">        </span><span class="n">play_context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">core</span><span class="p">.</span><span class="n">PlayContext</span><span class="p">()</span><span class="w"></span>

<span class="w">        </span><span class="n">play_context</span><span class="p">.</span><span class="n">max_steps_per_episode</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">max_steps_per_episode</span><span class="w"></span>

<span class="w">        </span><span class="n">play_context</span><span class="p">.</span><span class="n">num_episodes</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">num_episodes</span><span class="w"></span>

<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">play</span><span class="p">(</span><span class="n">play_context</span><span class="o">=</span><span class="n">play_context</span><span class="p">,</span><span class="w"> </span><span class="n">default_plots</span><span class="o">=</span><span class="k">False</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">Metrics</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;Metrics&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;steps rewards&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">Rewards</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;Rewards&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;mean std min max all&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">all_rewards</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">list</span><span class="p">(</span><span class="n">play_context</span><span class="p">.</span><span class="n">sum_of_rewards</span><span class="p">.</span><span class="k">values</span><span class="p">())</span><span class="w"></span>

<span class="w">        </span><span class="n">mean_reward</span><span class="p">,</span><span class="w"> </span><span class="n">std_reward</span><span class="p">,</span><span class="w"> </span><span class="n">min_reward</span><span class="p">,</span><span class="w"> </span><span class="n">max_reward</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">statistics</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">all_rewards</span><span class="p">),</span><span class="w"> </span><span class="k">statistics</span><span class="p">.</span><span class="nf">stdev</span><span class="p">(</span><span class="w"></span>

<span class="w">            </span><span class="n">all_rewards</span><span class="p">),</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">all_rewards</span><span class="p">),</span><span class="w"> </span><span class="nf">max</span><span class="p">(</span><span class="n">all_rewards</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">rewards</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Rewards</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mean_reward</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">=</span><span class="n">std_reward</span><span class="p">,</span><span class="w"> </span><span class="nf">min</span><span class="o">=</span><span class="n">min_reward</span><span class="p">,</span><span class="w"> </span><span class="nf">max</span><span class="o">=</span><span class="n">max_reward</span><span class="p">,</span><span class="w"> </span><span class="ow">all</span><span class="o">=</span><span class="n">all_rewards</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">Steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;Steps&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;mean std min max all&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">all_num_steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">[]</span><span class="w"></span>

<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">play_context</span><span class="p">.</span><span class="n">rewards</span><span class="p">.</span><span class="n">keys</span><span class="p">()</span><span class="err">:</span><span class="w"></span>

<span class="w">            </span><span class="n">all_num_steps</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">play_context</span><span class="p">.</span><span class="n">rewards</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="p">))</span><span class="w"></span>

<span class="w">        </span><span class="n">mean_steps</span><span class="p">,</span><span class="w"> </span><span class="n">std_steps</span><span class="p">,</span><span class="w"> </span><span class="n">min_steps</span><span class="p">,</span><span class="w"> </span><span class="n">max_steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">statistics</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">all_num_steps</span><span class="p">),</span><span class="w"> </span><span class="k">statistics</span><span class="p">.</span><span class="nf">stdev</span><span class="p">(</span><span class="w"></span>

<span class="w">            </span><span class="n">all_num_steps</span><span class="p">),</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">all_num_steps</span><span class="p">),</span><span class="w"> </span><span class="nf">max</span><span class="p">(</span><span class="n">all_num_steps</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Steps</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mean_steps</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">=</span><span class="n">std_steps</span><span class="p">,</span><span class="w"> </span><span class="nf">min</span><span class="o">=</span><span class="n">min_steps</span><span class="p">,</span><span class="w"> </span><span class="nf">max</span><span class="o">=</span><span class="n">max_steps</span><span class="p">,</span><span class="w"> </span><span class="ow">all</span><span class="o">=</span><span class="n">all_num_steps</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">metrics</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Metrics</span><span class="p">(</span><span class="n">rewards</span><span class="o">=</span><span class="n">rewards</span><span class="p">,</span><span class="w"> </span><span class="n">steps</span><span class="o">=</span><span class="n">steps</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">metrics</span><span class="w"></span>
</pre></div>


</details>
<h5 id="play_3">play</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">play</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_episodes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="n">play_context</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">PlayContext</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">default_plots</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span>
</pre></div>


<p>Plays num_episodes with the current policy.</p>
<p>Args:
    callbacks: list of callbacks called during each episode play
    num_episodes: number of episodes to play
    max_steps_per_episode: max steps per episode
    play_context: play configuration to be used. If set override all other play context arguments
    default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...)</p>
<p>Returns:
    play_context containg the actions taken and the rewards received during training</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="n">def</span> <span class="n">play</span><span class="p">(</span><span class="k">self</span><span class="p">,</span>

             <span class="n">callbacks</span><span class="p">:</span> <span class="k">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="k">None</span><span class="p">]</span> <span class="o">=</span> <span class="k">None</span><span class="p">,</span>

             <span class="n">num_episodes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>

             <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>

             <span class="n">play_context</span><span class="p">:</span> <span class="n">core</span><span class="p">.</span><span class="n">PlayContext</span> <span class="o">=</span> <span class="k">None</span><span class="p">,</span>

             <span class="n">default_plots</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="k">None</span><span class="p">):</span>

        <span class="ss">&quot;&quot;&quot;Plays num_episodes with the current policy.</span>

<span class="ss">        Args:</span>

<span class="ss">            callbacks: list of callbacks called during each episode play</span>

<span class="ss">            num_episodes: number of episodes to play</span>

<span class="ss">            max_steps_per_episode: max steps per episode</span>

<span class="ss">            play_context: play configuration to be used. If set override all other play context arguments</span>

<span class="ss">            default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...)</span>

<span class="ss">        Returns:</span>

<span class="ss">            play_context containg the actions taken and the rewards received during training</span>

<span class="ss">        &quot;&quot;&quot;</span>

        <span class="n">assert</span> <span class="k">self</span><span class="p">.</span><span class="n">_backend_agent</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">_is_policy_trained</span><span class="p">,</span> <span class="ss">&quot;No trained policy available. Call train() first.&quot;</span>

        <span class="k">if</span> <span class="n">play_context</span> <span class="k">is</span> <span class="k">None</span><span class="p">:</span>

            <span class="n">play_context</span> <span class="o">=</span> <span class="n">core</span><span class="p">.</span><span class="n">PlayContext</span><span class="p">()</span>

            <span class="n">play_context</span><span class="p">.</span><span class="n">max_steps_per_episode</span> <span class="o">=</span> <span class="n">max_steps_per_episode</span>

            <span class="n">play_context</span><span class="p">.</span><span class="n">num_episodes</span> <span class="o">=</span> <span class="n">num_episodes</span>

        <span class="n">callbacks</span> <span class="o">=</span> <span class="k">self</span><span class="p">.</span><span class="n">_to_callback_list</span><span class="p">(</span><span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span>

        <span class="n">callbacks</span> <span class="o">=</span> <span class="k">self</span><span class="p">.</span><span class="n">_add_plot_callbacks</span><span class="p">(</span><span class="n">callbacks</span><span class="p">,</span> <span class="n">default_plots</span><span class="p">,</span> <span class="p">[</span><span class="n">plot</span><span class="p">.</span><span class="n">Steps</span><span class="p">(),</span> <span class="n">plot</span><span class="p">.</span><span class="n">Rewards</span><span class="p">()])</span>

        <span class="k">self</span><span class="p">.</span><span class="n">_backend_agent</span><span class="p">.</span><span class="n">play</span><span class="p">(</span><span class="n">play_context</span><span class="o">=</span><span class="n">play_context</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">play_context</span>
</pre></div>


</details>
<h5 id="save_3">save</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">save</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span>
</pre></div>


<p>Saves the currently trained actor policy in directory.</p>
<p>If save is called before a trained policy is created, eg by calling train, an exception is raised.</p>
<p>Args:
     directory: the directory to save the policy weights to.
        if the directory does not exist yet, a new directory is created. if None the policy is saved
        in a temp directory.
     callbacks: list of callbacks called during save (eg log.Agent)</p>
<p>Returns:
    the absolute path to the directory containing the saved policy.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="k">save</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="nl">directory</span><span class="p">:</span><span class="w"> </span><span class="n">Optional</span><span class="o">[</span><span class="n">str</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">None</span><span class="p">,</span><span class="w"></span>

<span class="w">             </span><span class="nl">callbacks</span><span class="p">:</span><span class="w"> </span><span class="ow">Union</span><span class="o">[</span><span class="n">List[core.AgentCallback</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="err">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">None</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="nf">str</span><span class="err">:</span><span class="w"></span>

<span class="w">        </span><span class="ss">&quot;&quot;&quot;Saves the currently trained actor policy in directory.</span>

<span class="ss">        If save is called before a trained policy is created, eg by calling train, an exception is raised.</span>

<span class="ss">        Args:</span>

<span class="ss">             directory: the directory to save the policy weights to.</span>

<span class="ss">                if the directory does not exist yet, a new directory is created. if None the policy is saved</span>

<span class="ss">                in a temp directory.</span>

<span class="ss">             callbacks: list of callbacks called during save (eg log.Agent)</span>

<span class="ss">        Returns:</span>

<span class="ss">            the absolute path to the directory containing the saved policy.</span>

<span class="ss">        &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">directory</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">None</span><span class="err">:</span><span class="w"></span>

<span class="w">            </span><span class="n">directory</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bcore</span><span class="p">.</span><span class="n">_get_temp_path</span><span class="p">()</span><span class="w"></span>

<span class="w">        </span><span class="n">assert</span><span class="w"> </span><span class="n">directory</span><span class="w"></span>

<span class="w">        </span><span class="n">assert</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_backend_agent</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">_is_policy_trained</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;No trained policy available. Call train() first.&quot;</span><span class="w"></span>

<span class="w">        </span><span class="n">directory</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bcore</span><span class="p">.</span><span class="n">_mkdir</span><span class="p">(</span><span class="n">directory</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">agent_json_path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">os</span><span class="p">.</span><span class="k">path</span><span class="p">.</span><span class="k">join</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span><span class="w"> </span><span class="n">EasyAgent</span><span class="p">.</span><span class="n">_KEY_EASYAGENT_FILENAME</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">with</span><span class="w"> </span><span class="k">open</span><span class="p">(</span><span class="n">agent_json_path</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;w&#39;</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nl">jsonfile</span><span class="p">:</span><span class="w"></span>

<span class="w">            </span><span class="n">agent_dict</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_to_dict</span><span class="p">()</span><span class="w"></span>

<span class="w">            </span><span class="n">json</span><span class="p">.</span><span class="k">dump</span><span class="p">(</span><span class="n">agent_dict</span><span class="p">,</span><span class="w"> </span><span class="n">jsonfile</span><span class="p">,</span><span class="w"> </span><span class="n">sort_keys</span><span class="o">=</span><span class="k">True</span><span class="p">,</span><span class="w"> </span><span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">callbacks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_to_callback_list</span><span class="p">(</span><span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">policy_directory</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">os</span><span class="p">.</span><span class="k">path</span><span class="p">.</span><span class="k">join</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span><span class="w"> </span><span class="n">EasyAgent</span><span class="p">.</span><span class="n">_KEY_POLICY_DIRECTORY</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">policy_directory</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bcore</span><span class="p">.</span><span class="n">_mkdir</span><span class="p">(</span><span class="n">policy_directory</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">_backend_agent</span><span class="p">.</span><span class="k">save</span><span class="p">(</span><span class="n">directory</span><span class="o">=</span><span class="n">policy_directory</span><span class="p">,</span><span class="w"> </span><span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">directory</span><span class="w"></span>
</pre></div>


</details>
<h5 id="train_3">train</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20000</span><span class="p">,</span>
    <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span>
    <span class="n">num_steps_per_iteration</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">num_steps_buffer_preload</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">num_steps_sampled_from_buffer</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="n">num_iterations_between_eval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="n">num_episodes_per_eval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">,</span>
    <span class="n">train_context</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">StepsTrainContext</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">default_plots</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span>
</pre></div>


<p>Trains a new model using the gym environment passed during instantiation.</p>
<p>Args:
    callbacks: list of callbacks called during training and evaluation
    num_iterations: number of times the training is repeated (with additional data)
    max_steps_per_episode: maximum number of steps per episode
    num_steps_per_iteration: number of steps played per training iteration
    num_steps_buffer_preload: number of initial collect steps to preload the buffer
    num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training
    num_iterations_between_eval: number of training iterations before the current policy is evaluated.
        if 0 no evaluation is performed.
    num_episodes_per_eval: number of episodes played to estimate the average return and steps
    learning_rate: the learning rate used in the next iteration's policy training (0,1]
    train_context: training configuration to be used. if set overrides all other training context arguments.
    default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...).
        if None default callbacks are only added if the callbacks list is empty</p>
<p>Returns:
    train_context: the training configuration containing the loss and sum of rewards encountered
        during training</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="n">def</span> <span class="n">train</span><span class="p">(</span><span class="k">self</span><span class="p">,</span>

              <span class="n">callbacks</span><span class="p">:</span> <span class="k">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="k">None</span><span class="p">]</span> <span class="o">=</span> <span class="k">None</span><span class="p">,</span>

              <span class="n">num_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20000</span><span class="p">,</span>

              <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span>

              <span class="n">num_steps_per_iteration</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>

              <span class="n">num_steps_buffer_preload</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>

              <span class="n">num_steps_sampled_from_buffer</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>

              <span class="n">num_iterations_between_eval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>

              <span class="n">num_episodes_per_eval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>

              <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">0</span><span class="p">.</span><span class="mi">001</span><span class="p">,</span>

              <span class="n">train_context</span><span class="p">:</span> <span class="n">core</span><span class="p">.</span><span class="n">StepsTrainContext</span> <span class="o">=</span> <span class="k">None</span><span class="p">,</span>

              <span class="n">default_plots</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="k">None</span><span class="p">):</span>

        <span class="ss">&quot;&quot;&quot;Trains a new model using the gym environment passed during instantiation.</span>

<span class="ss">        Args:</span>

<span class="ss">            callbacks: list of callbacks called during training and evaluation</span>

<span class="ss">            num_iterations: number of times the training is repeated (with additional data)</span>

<span class="ss">            max_steps_per_episode: maximum number of steps per episode</span>

<span class="ss">            num_steps_per_iteration: number of steps played per training iteration</span>

<span class="ss">            num_steps_buffer_preload: number of initial collect steps to preload the buffer</span>

<span class="ss">            num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training</span>

<span class="ss">            num_iterations_between_eval: number of training iterations before the current policy is evaluated.</span>

<span class="ss">                if 0 no evaluation is performed.</span>

<span class="ss">            num_episodes_per_eval: number of episodes played to estimate the average return and steps</span>

<span class="ss">            learning_rate: the learning rate used in the next iteration&#39;s policy training (0,1]</span>

<span class="ss">            train_context: training configuration to be used. if set overrides all other training context arguments.</span>

<span class="ss">            default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...).</span>

<span class="ss">                if None default callbacks are only added if the callbacks list is empty</span>

<span class="ss">        Returns:</span>

<span class="ss">            train_context: the training configuration containing the loss and sum of rewards encountered</span>

<span class="ss">                during training</span>

<span class="ss">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">train_context</span> <span class="k">is</span> <span class="k">None</span><span class="p">:</span>

            <span class="n">train_context</span> <span class="o">=</span> <span class="n">core</span><span class="p">.</span><span class="n">StepsTrainContext</span><span class="p">()</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">num_iterations</span> <span class="o">=</span> <span class="n">num_iterations</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">max_steps_per_episode</span> <span class="o">=</span> <span class="n">max_steps_per_episode</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">num_steps_per_iteration</span> <span class="o">=</span> <span class="n">num_steps_per_iteration</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">num_steps_buffer_preload</span> <span class="o">=</span> <span class="n">num_steps_buffer_preload</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">num_steps_sampled_from_buffer</span> <span class="o">=</span> <span class="n">num_steps_sampled_from_buffer</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">num_iterations_between_eval</span> <span class="o">=</span> <span class="n">num_iterations_between_eval</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">num_episodes_per_eval</span> <span class="o">=</span> <span class="n">num_episodes_per_eval</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>

        <span class="n">super</span><span class="p">().</span><span class="n">train</span><span class="p">(</span><span class="n">train_context</span><span class="o">=</span><span class="n">train_context</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span> <span class="n">default_plots</span><span class="o">=</span><span class="n">default_plots</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">train_context</span>
</pre></div>


</details>
<h3 id="easyagent">EasyAgent</h3>
<div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">EasyAgent</span><span class="p">(</span>
    <span class="n">gym_env_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">fc_layers</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="nb">int</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">backend</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span>
</pre></div>


<p>Abstract base class for all easy reinforcment learning agents.</p>
<p>Besides forwarding train and play it implements persistence.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="n">EasyAgent</span><span class="p">(</span><span class="n">ABC</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">    </span><span class="ss">&quot;&quot;&quot;Abstract base class for all easy reinforcment learning agents.</span>

<span class="ss">    Besides forwarding train and play it implements persistence.&quot;&quot;&quot;</span><span class="w"></span>

<span class="w">    </span><span class="n">_KEY_BACKEND</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;backend&#39;</span><span class="w"></span>

<span class="w">    </span><span class="n">_KEY_EASYAGENT_CLASS</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;easyagent_class&#39;</span><span class="w"></span>

<span class="w">    </span><span class="n">_KEY_EASYAGENT_FILENAME</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;easyagent.json&#39;</span><span class="w"></span>

<span class="w">    </span><span class="n">_KEY_MODEL_CONFIG</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;model_config&#39;</span><span class="w"></span>

<span class="w">    </span><span class="n">_KEY_POLICY_DIRECTORY</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;policy&#39;</span><span class="w"></span>

<span class="w">    </span><span class="n">_KEY_VERSION</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;version&#39;</span><span class="w"></span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="nl">gym_env_name</span><span class="p">:</span><span class="w"> </span><span class="nf">str</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="nl">fc_layers</span><span class="p">:</span><span class="w"> </span><span class="ow">Union</span><span class="o">[</span><span class="n">Tuple[int, ...</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="nc">int</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="err">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">None</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="nl">backend</span><span class="p">:</span><span class="w"> </span><span class="nf">str</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">None</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">        </span><span class="ss">&quot;&quot;&quot;</span>

<span class="ss">            Args:</span>

<span class="ss">                gym_env_name: name of an OpenAI gym environment to be used for training and evaluation</span>

<span class="ss">                fc_layers: defines the neural network to be used, a sequence of fully connected</span>

<span class="ss">                    layers of the given size. Eg (75,40) yields a neural network consisting</span>

<span class="ss">                    out of 2 hidden layers, the first one containing 75 and the second layer</span>

<span class="ss">                    containing 40 neurons.</span>

<span class="ss">                backend=the backend to be used (eg &#39;tfagents&#39;), if None a default implementation is used.</span>

<span class="ss">                    call get_backends() to get a list of the available backends.</span>

<span class="ss">        &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">        </span><span class="n">model_config</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">core</span><span class="p">.</span><span class="n">ModelConfig</span><span class="p">(</span><span class="n">gym_env_name</span><span class="o">=</span><span class="n">gym_env_name</span><span class="p">,</span><span class="w"> </span><span class="n">fc_layers</span><span class="o">=</span><span class="n">fc_layers</span><span class="p">,</span><span class="w"> </span><span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">_initialize</span><span class="p">(</span><span class="n">model_config</span><span class="o">=</span><span class="n">model_config</span><span class="p">,</span><span class="w"> </span><span class="n">backend_name</span><span class="o">=</span><span class="n">backend</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">return</span><span class="w"></span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">_initialize</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="nl">model_config</span><span class="p">:</span><span class="w"> </span><span class="n">core</span><span class="p">.</span><span class="n">ModelConfig</span><span class="p">,</span><span class="w"> </span><span class="nl">backend_name</span><span class="p">:</span><span class="w"> </span><span class="nf">str</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">None</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">backend_name</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">None</span><span class="err">:</span><span class="w"></span>

<span class="w">            </span><span class="n">backend_name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">easyagents</span><span class="p">.</span><span class="n">backends</span><span class="p">.</span><span class="k">default</span><span class="p">.</span><span class="n">DefaultAgentFactory</span><span class="p">.</span><span class="n">backend_name</span><span class="w"></span>

<span class="w">        </span><span class="nl">backend</span><span class="p">:</span><span class="w"> </span><span class="n">bcore</span><span class="p">.</span><span class="n">BackendAgentFactory</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">_get_backend</span><span class="p">(</span><span class="n">backend_name</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">assert</span><span class="w"> </span><span class="n">model_config</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="k">None</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;model_config not set.&quot;</span><span class="w"></span>

<span class="w">        </span><span class="n">assert</span><span class="w"> </span><span class="n">backend</span><span class="p">,</span><span class="w"> </span><span class="n">f</span><span class="s1">&#39;Backend &quot;{backend_name}&quot; not found. The registered backends are {get_backends()}.&#39;</span><span class="w"></span>

<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="nl">_model_config</span><span class="p">:</span><span class="w"> </span><span class="n">core</span><span class="p">.</span><span class="n">ModelConfig</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model_config</span><span class="w"></span>

<span class="w">        </span><span class="n">backend_agent</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">backend</span><span class="p">.</span><span class="n">create_agent</span><span class="p">(</span><span class="n">easyagent_type</span><span class="o">=</span><span class="n">type</span><span class="p">(</span><span class="n">self</span><span class="p">),</span><span class="w"> </span><span class="n">model_config</span><span class="o">=</span><span class="n">model_config</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">assert</span><span class="w"> </span><span class="n">backend_agent</span><span class="p">,</span><span class="w"> </span><span class="n">f</span><span class="s1">&#39;Backend &quot;{backend_name}&quot; does not implement &quot;{type(self).__name__}&quot;. &#39;</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="err">\</span><span class="w"></span>

<span class="w">                              </span><span class="n">f</span><span class="s1">&#39;Choose one of the following backend {get_backends(type(self))}.&#39;</span><span class="w"></span>

<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="nl">_backend_agent</span><span class="p">:</span><span class="w"> </span><span class="n">Optional</span><span class="o">[</span><span class="n">bcore._BackendAgent</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">backend_agent</span><span class="w"></span>

<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="nl">_backend_name</span><span class="p">:</span><span class="w"> </span><span class="nf">str</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">backend_name</span><span class="w"></span>

<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">_backend_agent</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">_agent_saver</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="k">save</span><span class="w"></span>

<span class="w">        </span><span class="k">return</span><span class="w"></span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">_add_plot_callbacks</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="nl">callbacks</span><span class="p">:</span><span class="w"> </span><span class="n">List</span><span class="o">[</span><span class="n">core.AgentCallback</span><span class="o">]</span><span class="p">,</span><span class="w"></span>

<span class="w">                            </span><span class="nl">default_plots</span><span class="p">:</span><span class="w"> </span><span class="n">Optional</span><span class="o">[</span><span class="n">bool</span><span class="o">]</span><span class="p">,</span><span class="w"></span>

<span class="w">                            </span><span class="nl">default_plot_callbacks</span><span class="p">:</span><span class="w"> </span><span class="n">List</span><span class="o">[</span><span class="n">plot._PlotCallback</span><span class="o">]</span><span class="w"></span>

<span class="w">                            </span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">List</span><span class="o">[</span><span class="n">core.AgentCallback</span><span class="o">]</span><span class="err">:</span><span class="w"></span>

<span class="w">        </span><span class="ss">&quot;&quot;&quot;Adds the default callbacks and sorts all callbacks in the order</span>

<span class="ss">            _PreProcessCallbacks, AgentCallbacks, _PostProcessCallbacks.</span>

<span class="ss">        Args:</span>

<span class="ss">            callbacks: existing callbacks to prepare</span>

<span class="ss">            default_plots: if set or if None and callbacks does not contain plots then the default plots are added</span>

<span class="ss">            default_plot_callbacks: plot callbacks to add.</span>

<span class="ss">        &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">        </span><span class="nl">pre_process</span><span class="p">:</span><span class="w"> </span><span class="n">List</span><span class="o">[</span><span class="n">core.AgentCallback</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">plot._PreProcess()</span><span class="o">]</span><span class="w"></span>

<span class="w">        </span><span class="nl">agent</span><span class="p">:</span><span class="w"> </span><span class="n">List</span><span class="o">[</span><span class="n">core.AgentCallback</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">[]</span><span class="w"></span>

<span class="w">        </span><span class="nl">post_process</span><span class="p">:</span><span class="w"> </span><span class="n">List</span><span class="o">[</span><span class="n">core.AgentCallback</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">plot._PostProcess()</span><span class="o">]</span><span class="w"></span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">default_plots</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">None</span><span class="err">:</span><span class="w"></span>

<span class="w">            </span><span class="n">default_plots</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">True</span><span class="w"></span>

<span class="w">            </span><span class="k">for</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="nl">callbacks</span><span class="p">:</span><span class="w"></span>

<span class="w">                </span><span class="n">default_plots</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">default_plots</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="p">(</span><span class="ow">not</span><span class="w"> </span><span class="n">isinstance</span><span class="p">(</span><span class="n">c</span><span class="p">,</span><span class="w"> </span><span class="n">plot</span><span class="p">.</span><span class="n">_PlotCallback</span><span class="p">))</span><span class="w"></span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="nl">default_plots</span><span class="p">:</span><span class="w"></span>

<span class="w">            </span><span class="n">agent</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">default_plot_callbacks</span><span class="w"></span>

<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="nl">callbacks</span><span class="p">:</span><span class="w"></span>

<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="n">isinstance</span><span class="p">(</span><span class="n">c</span><span class="p">,</span><span class="w"> </span><span class="n">core</span><span class="p">.</span><span class="n">_PreProcessCallback</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">                </span><span class="n">pre_process</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">c</span><span class="p">)</span><span class="w"></span>

<span class="w">            </span><span class="k">else</span><span class="err">:</span><span class="w"></span>

<span class="w">                </span><span class="k">if</span><span class="w"> </span><span class="n">isinstance</span><span class="p">(</span><span class="n">c</span><span class="p">,</span><span class="w"> </span><span class="n">core</span><span class="p">.</span><span class="n">_PostProcessCallback</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">                    </span><span class="n">post_process</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">c</span><span class="p">)</span><span class="w"></span>

<span class="w">                </span><span class="k">else</span><span class="err">:</span><span class="w"></span>

<span class="w">                    </span><span class="n">agent</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">c</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">result</span><span class="err">:</span><span class="w"> </span><span class="n">List</span><span class="o">[</span><span class="n">core.AgentCallback</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pre_process</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">agent</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">post_process</span><span class="w"></span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="k">result</span><span class="w"></span>

<span class="w">    </span><span class="nv">@staticmethod</span><span class="w"></span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">_from_dict</span><span class="p">(</span><span class="nl">param_dict</span><span class="p">:</span><span class="w"> </span><span class="n">Dict</span><span class="o">[</span><span class="n">str, object</span><span class="o">]</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">        </span><span class="ss">&quot;&quot;&quot;recreates a new agent instance according to the definition previously created by _to_dict.</span>

<span class="ss">        Returns:</span>

<span class="ss">            new agent instance (excluding any trained policy), the agent type is preserved.</span>

<span class="ss">        &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">        </span><span class="n">assert</span><span class="w"> </span><span class="n">param_dict</span><span class="w"></span>

<span class="w">        </span><span class="nl">mc</span><span class="p">:</span><span class="w"> </span><span class="n">core</span><span class="p">.</span><span class="n">ModelConfig</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">core</span><span class="p">.</span><span class="n">ModelConfig</span><span class="p">.</span><span class="n">_from_dict</span><span class="p">(</span><span class="n">param_dict</span><span class="o">[</span><span class="n">EasyAgent._KEY_MODEL_CONFIG</span><span class="o">]</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">agent_class</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">globals</span><span class="p">()</span><span class="o">[</span><span class="n">param_dict[EasyAgent._KEY_EASYAGENT_CLASS</span><span class="o">]</span><span class="err">]</span><span class="w"></span>

<span class="w">        </span><span class="nl">backend</span><span class="p">:</span><span class="w"> </span><span class="nf">str</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">param_dict</span><span class="o">[</span><span class="n">EasyAgent._KEY_BACKEND</span><span class="o">]</span><span class="w"></span>

<span class="w">        </span><span class="k">result</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">agent_class</span><span class="p">(</span><span class="n">gym_env_name</span><span class="o">=</span><span class="n">mc</span><span class="p">.</span><span class="n">original_env_name</span><span class="p">,</span><span class="w"> </span><span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">result</span><span class="p">.</span><span class="n">_initialize</span><span class="p">(</span><span class="n">model_config</span><span class="o">=</span><span class="n">mc</span><span class="p">,</span><span class="w"> </span><span class="n">backend_name</span><span class="o">=</span><span class="n">backend</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="k">result</span><span class="w"></span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">_to_callback_list</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="nl">callbacks</span><span class="p">:</span><span class="w"> </span><span class="ow">Union</span><span class="o">[</span><span class="n">Optional[core.AgentCallback</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">List</span><span class="o">[</span><span class="n">core.AgentCallback</span><span class="o">]</span><span class="err">]</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">List</span><span class="o">[</span><span class="n"></span>

<span class="n">        core.AgentCallback</span><span class="o">]</span><span class="err">:</span><span class="w"></span>

<span class="w">        </span><span class="ss">&quot;&quot;&quot;maps callbacks to an admissible callback list.</span>

<span class="ss">        if callbacks is None an empty list is returned.</span>

<span class="ss">        if callbacks is an AgentCallback a list containing only this callback is returned</span>

<span class="ss">        otherwise callbacks is returned</span>

<span class="ss">        &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">        </span><span class="k">result</span><span class="err">:</span><span class="w"> </span><span class="n">List</span><span class="o">[</span><span class="n">core.AgentCallback</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">[]</span><span class="w"></span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">callbacks</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">None</span><span class="err">:</span><span class="w"></span>

<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="n">isinstance</span><span class="p">(</span><span class="n">callbacks</span><span class="p">,</span><span class="w"> </span><span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">                </span><span class="k">result</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">callbacks</span><span class="o">]</span><span class="w"></span>

<span class="w">            </span><span class="k">else</span><span class="err">:</span><span class="w"></span>

<span class="w">                </span><span class="n">assert</span><span class="w"> </span><span class="n">isinstance</span><span class="p">(</span><span class="n">callbacks</span><span class="p">,</span><span class="w"> </span><span class="n">list</span><span class="p">),</span><span class="w"> </span><span class="ss">&quot;callback not an AgentCallback or a list thereof.&quot;</span><span class="w"></span>

<span class="w">                </span><span class="k">result</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">callbacks</span><span class="w"></span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="k">result</span><span class="w"></span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">_to_dict</span><span class="p">(</span><span class="n">self</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">Dict</span><span class="o">[</span><span class="n">str, object</span><span class="o">]</span><span class="err">:</span><span class="w"></span>

<span class="w">        </span><span class="ss">&quot;&quot;&quot;saves the agent definition to a dict.</span>

<span class="ss">            Returns:</span>

<span class="ss">                dict containing all parameters to recreate the agent (excluding a trained policy)</span>

<span class="ss">        &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">        </span><span class="k">result</span><span class="err">:</span><span class="w"> </span><span class="n">Dict</span><span class="o">[</span><span class="n">str, object</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dict</span><span class="p">()</span><span class="w"></span>

<span class="w">        </span><span class="k">result</span><span class="o">[</span><span class="n">EasyAgent._KEY_VERSION</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">easyagents</span><span class="p">.</span><span class="n">__version__</span><span class="w"></span>

<span class="w">        </span><span class="k">result</span><span class="o">[</span><span class="n">EasyAgent._KEY_EASYAGENT_CLASS</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">__class__</span><span class="p">.</span><span class="n">__name__</span><span class="w"></span>

<span class="w">        </span><span class="k">result</span><span class="o">[</span><span class="n">EasyAgent._KEY_BACKEND</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_backend_name</span><span class="w"></span>

<span class="w">        </span><span class="k">result</span><span class="o">[</span><span class="n">EasyAgent._KEY_MODEL_CONFIG</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_model_config</span><span class="p">.</span><span class="n">_to_dict</span><span class="p">()</span><span class="w"></span>

<span class="w">        </span><span class="k">result</span><span class="o">[</span><span class="n">EasyAgent._KEY_POLICY_DIRECTORY</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;policy&#39;</span><span class="w"></span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="k">result</span><span class="w"></span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">evaluate</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="nl">num_episodes</span><span class="p">:</span><span class="w"> </span><span class="nc">int</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">50</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="nl">max_steps_per_episode</span><span class="p">:</span><span class="w"> </span><span class="nc">int</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">50</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">        </span><span class="ss">&quot;&quot;&quot;Plays num_episodes with the current policy and computes metrics on rewards.</span>

<span class="ss">        Args:</span>

<span class="ss">            num_episodes: number of episodes to play</span>

<span class="ss">            max_steps_per_episode: max steps per episode</span>

<span class="ss">        Returns:</span>

<span class="ss">            extensible score metrics</span>

<span class="ss">        &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">        </span><span class="n">play_context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">core</span><span class="p">.</span><span class="n">PlayContext</span><span class="p">()</span><span class="w"></span>

<span class="w">        </span><span class="n">play_context</span><span class="p">.</span><span class="n">max_steps_per_episode</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">max_steps_per_episode</span><span class="w"></span>

<span class="w">        </span><span class="n">play_context</span><span class="p">.</span><span class="n">num_episodes</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">num_episodes</span><span class="w"></span>

<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">play</span><span class="p">(</span><span class="n">play_context</span><span class="o">=</span><span class="n">play_context</span><span class="p">,</span><span class="w"> </span><span class="n">default_plots</span><span class="o">=</span><span class="k">False</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">Metrics</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;Metrics&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;steps rewards&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">Rewards</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;Rewards&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;mean std min max all&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">all_rewards</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">list</span><span class="p">(</span><span class="n">play_context</span><span class="p">.</span><span class="n">sum_of_rewards</span><span class="p">.</span><span class="k">values</span><span class="p">())</span><span class="w"></span>

<span class="w">        </span><span class="n">mean_reward</span><span class="p">,</span><span class="w"> </span><span class="n">std_reward</span><span class="p">,</span><span class="w"> </span><span class="n">min_reward</span><span class="p">,</span><span class="w"> </span><span class="n">max_reward</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">statistics</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">all_rewards</span><span class="p">),</span><span class="w"> </span><span class="k">statistics</span><span class="p">.</span><span class="nf">stdev</span><span class="p">(</span><span class="w"></span>

<span class="w">            </span><span class="n">all_rewards</span><span class="p">),</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">all_rewards</span><span class="p">),</span><span class="w"> </span><span class="nf">max</span><span class="p">(</span><span class="n">all_rewards</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">rewards</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Rewards</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mean_reward</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">=</span><span class="n">std_reward</span><span class="p">,</span><span class="w"> </span><span class="nf">min</span><span class="o">=</span><span class="n">min_reward</span><span class="p">,</span><span class="w"> </span><span class="nf">max</span><span class="o">=</span><span class="n">max_reward</span><span class="p">,</span><span class="w"> </span><span class="ow">all</span><span class="o">=</span><span class="n">all_rewards</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">Steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;Steps&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;mean std min max all&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">all_num_steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">[]</span><span class="w"></span>

<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">play_context</span><span class="p">.</span><span class="n">rewards</span><span class="p">.</span><span class="n">keys</span><span class="p">()</span><span class="err">:</span><span class="w"></span>

<span class="w">            </span><span class="n">all_num_steps</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">play_context</span><span class="p">.</span><span class="n">rewards</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="p">))</span><span class="w"></span>

<span class="w">        </span><span class="n">mean_steps</span><span class="p">,</span><span class="w"> </span><span class="n">std_steps</span><span class="p">,</span><span class="w"> </span><span class="n">min_steps</span><span class="p">,</span><span class="w"> </span><span class="n">max_steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">statistics</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">all_num_steps</span><span class="p">),</span><span class="w"> </span><span class="k">statistics</span><span class="p">.</span><span class="nf">stdev</span><span class="p">(</span><span class="w"></span>

<span class="w">            </span><span class="n">all_num_steps</span><span class="p">),</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">all_num_steps</span><span class="p">),</span><span class="w"> </span><span class="nf">max</span><span class="p">(</span><span class="n">all_num_steps</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Steps</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mean_steps</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">=</span><span class="n">std_steps</span><span class="p">,</span><span class="w"> </span><span class="nf">min</span><span class="o">=</span><span class="n">min_steps</span><span class="p">,</span><span class="w"> </span><span class="nf">max</span><span class="o">=</span><span class="n">max_steps</span><span class="p">,</span><span class="w"> </span><span class="ow">all</span><span class="o">=</span><span class="n">all_num_steps</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">metrics</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Metrics</span><span class="p">(</span><span class="n">rewards</span><span class="o">=</span><span class="n">rewards</span><span class="p">,</span><span class="w"> </span><span class="n">steps</span><span class="o">=</span><span class="n">steps</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">metrics</span><span class="w"></span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">play</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"></span>

<span class="w">             </span><span class="nl">callbacks</span><span class="p">:</span><span class="w"> </span><span class="ow">Union</span><span class="o">[</span><span class="n">List[core.AgentCallback</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="err">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">None</span><span class="p">,</span><span class="w"></span>

<span class="w">             </span><span class="nl">num_episodes</span><span class="p">:</span><span class="w"> </span><span class="nc">int</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"></span>

<span class="w">             </span><span class="nl">max_steps_per_episode</span><span class="p">:</span><span class="w"> </span><span class="nc">int</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1000</span><span class="p">,</span><span class="w"></span>

<span class="w">             </span><span class="nl">play_context</span><span class="p">:</span><span class="w"> </span><span class="n">core</span><span class="p">.</span><span class="n">PlayContext</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">None</span><span class="p">,</span><span class="w"></span>

<span class="w">             </span><span class="nl">default_plots</span><span class="p">:</span><span class="w"> </span><span class="n">bool</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">None</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">        </span><span class="ss">&quot;&quot;&quot;Plays num_episodes with the current policy.</span>

<span class="ss">        Args:</span>

<span class="ss">            callbacks: list of callbacks called during each episode play</span>

<span class="ss">            num_episodes: number of episodes to play</span>

<span class="ss">            max_steps_per_episode: max steps per episode</span>

<span class="ss">            play_context: play configuration to be used. If set override all other play context arguments</span>

<span class="ss">            default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...)</span>

<span class="ss">        Returns:</span>

<span class="ss">            play_context containg the actions taken and the rewards received during training</span>

<span class="ss">        &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">        </span><span class="n">assert</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_backend_agent</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">_is_policy_trained</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;No trained policy available. Call train() first.&quot;</span><span class="w"></span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">play_context</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">None</span><span class="err">:</span><span class="w"></span>

<span class="w">            </span><span class="n">play_context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">core</span><span class="p">.</span><span class="n">PlayContext</span><span class="p">()</span><span class="w"></span>

<span class="w">            </span><span class="n">play_context</span><span class="p">.</span><span class="n">max_steps_per_episode</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">max_steps_per_episode</span><span class="w"></span>

<span class="w">            </span><span class="n">play_context</span><span class="p">.</span><span class="n">num_episodes</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">num_episodes</span><span class="w"></span>

<span class="w">        </span><span class="n">callbacks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_to_callback_list</span><span class="p">(</span><span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">callbacks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_add_plot_callbacks</span><span class="p">(</span><span class="n">callbacks</span><span class="p">,</span><span class="w"> </span><span class="n">default_plots</span><span class="p">,</span><span class="w"> </span><span class="o">[</span><span class="n">plot.Steps(), plot.Rewards()</span><span class="o">]</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">_backend_agent</span><span class="p">.</span><span class="n">play</span><span class="p">(</span><span class="n">play_context</span><span class="o">=</span><span class="n">play_context</span><span class="p">,</span><span class="w"> </span><span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">play_context</span><span class="w"></span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="k">save</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="nl">directory</span><span class="p">:</span><span class="w"> </span><span class="n">Optional</span><span class="o">[</span><span class="n">str</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">None</span><span class="p">,</span><span class="w"></span>

<span class="w">             </span><span class="nl">callbacks</span><span class="p">:</span><span class="w"> </span><span class="ow">Union</span><span class="o">[</span><span class="n">List[core.AgentCallback</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="err">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">None</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="nf">str</span><span class="err">:</span><span class="w"></span>

<span class="w">        </span><span class="ss">&quot;&quot;&quot;Saves the currently trained actor policy in directory.</span>

<span class="ss">        If save is called before a trained policy is created, eg by calling train, an exception is raised.</span>

<span class="ss">        Args:</span>

<span class="ss">             directory: the directory to save the policy weights to.</span>

<span class="ss">                if the directory does not exist yet, a new directory is created. if None the policy is saved</span>

<span class="ss">                in a temp directory.</span>

<span class="ss">             callbacks: list of callbacks called during save (eg log.Agent)</span>

<span class="ss">        Returns:</span>

<span class="ss">            the absolute path to the directory containing the saved policy.</span>

<span class="ss">        &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">directory</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">None</span><span class="err">:</span><span class="w"></span>

<span class="w">            </span><span class="n">directory</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bcore</span><span class="p">.</span><span class="n">_get_temp_path</span><span class="p">()</span><span class="w"></span>

<span class="w">        </span><span class="n">assert</span><span class="w"> </span><span class="n">directory</span><span class="w"></span>

<span class="w">        </span><span class="n">assert</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_backend_agent</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">_is_policy_trained</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;No trained policy available. Call train() first.&quot;</span><span class="w"></span>

<span class="w">        </span><span class="n">directory</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bcore</span><span class="p">.</span><span class="n">_mkdir</span><span class="p">(</span><span class="n">directory</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">agent_json_path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">os</span><span class="p">.</span><span class="k">path</span><span class="p">.</span><span class="k">join</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span><span class="w"> </span><span class="n">EasyAgent</span><span class="p">.</span><span class="n">_KEY_EASYAGENT_FILENAME</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">with</span><span class="w"> </span><span class="k">open</span><span class="p">(</span><span class="n">agent_json_path</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;w&#39;</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nl">jsonfile</span><span class="p">:</span><span class="w"></span>

<span class="w">            </span><span class="n">agent_dict</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_to_dict</span><span class="p">()</span><span class="w"></span>

<span class="w">            </span><span class="n">json</span><span class="p">.</span><span class="k">dump</span><span class="p">(</span><span class="n">agent_dict</span><span class="p">,</span><span class="w"> </span><span class="n">jsonfile</span><span class="p">,</span><span class="w"> </span><span class="n">sort_keys</span><span class="o">=</span><span class="k">True</span><span class="p">,</span><span class="w"> </span><span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">callbacks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_to_callback_list</span><span class="p">(</span><span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">policy_directory</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">os</span><span class="p">.</span><span class="k">path</span><span class="p">.</span><span class="k">join</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span><span class="w"> </span><span class="n">EasyAgent</span><span class="p">.</span><span class="n">_KEY_POLICY_DIRECTORY</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">policy_directory</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bcore</span><span class="p">.</span><span class="n">_mkdir</span><span class="p">(</span><span class="n">policy_directory</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">_backend_agent</span><span class="p">.</span><span class="k">save</span><span class="p">(</span><span class="n">directory</span><span class="o">=</span><span class="n">policy_directory</span><span class="p">,</span><span class="w"> </span><span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">directory</span><span class="w"></span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">train</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="nl">train_context</span><span class="p">:</span><span class="w"> </span><span class="n">core</span><span class="p">.</span><span class="n">TrainContext</span><span class="p">,</span><span class="w"></span>

<span class="w">              </span><span class="nl">callbacks</span><span class="p">:</span><span class="w"> </span><span class="ow">Union</span><span class="o">[</span><span class="n">List[core.AgentCallback</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="err">]</span><span class="p">,</span><span class="w"></span>

<span class="w">              </span><span class="nl">default_plots</span><span class="p">:</span><span class="w"> </span><span class="n">Optional</span><span class="o">[</span><span class="n">bool</span><span class="o">]</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">        </span><span class="ss">&quot;&quot;&quot;Trains a new model using the gym environment passed during instantiation.</span>

<span class="ss">        Args:</span>

<span class="ss">            callbacks: list of callbacks called during the training and evaluation</span>

<span class="ss">            train_context: training configuration to be used (num_iterations,num_episodes_per_iteration,...)</span>

<span class="ss">            default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...).</span>

<span class="ss">                if None default callbacks are only added if the callbacks list is empty</span>

<span class="ss">        &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">        </span><span class="n">assert</span><span class="w"> </span><span class="n">train_context</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;train_context not set.&quot;</span><span class="w"></span>

<span class="w">        </span><span class="n">callbacks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_to_callback_list</span><span class="p">(</span><span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">callbacks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_add_plot_callbacks</span><span class="p">(</span><span class="n">callbacks</span><span class="p">,</span><span class="w"> </span><span class="n">default_plots</span><span class="p">,</span><span class="w"> </span><span class="o">[</span><span class="n">plot.Loss(), plot.Steps(), plot.Rewards()</span><span class="o">]</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">_backend_agent</span><span class="p">.</span><span class="n">train</span><span class="p">(</span><span class="n">train_context</span><span class="o">=</span><span class="n">train_context</span><span class="p">,</span><span class="w"> </span><span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span><span class="w"></span>
</pre></div>


</details>
<hr />
<h4 id="ancestors-in-mro_4">Ancestors (in MRO)</h4>
<ul>
<li>abc.ABC</li>
</ul>
<h4 id="descendants_1">Descendants</h4>
<ul>
<li>easyagents.agents.CemAgent</li>
<li>easyagents.agents.DqnAgent</li>
<li>easyagents.agents.PpoAgent</li>
<li>easyagents.agents.RandomAgent</li>
<li>easyagents.agents.ReinforceAgent</li>
</ul>
<h4 id="methods_4">Methods</h4>
<h5 id="evaluate_4">evaluate</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">num_episodes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span>
<span class="p">)</span>
</pre></div>


<p>Plays num_episodes with the current policy and computes metrics on rewards.</p>
<p>Args:
    num_episodes: number of episodes to play
    max_steps_per_episode: max steps per episode</p>
<p>Returns:
    extensible score metrics</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">evaluate</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="nl">num_episodes</span><span class="p">:</span><span class="w"> </span><span class="nc">int</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">50</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="nl">max_steps_per_episode</span><span class="p">:</span><span class="w"> </span><span class="nc">int</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">50</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">        </span><span class="ss">&quot;&quot;&quot;Plays num_episodes with the current policy and computes metrics on rewards.</span>

<span class="ss">        Args:</span>

<span class="ss">            num_episodes: number of episodes to play</span>

<span class="ss">            max_steps_per_episode: max steps per episode</span>

<span class="ss">        Returns:</span>

<span class="ss">            extensible score metrics</span>

<span class="ss">        &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">        </span><span class="n">play_context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">core</span><span class="p">.</span><span class="n">PlayContext</span><span class="p">()</span><span class="w"></span>

<span class="w">        </span><span class="n">play_context</span><span class="p">.</span><span class="n">max_steps_per_episode</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">max_steps_per_episode</span><span class="w"></span>

<span class="w">        </span><span class="n">play_context</span><span class="p">.</span><span class="n">num_episodes</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">num_episodes</span><span class="w"></span>

<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">play</span><span class="p">(</span><span class="n">play_context</span><span class="o">=</span><span class="n">play_context</span><span class="p">,</span><span class="w"> </span><span class="n">default_plots</span><span class="o">=</span><span class="k">False</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">Metrics</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;Metrics&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;steps rewards&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">Rewards</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;Rewards&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;mean std min max all&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">all_rewards</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">list</span><span class="p">(</span><span class="n">play_context</span><span class="p">.</span><span class="n">sum_of_rewards</span><span class="p">.</span><span class="k">values</span><span class="p">())</span><span class="w"></span>

<span class="w">        </span><span class="n">mean_reward</span><span class="p">,</span><span class="w"> </span><span class="n">std_reward</span><span class="p">,</span><span class="w"> </span><span class="n">min_reward</span><span class="p">,</span><span class="w"> </span><span class="n">max_reward</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">statistics</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">all_rewards</span><span class="p">),</span><span class="w"> </span><span class="k">statistics</span><span class="p">.</span><span class="nf">stdev</span><span class="p">(</span><span class="w"></span>

<span class="w">            </span><span class="n">all_rewards</span><span class="p">),</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">all_rewards</span><span class="p">),</span><span class="w"> </span><span class="nf">max</span><span class="p">(</span><span class="n">all_rewards</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">rewards</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Rewards</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mean_reward</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">=</span><span class="n">std_reward</span><span class="p">,</span><span class="w"> </span><span class="nf">min</span><span class="o">=</span><span class="n">min_reward</span><span class="p">,</span><span class="w"> </span><span class="nf">max</span><span class="o">=</span><span class="n">max_reward</span><span class="p">,</span><span class="w"> </span><span class="ow">all</span><span class="o">=</span><span class="n">all_rewards</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">Steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;Steps&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;mean std min max all&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">all_num_steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">[]</span><span class="w"></span>

<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">play_context</span><span class="p">.</span><span class="n">rewards</span><span class="p">.</span><span class="n">keys</span><span class="p">()</span><span class="err">:</span><span class="w"></span>

<span class="w">            </span><span class="n">all_num_steps</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">play_context</span><span class="p">.</span><span class="n">rewards</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="p">))</span><span class="w"></span>

<span class="w">        </span><span class="n">mean_steps</span><span class="p">,</span><span class="w"> </span><span class="n">std_steps</span><span class="p">,</span><span class="w"> </span><span class="n">min_steps</span><span class="p">,</span><span class="w"> </span><span class="n">max_steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">statistics</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">all_num_steps</span><span class="p">),</span><span class="w"> </span><span class="k">statistics</span><span class="p">.</span><span class="nf">stdev</span><span class="p">(</span><span class="w"></span>

<span class="w">            </span><span class="n">all_num_steps</span><span class="p">),</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">all_num_steps</span><span class="p">),</span><span class="w"> </span><span class="nf">max</span><span class="p">(</span><span class="n">all_num_steps</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Steps</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mean_steps</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">=</span><span class="n">std_steps</span><span class="p">,</span><span class="w"> </span><span class="nf">min</span><span class="o">=</span><span class="n">min_steps</span><span class="p">,</span><span class="w"> </span><span class="nf">max</span><span class="o">=</span><span class="n">max_steps</span><span class="p">,</span><span class="w"> </span><span class="ow">all</span><span class="o">=</span><span class="n">all_num_steps</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">metrics</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Metrics</span><span class="p">(</span><span class="n">rewards</span><span class="o">=</span><span class="n">rewards</span><span class="p">,</span><span class="w"> </span><span class="n">steps</span><span class="o">=</span><span class="n">steps</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">metrics</span><span class="w"></span>
</pre></div>


</details>
<h5 id="play_4">play</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">play</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_episodes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="n">play_context</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">PlayContext</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">default_plots</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span>
</pre></div>


<p>Plays num_episodes with the current policy.</p>
<p>Args:
    callbacks: list of callbacks called during each episode play
    num_episodes: number of episodes to play
    max_steps_per_episode: max steps per episode
    play_context: play configuration to be used. If set override all other play context arguments
    default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...)</p>
<p>Returns:
    play_context containg the actions taken and the rewards received during training</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="n">def</span> <span class="n">play</span><span class="p">(</span><span class="k">self</span><span class="p">,</span>

             <span class="n">callbacks</span><span class="p">:</span> <span class="k">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="k">None</span><span class="p">]</span> <span class="o">=</span> <span class="k">None</span><span class="p">,</span>

             <span class="n">num_episodes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>

             <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>

             <span class="n">play_context</span><span class="p">:</span> <span class="n">core</span><span class="p">.</span><span class="n">PlayContext</span> <span class="o">=</span> <span class="k">None</span><span class="p">,</span>

             <span class="n">default_plots</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="k">None</span><span class="p">):</span>

        <span class="ss">&quot;&quot;&quot;Plays num_episodes with the current policy.</span>

<span class="ss">        Args:</span>

<span class="ss">            callbacks: list of callbacks called during each episode play</span>

<span class="ss">            num_episodes: number of episodes to play</span>

<span class="ss">            max_steps_per_episode: max steps per episode</span>

<span class="ss">            play_context: play configuration to be used. If set override all other play context arguments</span>

<span class="ss">            default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...)</span>

<span class="ss">        Returns:</span>

<span class="ss">            play_context containg the actions taken and the rewards received during training</span>

<span class="ss">        &quot;&quot;&quot;</span>

        <span class="n">assert</span> <span class="k">self</span><span class="p">.</span><span class="n">_backend_agent</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">_is_policy_trained</span><span class="p">,</span> <span class="ss">&quot;No trained policy available. Call train() first.&quot;</span>

        <span class="k">if</span> <span class="n">play_context</span> <span class="k">is</span> <span class="k">None</span><span class="p">:</span>

            <span class="n">play_context</span> <span class="o">=</span> <span class="n">core</span><span class="p">.</span><span class="n">PlayContext</span><span class="p">()</span>

            <span class="n">play_context</span><span class="p">.</span><span class="n">max_steps_per_episode</span> <span class="o">=</span> <span class="n">max_steps_per_episode</span>

            <span class="n">play_context</span><span class="p">.</span><span class="n">num_episodes</span> <span class="o">=</span> <span class="n">num_episodes</span>

        <span class="n">callbacks</span> <span class="o">=</span> <span class="k">self</span><span class="p">.</span><span class="n">_to_callback_list</span><span class="p">(</span><span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span>

        <span class="n">callbacks</span> <span class="o">=</span> <span class="k">self</span><span class="p">.</span><span class="n">_add_plot_callbacks</span><span class="p">(</span><span class="n">callbacks</span><span class="p">,</span> <span class="n">default_plots</span><span class="p">,</span> <span class="p">[</span><span class="n">plot</span><span class="p">.</span><span class="n">Steps</span><span class="p">(),</span> <span class="n">plot</span><span class="p">.</span><span class="n">Rewards</span><span class="p">()])</span>

        <span class="k">self</span><span class="p">.</span><span class="n">_backend_agent</span><span class="p">.</span><span class="n">play</span><span class="p">(</span><span class="n">play_context</span><span class="o">=</span><span class="n">play_context</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">play_context</span>
</pre></div>


</details>
<h5 id="save_4">save</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">save</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span>
</pre></div>


<p>Saves the currently trained actor policy in directory.</p>
<p>If save is called before a trained policy is created, eg by calling train, an exception is raised.</p>
<p>Args:
     directory: the directory to save the policy weights to.
        if the directory does not exist yet, a new directory is created. if None the policy is saved
        in a temp directory.
     callbacks: list of callbacks called during save (eg log.Agent)</p>
<p>Returns:
    the absolute path to the directory containing the saved policy.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="k">save</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="nl">directory</span><span class="p">:</span><span class="w"> </span><span class="n">Optional</span><span class="o">[</span><span class="n">str</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">None</span><span class="p">,</span><span class="w"></span>

<span class="w">             </span><span class="nl">callbacks</span><span class="p">:</span><span class="w"> </span><span class="ow">Union</span><span class="o">[</span><span class="n">List[core.AgentCallback</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="err">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">None</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="nf">str</span><span class="err">:</span><span class="w"></span>

<span class="w">        </span><span class="ss">&quot;&quot;&quot;Saves the currently trained actor policy in directory.</span>

<span class="ss">        If save is called before a trained policy is created, eg by calling train, an exception is raised.</span>

<span class="ss">        Args:</span>

<span class="ss">             directory: the directory to save the policy weights to.</span>

<span class="ss">                if the directory does not exist yet, a new directory is created. if None the policy is saved</span>

<span class="ss">                in a temp directory.</span>

<span class="ss">             callbacks: list of callbacks called during save (eg log.Agent)</span>

<span class="ss">        Returns:</span>

<span class="ss">            the absolute path to the directory containing the saved policy.</span>

<span class="ss">        &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">directory</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">None</span><span class="err">:</span><span class="w"></span>

<span class="w">            </span><span class="n">directory</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bcore</span><span class="p">.</span><span class="n">_get_temp_path</span><span class="p">()</span><span class="w"></span>

<span class="w">        </span><span class="n">assert</span><span class="w"> </span><span class="n">directory</span><span class="w"></span>

<span class="w">        </span><span class="n">assert</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_backend_agent</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">_is_policy_trained</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;No trained policy available. Call train() first.&quot;</span><span class="w"></span>

<span class="w">        </span><span class="n">directory</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bcore</span><span class="p">.</span><span class="n">_mkdir</span><span class="p">(</span><span class="n">directory</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">agent_json_path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">os</span><span class="p">.</span><span class="k">path</span><span class="p">.</span><span class="k">join</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span><span class="w"> </span><span class="n">EasyAgent</span><span class="p">.</span><span class="n">_KEY_EASYAGENT_FILENAME</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">with</span><span class="w"> </span><span class="k">open</span><span class="p">(</span><span class="n">agent_json_path</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;w&#39;</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nl">jsonfile</span><span class="p">:</span><span class="w"></span>

<span class="w">            </span><span class="n">agent_dict</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_to_dict</span><span class="p">()</span><span class="w"></span>

<span class="w">            </span><span class="n">json</span><span class="p">.</span><span class="k">dump</span><span class="p">(</span><span class="n">agent_dict</span><span class="p">,</span><span class="w"> </span><span class="n">jsonfile</span><span class="p">,</span><span class="w"> </span><span class="n">sort_keys</span><span class="o">=</span><span class="k">True</span><span class="p">,</span><span class="w"> </span><span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">callbacks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_to_callback_list</span><span class="p">(</span><span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">policy_directory</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">os</span><span class="p">.</span><span class="k">path</span><span class="p">.</span><span class="k">join</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span><span class="w"> </span><span class="n">EasyAgent</span><span class="p">.</span><span class="n">_KEY_POLICY_DIRECTORY</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">policy_directory</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bcore</span><span class="p">.</span><span class="n">_mkdir</span><span class="p">(</span><span class="n">policy_directory</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">_backend_agent</span><span class="p">.</span><span class="k">save</span><span class="p">(</span><span class="n">directory</span><span class="o">=</span><span class="n">policy_directory</span><span class="p">,</span><span class="w"> </span><span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">directory</span><span class="w"></span>
</pre></div>


</details>
<h5 id="train_4">train</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">train_context</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">TrainContext</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">],</span>
    <span class="n">default_plots</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>


<p>Trains a new model using the gym environment passed during instantiation.</p>
<p>Args:
    callbacks: list of callbacks called during the training and evaluation
    train_context: training configuration to be used (num_iterations,num_episodes_per_iteration,...)
    default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...).
        if None default callbacks are only added if the callbacks list is empty</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">train</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="nl">train_context</span><span class="p">:</span><span class="w"> </span><span class="n">core</span><span class="p">.</span><span class="n">TrainContext</span><span class="p">,</span><span class="w"></span>

<span class="w">              </span><span class="nl">callbacks</span><span class="p">:</span><span class="w"> </span><span class="ow">Union</span><span class="o">[</span><span class="n">List[core.AgentCallback</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="err">]</span><span class="p">,</span><span class="w"></span>

<span class="w">              </span><span class="nl">default_plots</span><span class="p">:</span><span class="w"> </span><span class="n">Optional</span><span class="o">[</span><span class="n">bool</span><span class="o">]</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">        </span><span class="ss">&quot;&quot;&quot;Trains a new model using the gym environment passed during instantiation.</span>

<span class="ss">        Args:</span>

<span class="ss">            callbacks: list of callbacks called during the training and evaluation</span>

<span class="ss">            train_context: training configuration to be used (num_iterations,num_episodes_per_iteration,...)</span>

<span class="ss">            default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...).</span>

<span class="ss">                if None default callbacks are only added if the callbacks list is empty</span>

<span class="ss">        &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">        </span><span class="n">assert</span><span class="w"> </span><span class="n">train_context</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;train_context not set.&quot;</span><span class="w"></span>

<span class="w">        </span><span class="n">callbacks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_to_callback_list</span><span class="p">(</span><span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">callbacks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_add_plot_callbacks</span><span class="p">(</span><span class="n">callbacks</span><span class="p">,</span><span class="w"> </span><span class="n">default_plots</span><span class="p">,</span><span class="w"> </span><span class="o">[</span><span class="n">plot.Loss(), plot.Steps(), plot.Rewards()</span><span class="o">]</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">_backend_agent</span><span class="p">.</span><span class="n">train</span><span class="p">(</span><span class="n">train_context</span><span class="o">=</span><span class="n">train_context</span><span class="p">,</span><span class="w"> </span><span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span><span class="w"></span>
</pre></div>


</details>
<h3 id="ppoagent">PpoAgent</h3>
<div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">PpoAgent</span><span class="p">(</span>
    <span class="n">gym_env_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">fc_layers</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="nb">int</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">backend</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span>
</pre></div>


<p>creates a new agent based on the PPO algorithm.</p>
<p>PPO is an actor-critic algorithm using 2 neural networks. The actor network
to predict the next action to be taken and the critic network to estimate
the value of the game state we are currently in (the expected, discounted
sum of future rewards when following the current actor network).</p>
<p>see also: https://spinningup.openai.com/en/latest/algorithms/ppo.html</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><span class="k">class</span> <span class="n">PpoAgent</span>(<span class="n">EasyAgent</span>):

    <span class="s">&quot;&quot;&quot;creates a new agent based on the PPO algorithm.</span>

<span class="s">        PPO is an actor-critic algorithm using 2 neural networks. The actor network</span>

<span class="s">        to predict the next action to be taken and the critic network to estimate</span>

<span class="s">        the value of the game state we are currently in (the expected, discounted</span>

<span class="s">        sum of future rewards when following the current actor network).</span>

<span class="s">        see also: https://spinningup.openai.com/en/latest/algorithms/ppo.html</span>

<span class="s">    &quot;&quot;&quot;</span>

    <span class="n">def</span> <span class="n">train</span>(<span class="k">self</span>,

              <span class="n">callbacks:</span> <span class="n">Union</span>[<span class="nb">List</span>[<span class="n">core</span>.<span class="n">AgentCallback</span>], <span class="n">core</span>.<span class="n">AgentCallback</span>, <span class="n">None</span>] = <span class="n">None</span>,

              <span class="n">num_iterations:</span> <span class="nb">int</span> = <span class="mi">100</span>,

              <span class="n">num_episodes_per_iteration:</span> <span class="nb">int</span> = <span class="mi">10</span>,

              <span class="n">max_steps_per_episode:</span> <span class="nb">int</span> = <span class="mi">500</span>,

              <span class="n">num_epochs_per_iteration:</span> <span class="nb">int</span> = <span class="mi">10</span>,

              <span class="n">num_iterations_between_eval:</span> <span class="nb">int</span> = <span class="mi">5</span>,

              <span class="n">num_episodes_per_eval:</span> <span class="nb">int</span> = <span class="mi">10</span>,

              <span class="n">learning_rate:</span> <span class="n">float</span> = <span class="mf">0.001</span>,

              <span class="n">train_context:</span> <span class="n">core</span>.<span class="n">PpoTrainContext</span> = <span class="n">None</span>,

              <span class="n">default_plots:</span> <span class="nb">bool</span> = <span class="n">None</span>):

        <span class="s">&quot;&quot;&quot;Trains a new model using the gym environment passed during instantiation.</span>

<span class="s">        Args:</span>

<span class="s">            callbacks: list of callbacks called during training and evaluation</span>

<span class="s">            num_iterations: number of times the training is repeated (with additional data)</span>

<span class="s">            num_episodes_per_iteration: number of episodes played per training iteration</span>

<span class="s">            max_steps_per_episode: maximum number of steps per episode</span>

<span class="s">            num_epochs_per_iteration: number of times the data collected for the current iteration</span>

<span class="s">                is used to retrain the current policy</span>

<span class="s">            num_iterations_between_eval: number of training iterations before the current policy is evaluated.</span>

<span class="s">                if 0 no evaluation is performed.</span>

<span class="s">            num_episodes_per_eval: number of episodes played to estimate the average return and steps</span>

<span class="s">            learning_rate: the learning rate used in the next iteration&#39;s policy training (0,1]</span>

<span class="s">            train_context: training configuration to be used. if set overrides all other training context arguments.</span>

<span class="s">            default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...).</span>

<span class="s">                if None default callbacks are only added if the callbacks list is empty</span>

<span class="s">        Returns:</span>

<span class="s">            train_context: the training configuration containing the loss and sum of rewards encountered</span>

<span class="s">                during training</span>

<span class="s">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">train_context</span> <span class="k">is</span> <span class="n">None:</span>

            <span class="n">train_context</span> = <span class="n">core</span>.<span class="n">PpoTrainContext</span>()

            <span class="n">train_context</span>.<span class="n">num_iterations</span> = <span class="n">num_iterations</span>

            <span class="n">train_context</span>.<span class="n">num_episodes_per_iteration</span> = <span class="n">num_episodes_per_iteration</span>

            <span class="n">train_context</span>.<span class="n">max_steps_per_episode</span> = <span class="n">max_steps_per_episode</span>

            <span class="n">train_context</span>.<span class="n">num_epochs_per_iteration</span> = <span class="n">num_epochs_per_iteration</span>

            <span class="n">train_context</span>.<span class="n">num_iterations_between_eval</span> = <span class="n">num_iterations_between_eval</span>

            <span class="n">train_context</span>.<span class="n">num_episodes_per_eval</span> = <span class="n">num_episodes_per_eval</span>

            <span class="n">train_context</span>.<span class="n">learning_rate</span> = <span class="n">learning_rate</span>

        <span class="n">super</span>().<span class="n">train</span>(<span class="n">train_context</span>=<span class="n">train_context</span>, <span class="n">callbacks</span>=<span class="n">callbacks</span>, <span class="n">default_plots</span>=<span class="n">default_plots</span>)

        <span class="k">return</span> <span class="n">train_context</span>
</pre></div>


</details>
<hr />
<h4 id="ancestors-in-mro_5">Ancestors (in MRO)</h4>
<ul>
<li>easyagents.agents.EasyAgent</li>
<li>abc.ABC</li>
</ul>
<h4 id="methods_5">Methods</h4>
<h5 id="evaluate_5">evaluate</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">num_episodes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span>
<span class="p">)</span>
</pre></div>


<p>Plays num_episodes with the current policy and computes metrics on rewards.</p>
<p>Args:
    num_episodes: number of episodes to play
    max_steps_per_episode: max steps per episode</p>
<p>Returns:
    extensible score metrics</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">evaluate</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="nl">num_episodes</span><span class="p">:</span><span class="w"> </span><span class="nc">int</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">50</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="nl">max_steps_per_episode</span><span class="p">:</span><span class="w"> </span><span class="nc">int</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">50</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">        </span><span class="ss">&quot;&quot;&quot;Plays num_episodes with the current policy and computes metrics on rewards.</span>

<span class="ss">        Args:</span>

<span class="ss">            num_episodes: number of episodes to play</span>

<span class="ss">            max_steps_per_episode: max steps per episode</span>

<span class="ss">        Returns:</span>

<span class="ss">            extensible score metrics</span>

<span class="ss">        &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">        </span><span class="n">play_context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">core</span><span class="p">.</span><span class="n">PlayContext</span><span class="p">()</span><span class="w"></span>

<span class="w">        </span><span class="n">play_context</span><span class="p">.</span><span class="n">max_steps_per_episode</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">max_steps_per_episode</span><span class="w"></span>

<span class="w">        </span><span class="n">play_context</span><span class="p">.</span><span class="n">num_episodes</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">num_episodes</span><span class="w"></span>

<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">play</span><span class="p">(</span><span class="n">play_context</span><span class="o">=</span><span class="n">play_context</span><span class="p">,</span><span class="w"> </span><span class="n">default_plots</span><span class="o">=</span><span class="k">False</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">Metrics</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;Metrics&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;steps rewards&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">Rewards</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;Rewards&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;mean std min max all&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">all_rewards</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">list</span><span class="p">(</span><span class="n">play_context</span><span class="p">.</span><span class="n">sum_of_rewards</span><span class="p">.</span><span class="k">values</span><span class="p">())</span><span class="w"></span>

<span class="w">        </span><span class="n">mean_reward</span><span class="p">,</span><span class="w"> </span><span class="n">std_reward</span><span class="p">,</span><span class="w"> </span><span class="n">min_reward</span><span class="p">,</span><span class="w"> </span><span class="n">max_reward</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">statistics</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">all_rewards</span><span class="p">),</span><span class="w"> </span><span class="k">statistics</span><span class="p">.</span><span class="nf">stdev</span><span class="p">(</span><span class="w"></span>

<span class="w">            </span><span class="n">all_rewards</span><span class="p">),</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">all_rewards</span><span class="p">),</span><span class="w"> </span><span class="nf">max</span><span class="p">(</span><span class="n">all_rewards</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">rewards</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Rewards</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mean_reward</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">=</span><span class="n">std_reward</span><span class="p">,</span><span class="w"> </span><span class="nf">min</span><span class="o">=</span><span class="n">min_reward</span><span class="p">,</span><span class="w"> </span><span class="nf">max</span><span class="o">=</span><span class="n">max_reward</span><span class="p">,</span><span class="w"> </span><span class="ow">all</span><span class="o">=</span><span class="n">all_rewards</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">Steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;Steps&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;mean std min max all&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">all_num_steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">[]</span><span class="w"></span>

<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">play_context</span><span class="p">.</span><span class="n">rewards</span><span class="p">.</span><span class="n">keys</span><span class="p">()</span><span class="err">:</span><span class="w"></span>

<span class="w">            </span><span class="n">all_num_steps</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">play_context</span><span class="p">.</span><span class="n">rewards</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="p">))</span><span class="w"></span>

<span class="w">        </span><span class="n">mean_steps</span><span class="p">,</span><span class="w"> </span><span class="n">std_steps</span><span class="p">,</span><span class="w"> </span><span class="n">min_steps</span><span class="p">,</span><span class="w"> </span><span class="n">max_steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">statistics</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">all_num_steps</span><span class="p">),</span><span class="w"> </span><span class="k">statistics</span><span class="p">.</span><span class="nf">stdev</span><span class="p">(</span><span class="w"></span>

<span class="w">            </span><span class="n">all_num_steps</span><span class="p">),</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">all_num_steps</span><span class="p">),</span><span class="w"> </span><span class="nf">max</span><span class="p">(</span><span class="n">all_num_steps</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Steps</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mean_steps</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">=</span><span class="n">std_steps</span><span class="p">,</span><span class="w"> </span><span class="nf">min</span><span class="o">=</span><span class="n">min_steps</span><span class="p">,</span><span class="w"> </span><span class="nf">max</span><span class="o">=</span><span class="n">max_steps</span><span class="p">,</span><span class="w"> </span><span class="ow">all</span><span class="o">=</span><span class="n">all_num_steps</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">metrics</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Metrics</span><span class="p">(</span><span class="n">rewards</span><span class="o">=</span><span class="n">rewards</span><span class="p">,</span><span class="w"> </span><span class="n">steps</span><span class="o">=</span><span class="n">steps</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">metrics</span><span class="w"></span>
</pre></div>


</details>
<h5 id="play_5">play</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">play</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_episodes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="n">play_context</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">PlayContext</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">default_plots</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span>
</pre></div>


<p>Plays num_episodes with the current policy.</p>
<p>Args:
    callbacks: list of callbacks called during each episode play
    num_episodes: number of episodes to play
    max_steps_per_episode: max steps per episode
    play_context: play configuration to be used. If set override all other play context arguments
    default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...)</p>
<p>Returns:
    play_context containg the actions taken and the rewards received during training</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="n">def</span> <span class="n">play</span><span class="p">(</span><span class="k">self</span><span class="p">,</span>

             <span class="n">callbacks</span><span class="p">:</span> <span class="k">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="k">None</span><span class="p">]</span> <span class="o">=</span> <span class="k">None</span><span class="p">,</span>

             <span class="n">num_episodes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>

             <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>

             <span class="n">play_context</span><span class="p">:</span> <span class="n">core</span><span class="p">.</span><span class="n">PlayContext</span> <span class="o">=</span> <span class="k">None</span><span class="p">,</span>

             <span class="n">default_plots</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="k">None</span><span class="p">):</span>

        <span class="ss">&quot;&quot;&quot;Plays num_episodes with the current policy.</span>

<span class="ss">        Args:</span>

<span class="ss">            callbacks: list of callbacks called during each episode play</span>

<span class="ss">            num_episodes: number of episodes to play</span>

<span class="ss">            max_steps_per_episode: max steps per episode</span>

<span class="ss">            play_context: play configuration to be used. If set override all other play context arguments</span>

<span class="ss">            default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...)</span>

<span class="ss">        Returns:</span>

<span class="ss">            play_context containg the actions taken and the rewards received during training</span>

<span class="ss">        &quot;&quot;&quot;</span>

        <span class="n">assert</span> <span class="k">self</span><span class="p">.</span><span class="n">_backend_agent</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">_is_policy_trained</span><span class="p">,</span> <span class="ss">&quot;No trained policy available. Call train() first.&quot;</span>

        <span class="k">if</span> <span class="n">play_context</span> <span class="k">is</span> <span class="k">None</span><span class="p">:</span>

            <span class="n">play_context</span> <span class="o">=</span> <span class="n">core</span><span class="p">.</span><span class="n">PlayContext</span><span class="p">()</span>

            <span class="n">play_context</span><span class="p">.</span><span class="n">max_steps_per_episode</span> <span class="o">=</span> <span class="n">max_steps_per_episode</span>

            <span class="n">play_context</span><span class="p">.</span><span class="n">num_episodes</span> <span class="o">=</span> <span class="n">num_episodes</span>

        <span class="n">callbacks</span> <span class="o">=</span> <span class="k">self</span><span class="p">.</span><span class="n">_to_callback_list</span><span class="p">(</span><span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span>

        <span class="n">callbacks</span> <span class="o">=</span> <span class="k">self</span><span class="p">.</span><span class="n">_add_plot_callbacks</span><span class="p">(</span><span class="n">callbacks</span><span class="p">,</span> <span class="n">default_plots</span><span class="p">,</span> <span class="p">[</span><span class="n">plot</span><span class="p">.</span><span class="n">Steps</span><span class="p">(),</span> <span class="n">plot</span><span class="p">.</span><span class="n">Rewards</span><span class="p">()])</span>

        <span class="k">self</span><span class="p">.</span><span class="n">_backend_agent</span><span class="p">.</span><span class="n">play</span><span class="p">(</span><span class="n">play_context</span><span class="o">=</span><span class="n">play_context</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">play_context</span>
</pre></div>


</details>
<h5 id="save_5">save</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">save</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span>
</pre></div>


<p>Saves the currently trained actor policy in directory.</p>
<p>If save is called before a trained policy is created, eg by calling train, an exception is raised.</p>
<p>Args:
     directory: the directory to save the policy weights to.
        if the directory does not exist yet, a new directory is created. if None the policy is saved
        in a temp directory.
     callbacks: list of callbacks called during save (eg log.Agent)</p>
<p>Returns:
    the absolute path to the directory containing the saved policy.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="k">save</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="nl">directory</span><span class="p">:</span><span class="w"> </span><span class="n">Optional</span><span class="o">[</span><span class="n">str</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">None</span><span class="p">,</span><span class="w"></span>

<span class="w">             </span><span class="nl">callbacks</span><span class="p">:</span><span class="w"> </span><span class="ow">Union</span><span class="o">[</span><span class="n">List[core.AgentCallback</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="err">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">None</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="nf">str</span><span class="err">:</span><span class="w"></span>

<span class="w">        </span><span class="ss">&quot;&quot;&quot;Saves the currently trained actor policy in directory.</span>

<span class="ss">        If save is called before a trained policy is created, eg by calling train, an exception is raised.</span>

<span class="ss">        Args:</span>

<span class="ss">             directory: the directory to save the policy weights to.</span>

<span class="ss">                if the directory does not exist yet, a new directory is created. if None the policy is saved</span>

<span class="ss">                in a temp directory.</span>

<span class="ss">             callbacks: list of callbacks called during save (eg log.Agent)</span>

<span class="ss">        Returns:</span>

<span class="ss">            the absolute path to the directory containing the saved policy.</span>

<span class="ss">        &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">directory</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">None</span><span class="err">:</span><span class="w"></span>

<span class="w">            </span><span class="n">directory</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bcore</span><span class="p">.</span><span class="n">_get_temp_path</span><span class="p">()</span><span class="w"></span>

<span class="w">        </span><span class="n">assert</span><span class="w"> </span><span class="n">directory</span><span class="w"></span>

<span class="w">        </span><span class="n">assert</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_backend_agent</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">_is_policy_trained</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;No trained policy available. Call train() first.&quot;</span><span class="w"></span>

<span class="w">        </span><span class="n">directory</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bcore</span><span class="p">.</span><span class="n">_mkdir</span><span class="p">(</span><span class="n">directory</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">agent_json_path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">os</span><span class="p">.</span><span class="k">path</span><span class="p">.</span><span class="k">join</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span><span class="w"> </span><span class="n">EasyAgent</span><span class="p">.</span><span class="n">_KEY_EASYAGENT_FILENAME</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">with</span><span class="w"> </span><span class="k">open</span><span class="p">(</span><span class="n">agent_json_path</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;w&#39;</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nl">jsonfile</span><span class="p">:</span><span class="w"></span>

<span class="w">            </span><span class="n">agent_dict</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_to_dict</span><span class="p">()</span><span class="w"></span>

<span class="w">            </span><span class="n">json</span><span class="p">.</span><span class="k">dump</span><span class="p">(</span><span class="n">agent_dict</span><span class="p">,</span><span class="w"> </span><span class="n">jsonfile</span><span class="p">,</span><span class="w"> </span><span class="n">sort_keys</span><span class="o">=</span><span class="k">True</span><span class="p">,</span><span class="w"> </span><span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">callbacks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_to_callback_list</span><span class="p">(</span><span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">policy_directory</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">os</span><span class="p">.</span><span class="k">path</span><span class="p">.</span><span class="k">join</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span><span class="w"> </span><span class="n">EasyAgent</span><span class="p">.</span><span class="n">_KEY_POLICY_DIRECTORY</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">policy_directory</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bcore</span><span class="p">.</span><span class="n">_mkdir</span><span class="p">(</span><span class="n">policy_directory</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">_backend_agent</span><span class="p">.</span><span class="k">save</span><span class="p">(</span><span class="n">directory</span><span class="o">=</span><span class="n">policy_directory</span><span class="p">,</span><span class="w"> </span><span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">directory</span><span class="w"></span>
</pre></div>


</details>
<h5 id="train_5">train</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">num_episodes_per_iteration</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span>
    <span class="n">num_epochs_per_iteration</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">num_iterations_between_eval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
    <span class="n">num_episodes_per_eval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">,</span>
    <span class="n">train_context</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">PpoTrainContext</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">default_plots</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span>
</pre></div>


<p>Trains a new model using the gym environment passed during instantiation.</p>
<p>Args:
    callbacks: list of callbacks called during training and evaluation
    num_iterations: number of times the training is repeated (with additional data)
    num_episodes_per_iteration: number of episodes played per training iteration
    max_steps_per_episode: maximum number of steps per episode
    num_epochs_per_iteration: number of times the data collected for the current iteration
        is used to retrain the current policy
    num_iterations_between_eval: number of training iterations before the current policy is evaluated.
        if 0 no evaluation is performed.
    num_episodes_per_eval: number of episodes played to estimate the average return and steps
    learning_rate: the learning rate used in the next iteration's policy training (0,1]
    train_context: training configuration to be used. if set overrides all other training context arguments.
    default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...).
        if None default callbacks are only added if the callbacks list is empty</p>
<p>Returns:
    train_context: the training configuration containing the loss and sum of rewards encountered
        during training</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="n">def</span> <span class="n">train</span><span class="p">(</span><span class="k">self</span><span class="p">,</span>

              <span class="n">callbacks</span><span class="p">:</span> <span class="k">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="k">None</span><span class="p">]</span> <span class="o">=</span> <span class="k">None</span><span class="p">,</span>

              <span class="n">num_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>

              <span class="n">num_episodes_per_iteration</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>

              <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span>

              <span class="n">num_epochs_per_iteration</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>

              <span class="n">num_iterations_between_eval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>

              <span class="n">num_episodes_per_eval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>

              <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">0</span><span class="p">.</span><span class="mi">001</span><span class="p">,</span>

              <span class="n">train_context</span><span class="p">:</span> <span class="n">core</span><span class="p">.</span><span class="n">PpoTrainContext</span> <span class="o">=</span> <span class="k">None</span><span class="p">,</span>

              <span class="n">default_plots</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="k">None</span><span class="p">):</span>

        <span class="ss">&quot;&quot;&quot;Trains a new model using the gym environment passed during instantiation.</span>

<span class="ss">        Args:</span>

<span class="ss">            callbacks: list of callbacks called during training and evaluation</span>

<span class="ss">            num_iterations: number of times the training is repeated (with additional data)</span>

<span class="ss">            num_episodes_per_iteration: number of episodes played per training iteration</span>

<span class="ss">            max_steps_per_episode: maximum number of steps per episode</span>

<span class="ss">            num_epochs_per_iteration: number of times the data collected for the current iteration</span>

<span class="ss">                is used to retrain the current policy</span>

<span class="ss">            num_iterations_between_eval: number of training iterations before the current policy is evaluated.</span>

<span class="ss">                if 0 no evaluation is performed.</span>

<span class="ss">            num_episodes_per_eval: number of episodes played to estimate the average return and steps</span>

<span class="ss">            learning_rate: the learning rate used in the next iteration&#39;s policy training (0,1]</span>

<span class="ss">            train_context: training configuration to be used. if set overrides all other training context arguments.</span>

<span class="ss">            default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...).</span>

<span class="ss">                if None default callbacks are only added if the callbacks list is empty</span>

<span class="ss">        Returns:</span>

<span class="ss">            train_context: the training configuration containing the loss and sum of rewards encountered</span>

<span class="ss">                during training</span>

<span class="ss">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">train_context</span> <span class="k">is</span> <span class="k">None</span><span class="p">:</span>

            <span class="n">train_context</span> <span class="o">=</span> <span class="n">core</span><span class="p">.</span><span class="n">PpoTrainContext</span><span class="p">()</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">num_iterations</span> <span class="o">=</span> <span class="n">num_iterations</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">num_episodes_per_iteration</span> <span class="o">=</span> <span class="n">num_episodes_per_iteration</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">max_steps_per_episode</span> <span class="o">=</span> <span class="n">max_steps_per_episode</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">num_epochs_per_iteration</span> <span class="o">=</span> <span class="n">num_epochs_per_iteration</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">num_iterations_between_eval</span> <span class="o">=</span> <span class="n">num_iterations_between_eval</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">num_episodes_per_eval</span> <span class="o">=</span> <span class="n">num_episodes_per_eval</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>

        <span class="n">super</span><span class="p">().</span><span class="n">train</span><span class="p">(</span><span class="n">train_context</span><span class="o">=</span><span class="n">train_context</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span> <span class="n">default_plots</span><span class="o">=</span><span class="n">default_plots</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">train_context</span>
</pre></div>


</details>
<h3 id="randomagent">RandomAgent</h3>
<div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">RandomAgent</span><span class="p">(</span>
    <span class="n">gym_env_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">fc_layers</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="nb">int</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">backend</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span>
</pre></div>


<p>Agent which always chooses uniform random actions.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><span class="k">class</span> <span class="n">RandomAgent</span>(<span class="n">EasyAgent</span>):

    <span class="s">&quot;&quot;&quot;Agent which always chooses uniform random actions.&quot;&quot;&quot;</span>

    <span class="n">def</span> <span class="n">train</span>(<span class="k">self</span>,

              <span class="n">callbacks:</span> <span class="n">Union</span>[<span class="nb">List</span>[<span class="n">core</span>.<span class="n">AgentCallback</span>], <span class="n">core</span>.<span class="n">AgentCallback</span>, <span class="n">None</span>] = <span class="n">None</span>,

              <span class="n">num_iterations:</span> <span class="nb">int</span> = <span class="mi">10</span>,

              <span class="n">max_steps_per_episode:</span> <span class="nb">int</span> = <span class="mi">1000</span>,

              <span class="n">num_episodes_per_eval:</span> <span class="nb">int</span> = <span class="mi">10</span>,

              <span class="n">train_context:</span> <span class="n">core</span>.<span class="n">TrainContext</span> = <span class="n">None</span>,

              <span class="n">default_plots:</span> <span class="nb">bool</span> = <span class="n">None</span>):

        <span class="s">&quot;&quot;&quot;Evaluates the environment using a uniform random policy.</span>

<span class="s">        The evaluation is performed in batches of num_episodes_per_eval episodes.</span>

<span class="s">        Args:</span>

<span class="s">            callbacks: list of callbacks called during training and evaluation</span>

<span class="s">            num_iterations: number of times a batch of num_episodes_per_eval episodes is evaluated.</span>

<span class="s">            max_steps_per_episode: maximum number of steps per episode</span>

<span class="s">            num_episodes_per_eval: number of episodes played to estimate the average return and steps</span>

<span class="s">            train_context: training configuration to be used. if set overrides all other training context arguments.</span>

<span class="s">            default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...)</span>

<span class="s">        Returns:</span>

<span class="s">            train_context: the training configuration containing the loss and sum of rewards encountered</span>

<span class="s">                during training</span>

<span class="s">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">train_context</span> <span class="k">is</span> <span class="n">None:</span>

            <span class="n">train_context</span> = <span class="n">core</span>.<span class="n">TrainContext</span>()

            <span class="n">train_context</span>.<span class="n">num_iterations</span> = <span class="n">num_iterations</span>

            <span class="n">train_context</span>.<span class="n">max_steps_per_episode</span> = <span class="n">max_steps_per_episode</span>

            <span class="n">train_context</span>.<span class="n">num_epochs_per_iteration</span> = <span class="mi">0</span>

            <span class="n">train_context</span>.<span class="n">num_iterations_between_eval</span> = <span class="mi">1</span>

            <span class="n">train_context</span>.<span class="n">num_episodes_per_eval</span> = <span class="n">num_episodes_per_eval</span>

            <span class="n">train_context</span>.<span class="n">learning_rate</span> = <span class="mi">1</span>

        <span class="n">super</span>().<span class="n">train</span>(<span class="n">train_context</span>=<span class="n">train_context</span>, <span class="n">callbacks</span>=<span class="n">callbacks</span>, <span class="n">default_plots</span>=<span class="n">default_plots</span>)

        <span class="k">return</span> <span class="n">train_context</span>
</pre></div>


</details>
<hr />
<h4 id="ancestors-in-mro_6">Ancestors (in MRO)</h4>
<ul>
<li>easyagents.agents.EasyAgent</li>
<li>abc.ABC</li>
</ul>
<h4 id="methods_6">Methods</h4>
<h5 id="evaluate_6">evaluate</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">num_episodes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span>
<span class="p">)</span>
</pre></div>


<p>Plays num_episodes with the current policy and computes metrics on rewards.</p>
<p>Args:
    num_episodes: number of episodes to play
    max_steps_per_episode: max steps per episode</p>
<p>Returns:
    extensible score metrics</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">evaluate</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="nl">num_episodes</span><span class="p">:</span><span class="w"> </span><span class="nc">int</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">50</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="nl">max_steps_per_episode</span><span class="p">:</span><span class="w"> </span><span class="nc">int</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">50</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">        </span><span class="ss">&quot;&quot;&quot;Plays num_episodes with the current policy and computes metrics on rewards.</span>

<span class="ss">        Args:</span>

<span class="ss">            num_episodes: number of episodes to play</span>

<span class="ss">            max_steps_per_episode: max steps per episode</span>

<span class="ss">        Returns:</span>

<span class="ss">            extensible score metrics</span>

<span class="ss">        &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">        </span><span class="n">play_context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">core</span><span class="p">.</span><span class="n">PlayContext</span><span class="p">()</span><span class="w"></span>

<span class="w">        </span><span class="n">play_context</span><span class="p">.</span><span class="n">max_steps_per_episode</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">max_steps_per_episode</span><span class="w"></span>

<span class="w">        </span><span class="n">play_context</span><span class="p">.</span><span class="n">num_episodes</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">num_episodes</span><span class="w"></span>

<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">play</span><span class="p">(</span><span class="n">play_context</span><span class="o">=</span><span class="n">play_context</span><span class="p">,</span><span class="w"> </span><span class="n">default_plots</span><span class="o">=</span><span class="k">False</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">Metrics</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;Metrics&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;steps rewards&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">Rewards</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;Rewards&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;mean std min max all&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">all_rewards</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">list</span><span class="p">(</span><span class="n">play_context</span><span class="p">.</span><span class="n">sum_of_rewards</span><span class="p">.</span><span class="k">values</span><span class="p">())</span><span class="w"></span>

<span class="w">        </span><span class="n">mean_reward</span><span class="p">,</span><span class="w"> </span><span class="n">std_reward</span><span class="p">,</span><span class="w"> </span><span class="n">min_reward</span><span class="p">,</span><span class="w"> </span><span class="n">max_reward</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">statistics</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">all_rewards</span><span class="p">),</span><span class="w"> </span><span class="k">statistics</span><span class="p">.</span><span class="nf">stdev</span><span class="p">(</span><span class="w"></span>

<span class="w">            </span><span class="n">all_rewards</span><span class="p">),</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">all_rewards</span><span class="p">),</span><span class="w"> </span><span class="nf">max</span><span class="p">(</span><span class="n">all_rewards</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">rewards</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Rewards</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mean_reward</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">=</span><span class="n">std_reward</span><span class="p">,</span><span class="w"> </span><span class="nf">min</span><span class="o">=</span><span class="n">min_reward</span><span class="p">,</span><span class="w"> </span><span class="nf">max</span><span class="o">=</span><span class="n">max_reward</span><span class="p">,</span><span class="w"> </span><span class="ow">all</span><span class="o">=</span><span class="n">all_rewards</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">Steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;Steps&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;mean std min max all&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">all_num_steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">[]</span><span class="w"></span>

<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">play_context</span><span class="p">.</span><span class="n">rewards</span><span class="p">.</span><span class="n">keys</span><span class="p">()</span><span class="err">:</span><span class="w"></span>

<span class="w">            </span><span class="n">all_num_steps</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">play_context</span><span class="p">.</span><span class="n">rewards</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="p">))</span><span class="w"></span>

<span class="w">        </span><span class="n">mean_steps</span><span class="p">,</span><span class="w"> </span><span class="n">std_steps</span><span class="p">,</span><span class="w"> </span><span class="n">min_steps</span><span class="p">,</span><span class="w"> </span><span class="n">max_steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">statistics</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">all_num_steps</span><span class="p">),</span><span class="w"> </span><span class="k">statistics</span><span class="p">.</span><span class="nf">stdev</span><span class="p">(</span><span class="w"></span>

<span class="w">            </span><span class="n">all_num_steps</span><span class="p">),</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">all_num_steps</span><span class="p">),</span><span class="w"> </span><span class="nf">max</span><span class="p">(</span><span class="n">all_num_steps</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Steps</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mean_steps</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">=</span><span class="n">std_steps</span><span class="p">,</span><span class="w"> </span><span class="nf">min</span><span class="o">=</span><span class="n">min_steps</span><span class="p">,</span><span class="w"> </span><span class="nf">max</span><span class="o">=</span><span class="n">max_steps</span><span class="p">,</span><span class="w"> </span><span class="ow">all</span><span class="o">=</span><span class="n">all_num_steps</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">metrics</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Metrics</span><span class="p">(</span><span class="n">rewards</span><span class="o">=</span><span class="n">rewards</span><span class="p">,</span><span class="w"> </span><span class="n">steps</span><span class="o">=</span><span class="n">steps</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">metrics</span><span class="w"></span>
</pre></div>


</details>
<h5 id="play_6">play</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">play</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_episodes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="n">play_context</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">PlayContext</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">default_plots</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span>
</pre></div>


<p>Plays num_episodes with the current policy.</p>
<p>Args:
    callbacks: list of callbacks called during each episode play
    num_episodes: number of episodes to play
    max_steps_per_episode: max steps per episode
    play_context: play configuration to be used. If set override all other play context arguments
    default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...)</p>
<p>Returns:
    play_context containg the actions taken and the rewards received during training</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="n">def</span> <span class="n">play</span><span class="p">(</span><span class="k">self</span><span class="p">,</span>

             <span class="n">callbacks</span><span class="p">:</span> <span class="k">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="k">None</span><span class="p">]</span> <span class="o">=</span> <span class="k">None</span><span class="p">,</span>

             <span class="n">num_episodes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>

             <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>

             <span class="n">play_context</span><span class="p">:</span> <span class="n">core</span><span class="p">.</span><span class="n">PlayContext</span> <span class="o">=</span> <span class="k">None</span><span class="p">,</span>

             <span class="n">default_plots</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="k">None</span><span class="p">):</span>

        <span class="ss">&quot;&quot;&quot;Plays num_episodes with the current policy.</span>

<span class="ss">        Args:</span>

<span class="ss">            callbacks: list of callbacks called during each episode play</span>

<span class="ss">            num_episodes: number of episodes to play</span>

<span class="ss">            max_steps_per_episode: max steps per episode</span>

<span class="ss">            play_context: play configuration to be used. If set override all other play context arguments</span>

<span class="ss">            default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...)</span>

<span class="ss">        Returns:</span>

<span class="ss">            play_context containg the actions taken and the rewards received during training</span>

<span class="ss">        &quot;&quot;&quot;</span>

        <span class="n">assert</span> <span class="k">self</span><span class="p">.</span><span class="n">_backend_agent</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">_is_policy_trained</span><span class="p">,</span> <span class="ss">&quot;No trained policy available. Call train() first.&quot;</span>

        <span class="k">if</span> <span class="n">play_context</span> <span class="k">is</span> <span class="k">None</span><span class="p">:</span>

            <span class="n">play_context</span> <span class="o">=</span> <span class="n">core</span><span class="p">.</span><span class="n">PlayContext</span><span class="p">()</span>

            <span class="n">play_context</span><span class="p">.</span><span class="n">max_steps_per_episode</span> <span class="o">=</span> <span class="n">max_steps_per_episode</span>

            <span class="n">play_context</span><span class="p">.</span><span class="n">num_episodes</span> <span class="o">=</span> <span class="n">num_episodes</span>

        <span class="n">callbacks</span> <span class="o">=</span> <span class="k">self</span><span class="p">.</span><span class="n">_to_callback_list</span><span class="p">(</span><span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span>

        <span class="n">callbacks</span> <span class="o">=</span> <span class="k">self</span><span class="p">.</span><span class="n">_add_plot_callbacks</span><span class="p">(</span><span class="n">callbacks</span><span class="p">,</span> <span class="n">default_plots</span><span class="p">,</span> <span class="p">[</span><span class="n">plot</span><span class="p">.</span><span class="n">Steps</span><span class="p">(),</span> <span class="n">plot</span><span class="p">.</span><span class="n">Rewards</span><span class="p">()])</span>

        <span class="k">self</span><span class="p">.</span><span class="n">_backend_agent</span><span class="p">.</span><span class="n">play</span><span class="p">(</span><span class="n">play_context</span><span class="o">=</span><span class="n">play_context</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">play_context</span>
</pre></div>


</details>
<h5 id="save_6">save</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">save</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span>
</pre></div>


<p>Saves the currently trained actor policy in directory.</p>
<p>If save is called before a trained policy is created, eg by calling train, an exception is raised.</p>
<p>Args:
     directory: the directory to save the policy weights to.
        if the directory does not exist yet, a new directory is created. if None the policy is saved
        in a temp directory.
     callbacks: list of callbacks called during save (eg log.Agent)</p>
<p>Returns:
    the absolute path to the directory containing the saved policy.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="k">save</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="nl">directory</span><span class="p">:</span><span class="w"> </span><span class="n">Optional</span><span class="o">[</span><span class="n">str</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">None</span><span class="p">,</span><span class="w"></span>

<span class="w">             </span><span class="nl">callbacks</span><span class="p">:</span><span class="w"> </span><span class="ow">Union</span><span class="o">[</span><span class="n">List[core.AgentCallback</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="err">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">None</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="nf">str</span><span class="err">:</span><span class="w"></span>

<span class="w">        </span><span class="ss">&quot;&quot;&quot;Saves the currently trained actor policy in directory.</span>

<span class="ss">        If save is called before a trained policy is created, eg by calling train, an exception is raised.</span>

<span class="ss">        Args:</span>

<span class="ss">             directory: the directory to save the policy weights to.</span>

<span class="ss">                if the directory does not exist yet, a new directory is created. if None the policy is saved</span>

<span class="ss">                in a temp directory.</span>

<span class="ss">             callbacks: list of callbacks called during save (eg log.Agent)</span>

<span class="ss">        Returns:</span>

<span class="ss">            the absolute path to the directory containing the saved policy.</span>

<span class="ss">        &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">directory</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">None</span><span class="err">:</span><span class="w"></span>

<span class="w">            </span><span class="n">directory</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bcore</span><span class="p">.</span><span class="n">_get_temp_path</span><span class="p">()</span><span class="w"></span>

<span class="w">        </span><span class="n">assert</span><span class="w"> </span><span class="n">directory</span><span class="w"></span>

<span class="w">        </span><span class="n">assert</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_backend_agent</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">_is_policy_trained</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;No trained policy available. Call train() first.&quot;</span><span class="w"></span>

<span class="w">        </span><span class="n">directory</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bcore</span><span class="p">.</span><span class="n">_mkdir</span><span class="p">(</span><span class="n">directory</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">agent_json_path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">os</span><span class="p">.</span><span class="k">path</span><span class="p">.</span><span class="k">join</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span><span class="w"> </span><span class="n">EasyAgent</span><span class="p">.</span><span class="n">_KEY_EASYAGENT_FILENAME</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">with</span><span class="w"> </span><span class="k">open</span><span class="p">(</span><span class="n">agent_json_path</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;w&#39;</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nl">jsonfile</span><span class="p">:</span><span class="w"></span>

<span class="w">            </span><span class="n">agent_dict</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_to_dict</span><span class="p">()</span><span class="w"></span>

<span class="w">            </span><span class="n">json</span><span class="p">.</span><span class="k">dump</span><span class="p">(</span><span class="n">agent_dict</span><span class="p">,</span><span class="w"> </span><span class="n">jsonfile</span><span class="p">,</span><span class="w"> </span><span class="n">sort_keys</span><span class="o">=</span><span class="k">True</span><span class="p">,</span><span class="w"> </span><span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">callbacks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_to_callback_list</span><span class="p">(</span><span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">policy_directory</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">os</span><span class="p">.</span><span class="k">path</span><span class="p">.</span><span class="k">join</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span><span class="w"> </span><span class="n">EasyAgent</span><span class="p">.</span><span class="n">_KEY_POLICY_DIRECTORY</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">policy_directory</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bcore</span><span class="p">.</span><span class="n">_mkdir</span><span class="p">(</span><span class="n">policy_directory</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">_backend_agent</span><span class="p">.</span><span class="k">save</span><span class="p">(</span><span class="n">directory</span><span class="o">=</span><span class="n">policy_directory</span><span class="p">,</span><span class="w"> </span><span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">directory</span><span class="w"></span>
</pre></div>


</details>
<h5 id="train_6">train</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="n">num_episodes_per_eval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">train_context</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">TrainContext</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">default_plots</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span>
</pre></div>


<p>Evaluates the environment using a uniform random policy.</p>
<p>The evaluation is performed in batches of num_episodes_per_eval episodes.</p>
<p>Args:
    callbacks: list of callbacks called during training and evaluation
    num_iterations: number of times a batch of num_episodes_per_eval episodes is evaluated.
    max_steps_per_episode: maximum number of steps per episode
    num_episodes_per_eval: number of episodes played to estimate the average return and steps
    train_context: training configuration to be used. if set overrides all other training context arguments.
    default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...)</p>
<p>Returns:
    train_context: the training configuration containing the loss and sum of rewards encountered
        during training</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="n">def</span> <span class="n">train</span><span class="p">(</span><span class="k">self</span><span class="p">,</span>

              <span class="n">callbacks</span><span class="p">:</span> <span class="k">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="k">None</span><span class="p">]</span> <span class="o">=</span> <span class="k">None</span><span class="p">,</span>

              <span class="n">num_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>

              <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>

              <span class="n">num_episodes_per_eval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>

              <span class="n">train_context</span><span class="p">:</span> <span class="n">core</span><span class="p">.</span><span class="n">TrainContext</span> <span class="o">=</span> <span class="k">None</span><span class="p">,</span>

              <span class="n">default_plots</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="k">None</span><span class="p">):</span>

        <span class="ss">&quot;&quot;&quot;Evaluates the environment using a uniform random policy.</span>

<span class="ss">        The evaluation is performed in batches of num_episodes_per_eval episodes.</span>

<span class="ss">        Args:</span>

<span class="ss">            callbacks: list of callbacks called during training and evaluation</span>

<span class="ss">            num_iterations: number of times a batch of num_episodes_per_eval episodes is evaluated.</span>

<span class="ss">            max_steps_per_episode: maximum number of steps per episode</span>

<span class="ss">            num_episodes_per_eval: number of episodes played to estimate the average return and steps</span>

<span class="ss">            train_context: training configuration to be used. if set overrides all other training context arguments.</span>

<span class="ss">            default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...)</span>

<span class="ss">        Returns:</span>

<span class="ss">            train_context: the training configuration containing the loss and sum of rewards encountered</span>

<span class="ss">                during training</span>

<span class="ss">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">train_context</span> <span class="k">is</span> <span class="k">None</span><span class="p">:</span>

            <span class="n">train_context</span> <span class="o">=</span> <span class="n">core</span><span class="p">.</span><span class="n">TrainContext</span><span class="p">()</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">num_iterations</span> <span class="o">=</span> <span class="n">num_iterations</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">max_steps_per_episode</span> <span class="o">=</span> <span class="n">max_steps_per_episode</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">num_epochs_per_iteration</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">num_iterations_between_eval</span> <span class="o">=</span> <span class="mi">1</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">num_episodes_per_eval</span> <span class="o">=</span> <span class="n">num_episodes_per_eval</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="n">super</span><span class="p">().</span><span class="n">train</span><span class="p">(</span><span class="n">train_context</span><span class="o">=</span><span class="n">train_context</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span> <span class="n">default_plots</span><span class="o">=</span><span class="n">default_plots</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">train_context</span>
</pre></div>


</details>
<h3 id="reinforceagent">ReinforceAgent</h3>
<div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">ReinforceAgent</span><span class="p">(</span>
    <span class="n">gym_env_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">fc_layers</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="nb">int</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">backend</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span>
</pre></div>


<p>creates a new agent based on the Reinforce algorithm.</p>
<p>Reinforce is a vanilla policy gradient algorithm using a single actor network.</p>
<p>see also: www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><span class="k">class</span> <span class="n">ReinforceAgent</span>(<span class="n">EasyAgent</span>):

    <span class="s">&quot;&quot;&quot;creates a new agent based on the Reinforce algorithm.</span>

<span class="s">        Reinforce is a vanilla policy gradient algorithm using a single actor network.</span>

<span class="s">        see also: www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf</span>

<span class="s">    &quot;&quot;&quot;</span>

    <span class="n">def</span> <span class="n">train</span>(<span class="k">self</span>,

              <span class="n">callbacks:</span> <span class="n">Union</span>[<span class="nb">List</span>[<span class="n">core</span>.<span class="n">AgentCallback</span>], <span class="n">core</span>.<span class="n">AgentCallback</span>, <span class="n">None</span>] = <span class="n">None</span>,

              <span class="n">num_iterations:</span> <span class="nb">int</span> = <span class="mi">100</span>,

              <span class="n">num_episodes_per_iteration:</span> <span class="nb">int</span> = <span class="mi">10</span>,

              <span class="n">max_steps_per_episode:</span> <span class="nb">int</span> = <span class="mi">500</span>,

              <span class="n">num_epochs_per_iteration:</span> <span class="nb">int</span> = <span class="mi">10</span>,

              <span class="n">num_iterations_between_eval:</span> <span class="nb">int</span> = <span class="mi">5</span>,

              <span class="n">num_episodes_per_eval:</span> <span class="nb">int</span> = <span class="mi">10</span>,

              <span class="n">learning_rate:</span> <span class="n">float</span> = <span class="mf">0.001</span>,

              <span class="n">train_context:</span> <span class="n">core</span>.<span class="n">EpisodesTrainContext</span> = <span class="n">None</span>,

              <span class="n">default_plots:</span> <span class="nb">bool</span> = <span class="n">None</span>):

        <span class="s">&quot;&quot;&quot;Trains a new model using the gym environment passed during instantiation.</span>

<span class="s">        Args:</span>

<span class="s">            callbacks: list of callbacks called during training and evaluation</span>

<span class="s">            num_iterations: number of times the training is repeated (with additional data)</span>

<span class="s">            num_episodes_per_iteration: number of episodes played per training iteration</span>

<span class="s">            max_steps_per_episode: maximum number of steps per episode</span>

<span class="s">            num_epochs_per_iteration: number of times the data collected for the current iteration</span>

<span class="s">                is used to retrain the current policy</span>

<span class="s">            num_iterations_between_eval: number of training iterations before the current policy is evaluated.</span>

<span class="s">                if 0 no evaluation is performed.</span>

<span class="s">            num_episodes_per_eval: number of episodes played to estimate the average return and steps</span>

<span class="s">            learning_rate: the learning rate used in the next iteration&#39;s policy training (0,1]</span>

<span class="s">            train_context: training configuration to be used. if set overrides all other training context arguments.</span>

<span class="s">            default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...).</span>

<span class="s">                if None default callbacks are only added if the callbacks list is empty</span>

<span class="s">        Returns:</span>

<span class="s">            train_context: the training configuration containing the loss and sum of rewards encountered</span>

<span class="s">                during training</span>

<span class="s">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">train_context</span> <span class="k">is</span> <span class="n">None:</span>

            <span class="n">train_context</span> = <span class="n">core</span>.<span class="n">EpisodesTrainContext</span>()

            <span class="n">train_context</span>.<span class="n">num_iterations</span> = <span class="n">num_iterations</span>

            <span class="n">train_context</span>.<span class="n">num_episodes_per_iteration</span> = <span class="n">num_episodes_per_iteration</span>

            <span class="n">train_context</span>.<span class="n">max_steps_per_episode</span> = <span class="n">max_steps_per_episode</span>

            <span class="n">train_context</span>.<span class="n">num_epochs_per_iteration</span> = <span class="n">num_epochs_per_iteration</span>

            <span class="n">train_context</span>.<span class="n">num_iterations_between_eval</span> = <span class="n">num_iterations_between_eval</span>

            <span class="n">train_context</span>.<span class="n">num_episodes_per_eval</span> = <span class="n">num_episodes_per_eval</span>

            <span class="n">train_context</span>.<span class="n">learning_rate</span> = <span class="n">learning_rate</span>

        <span class="n">super</span>().<span class="n">train</span>(<span class="n">train_context</span>=<span class="n">train_context</span>, <span class="n">callbacks</span>=<span class="n">callbacks</span>, <span class="n">default_plots</span>=<span class="n">default_plots</span>)

        <span class="k">return</span> <span class="n">train_context</span>
</pre></div>


</details>
<hr />
<h4 id="ancestors-in-mro_7">Ancestors (in MRO)</h4>
<ul>
<li>easyagents.agents.EasyAgent</li>
<li>abc.ABC</li>
</ul>
<h4 id="methods_7">Methods</h4>
<h5 id="evaluate_7">evaluate</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">num_episodes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span>
<span class="p">)</span>
</pre></div>


<p>Plays num_episodes with the current policy and computes metrics on rewards.</p>
<p>Args:
    num_episodes: number of episodes to play
    max_steps_per_episode: max steps per episode</p>
<p>Returns:
    extensible score metrics</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">evaluate</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="nl">num_episodes</span><span class="p">:</span><span class="w"> </span><span class="nc">int</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">50</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="nl">max_steps_per_episode</span><span class="p">:</span><span class="w"> </span><span class="nc">int</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">50</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">        </span><span class="ss">&quot;&quot;&quot;Plays num_episodes with the current policy and computes metrics on rewards.</span>

<span class="ss">        Args:</span>

<span class="ss">            num_episodes: number of episodes to play</span>

<span class="ss">            max_steps_per_episode: max steps per episode</span>

<span class="ss">        Returns:</span>

<span class="ss">            extensible score metrics</span>

<span class="ss">        &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">        </span><span class="n">play_context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">core</span><span class="p">.</span><span class="n">PlayContext</span><span class="p">()</span><span class="w"></span>

<span class="w">        </span><span class="n">play_context</span><span class="p">.</span><span class="n">max_steps_per_episode</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">max_steps_per_episode</span><span class="w"></span>

<span class="w">        </span><span class="n">play_context</span><span class="p">.</span><span class="n">num_episodes</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">num_episodes</span><span class="w"></span>

<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">play</span><span class="p">(</span><span class="n">play_context</span><span class="o">=</span><span class="n">play_context</span><span class="p">,</span><span class="w"> </span><span class="n">default_plots</span><span class="o">=</span><span class="k">False</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">Metrics</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;Metrics&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;steps rewards&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">Rewards</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;Rewards&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;mean std min max all&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">all_rewards</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">list</span><span class="p">(</span><span class="n">play_context</span><span class="p">.</span><span class="n">sum_of_rewards</span><span class="p">.</span><span class="k">values</span><span class="p">())</span><span class="w"></span>

<span class="w">        </span><span class="n">mean_reward</span><span class="p">,</span><span class="w"> </span><span class="n">std_reward</span><span class="p">,</span><span class="w"> </span><span class="n">min_reward</span><span class="p">,</span><span class="w"> </span><span class="n">max_reward</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">statistics</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">all_rewards</span><span class="p">),</span><span class="w"> </span><span class="k">statistics</span><span class="p">.</span><span class="nf">stdev</span><span class="p">(</span><span class="w"></span>

<span class="w">            </span><span class="n">all_rewards</span><span class="p">),</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">all_rewards</span><span class="p">),</span><span class="w"> </span><span class="nf">max</span><span class="p">(</span><span class="n">all_rewards</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">rewards</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Rewards</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mean_reward</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">=</span><span class="n">std_reward</span><span class="p">,</span><span class="w"> </span><span class="nf">min</span><span class="o">=</span><span class="n">min_reward</span><span class="p">,</span><span class="w"> </span><span class="nf">max</span><span class="o">=</span><span class="n">max_reward</span><span class="p">,</span><span class="w"> </span><span class="ow">all</span><span class="o">=</span><span class="n">all_rewards</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">Steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;Steps&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;mean std min max all&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">all_num_steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">[]</span><span class="w"></span>

<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">play_context</span><span class="p">.</span><span class="n">rewards</span><span class="p">.</span><span class="n">keys</span><span class="p">()</span><span class="err">:</span><span class="w"></span>

<span class="w">            </span><span class="n">all_num_steps</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">play_context</span><span class="p">.</span><span class="n">rewards</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="p">))</span><span class="w"></span>

<span class="w">        </span><span class="n">mean_steps</span><span class="p">,</span><span class="w"> </span><span class="n">std_steps</span><span class="p">,</span><span class="w"> </span><span class="n">min_steps</span><span class="p">,</span><span class="w"> </span><span class="n">max_steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">statistics</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">all_num_steps</span><span class="p">),</span><span class="w"> </span><span class="k">statistics</span><span class="p">.</span><span class="nf">stdev</span><span class="p">(</span><span class="w"></span>

<span class="w">            </span><span class="n">all_num_steps</span><span class="p">),</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">all_num_steps</span><span class="p">),</span><span class="w"> </span><span class="nf">max</span><span class="p">(</span><span class="n">all_num_steps</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Steps</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mean_steps</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">=</span><span class="n">std_steps</span><span class="p">,</span><span class="w"> </span><span class="nf">min</span><span class="o">=</span><span class="n">min_steps</span><span class="p">,</span><span class="w"> </span><span class="nf">max</span><span class="o">=</span><span class="n">max_steps</span><span class="p">,</span><span class="w"> </span><span class="ow">all</span><span class="o">=</span><span class="n">all_num_steps</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">metrics</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Metrics</span><span class="p">(</span><span class="n">rewards</span><span class="o">=</span><span class="n">rewards</span><span class="p">,</span><span class="w"> </span><span class="n">steps</span><span class="o">=</span><span class="n">steps</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">metrics</span><span class="w"></span>
</pre></div>


</details>
<h5 id="play_7">play</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">play</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_episodes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="n">play_context</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">PlayContext</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">default_plots</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span>
</pre></div>


<p>Plays num_episodes with the current policy.</p>
<p>Args:
    callbacks: list of callbacks called during each episode play
    num_episodes: number of episodes to play
    max_steps_per_episode: max steps per episode
    play_context: play configuration to be used. If set override all other play context arguments
    default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...)</p>
<p>Returns:
    play_context containg the actions taken and the rewards received during training</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="n">def</span> <span class="n">play</span><span class="p">(</span><span class="k">self</span><span class="p">,</span>

             <span class="n">callbacks</span><span class="p">:</span> <span class="k">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="k">None</span><span class="p">]</span> <span class="o">=</span> <span class="k">None</span><span class="p">,</span>

             <span class="n">num_episodes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>

             <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>

             <span class="n">play_context</span><span class="p">:</span> <span class="n">core</span><span class="p">.</span><span class="n">PlayContext</span> <span class="o">=</span> <span class="k">None</span><span class="p">,</span>

             <span class="n">default_plots</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="k">None</span><span class="p">):</span>

        <span class="ss">&quot;&quot;&quot;Plays num_episodes with the current policy.</span>

<span class="ss">        Args:</span>

<span class="ss">            callbacks: list of callbacks called during each episode play</span>

<span class="ss">            num_episodes: number of episodes to play</span>

<span class="ss">            max_steps_per_episode: max steps per episode</span>

<span class="ss">            play_context: play configuration to be used. If set override all other play context arguments</span>

<span class="ss">            default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...)</span>

<span class="ss">        Returns:</span>

<span class="ss">            play_context containg the actions taken and the rewards received during training</span>

<span class="ss">        &quot;&quot;&quot;</span>

        <span class="n">assert</span> <span class="k">self</span><span class="p">.</span><span class="n">_backend_agent</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">_is_policy_trained</span><span class="p">,</span> <span class="ss">&quot;No trained policy available. Call train() first.&quot;</span>

        <span class="k">if</span> <span class="n">play_context</span> <span class="k">is</span> <span class="k">None</span><span class="p">:</span>

            <span class="n">play_context</span> <span class="o">=</span> <span class="n">core</span><span class="p">.</span><span class="n">PlayContext</span><span class="p">()</span>

            <span class="n">play_context</span><span class="p">.</span><span class="n">max_steps_per_episode</span> <span class="o">=</span> <span class="n">max_steps_per_episode</span>

            <span class="n">play_context</span><span class="p">.</span><span class="n">num_episodes</span> <span class="o">=</span> <span class="n">num_episodes</span>

        <span class="n">callbacks</span> <span class="o">=</span> <span class="k">self</span><span class="p">.</span><span class="n">_to_callback_list</span><span class="p">(</span><span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span>

        <span class="n">callbacks</span> <span class="o">=</span> <span class="k">self</span><span class="p">.</span><span class="n">_add_plot_callbacks</span><span class="p">(</span><span class="n">callbacks</span><span class="p">,</span> <span class="n">default_plots</span><span class="p">,</span> <span class="p">[</span><span class="n">plot</span><span class="p">.</span><span class="n">Steps</span><span class="p">(),</span> <span class="n">plot</span><span class="p">.</span><span class="n">Rewards</span><span class="p">()])</span>

        <span class="k">self</span><span class="p">.</span><span class="n">_backend_agent</span><span class="p">.</span><span class="n">play</span><span class="p">(</span><span class="n">play_context</span><span class="o">=</span><span class="n">play_context</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">play_context</span>
</pre></div>


</details>
<h5 id="save_7">save</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">save</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span>
</pre></div>


<p>Saves the currently trained actor policy in directory.</p>
<p>If save is called before a trained policy is created, eg by calling train, an exception is raised.</p>
<p>Args:
     directory: the directory to save the policy weights to.
        if the directory does not exist yet, a new directory is created. if None the policy is saved
        in a temp directory.
     callbacks: list of callbacks called during save (eg log.Agent)</p>
<p>Returns:
    the absolute path to the directory containing the saved policy.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="k">save</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="nl">directory</span><span class="p">:</span><span class="w"> </span><span class="n">Optional</span><span class="o">[</span><span class="n">str</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">None</span><span class="p">,</span><span class="w"></span>

<span class="w">             </span><span class="nl">callbacks</span><span class="p">:</span><span class="w"> </span><span class="ow">Union</span><span class="o">[</span><span class="n">List[core.AgentCallback</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="err">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">None</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="nf">str</span><span class="err">:</span><span class="w"></span>

<span class="w">        </span><span class="ss">&quot;&quot;&quot;Saves the currently trained actor policy in directory.</span>

<span class="ss">        If save is called before a trained policy is created, eg by calling train, an exception is raised.</span>

<span class="ss">        Args:</span>

<span class="ss">             directory: the directory to save the policy weights to.</span>

<span class="ss">                if the directory does not exist yet, a new directory is created. if None the policy is saved</span>

<span class="ss">                in a temp directory.</span>

<span class="ss">             callbacks: list of callbacks called during save (eg log.Agent)</span>

<span class="ss">        Returns:</span>

<span class="ss">            the absolute path to the directory containing the saved policy.</span>

<span class="ss">        &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">directory</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">None</span><span class="err">:</span><span class="w"></span>

<span class="w">            </span><span class="n">directory</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bcore</span><span class="p">.</span><span class="n">_get_temp_path</span><span class="p">()</span><span class="w"></span>

<span class="w">        </span><span class="n">assert</span><span class="w"> </span><span class="n">directory</span><span class="w"></span>

<span class="w">        </span><span class="n">assert</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_backend_agent</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">_is_policy_trained</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;No trained policy available. Call train() first.&quot;</span><span class="w"></span>

<span class="w">        </span><span class="n">directory</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bcore</span><span class="p">.</span><span class="n">_mkdir</span><span class="p">(</span><span class="n">directory</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">agent_json_path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">os</span><span class="p">.</span><span class="k">path</span><span class="p">.</span><span class="k">join</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span><span class="w"> </span><span class="n">EasyAgent</span><span class="p">.</span><span class="n">_KEY_EASYAGENT_FILENAME</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">with</span><span class="w"> </span><span class="k">open</span><span class="p">(</span><span class="n">agent_json_path</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;w&#39;</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nl">jsonfile</span><span class="p">:</span><span class="w"></span>

<span class="w">            </span><span class="n">agent_dict</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_to_dict</span><span class="p">()</span><span class="w"></span>

<span class="w">            </span><span class="n">json</span><span class="p">.</span><span class="k">dump</span><span class="p">(</span><span class="n">agent_dict</span><span class="p">,</span><span class="w"> </span><span class="n">jsonfile</span><span class="p">,</span><span class="w"> </span><span class="n">sort_keys</span><span class="o">=</span><span class="k">True</span><span class="p">,</span><span class="w"> </span><span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">callbacks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_to_callback_list</span><span class="p">(</span><span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">policy_directory</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">os</span><span class="p">.</span><span class="k">path</span><span class="p">.</span><span class="k">join</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span><span class="w"> </span><span class="n">EasyAgent</span><span class="p">.</span><span class="n">_KEY_POLICY_DIRECTORY</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">policy_directory</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bcore</span><span class="p">.</span><span class="n">_mkdir</span><span class="p">(</span><span class="n">policy_directory</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">_backend_agent</span><span class="p">.</span><span class="k">save</span><span class="p">(</span><span class="n">directory</span><span class="o">=</span><span class="n">policy_directory</span><span class="p">,</span><span class="w"> </span><span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">directory</span><span class="w"></span>
</pre></div>


</details>
<h5 id="train_7">train</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">num_episodes_per_iteration</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span>
    <span class="n">num_epochs_per_iteration</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">num_iterations_between_eval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
    <span class="n">num_episodes_per_eval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">,</span>
    <span class="n">train_context</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">EpisodesTrainContext</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">default_plots</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span>
</pre></div>


<p>Trains a new model using the gym environment passed during instantiation.</p>
<p>Args:
    callbacks: list of callbacks called during training and evaluation
    num_iterations: number of times the training is repeated (with additional data)
    num_episodes_per_iteration: number of episodes played per training iteration
    max_steps_per_episode: maximum number of steps per episode
    num_epochs_per_iteration: number of times the data collected for the current iteration
        is used to retrain the current policy
    num_iterations_between_eval: number of training iterations before the current policy is evaluated.
        if 0 no evaluation is performed.
    num_episodes_per_eval: number of episodes played to estimate the average return and steps
    learning_rate: the learning rate used in the next iteration's policy training (0,1]
    train_context: training configuration to be used. if set overrides all other training context arguments.
    default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...).
        if None default callbacks are only added if the callbacks list is empty</p>
<p>Returns:
    train_context: the training configuration containing the loss and sum of rewards encountered
        during training</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="n">def</span> <span class="n">train</span><span class="p">(</span><span class="k">self</span><span class="p">,</span>

              <span class="n">callbacks</span><span class="p">:</span> <span class="k">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="k">None</span><span class="p">]</span> <span class="o">=</span> <span class="k">None</span><span class="p">,</span>

              <span class="n">num_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>

              <span class="n">num_episodes_per_iteration</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>

              <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span>

              <span class="n">num_epochs_per_iteration</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>

              <span class="n">num_iterations_between_eval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>

              <span class="n">num_episodes_per_eval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>

              <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">0</span><span class="p">.</span><span class="mi">001</span><span class="p">,</span>

              <span class="n">train_context</span><span class="p">:</span> <span class="n">core</span><span class="p">.</span><span class="n">EpisodesTrainContext</span> <span class="o">=</span> <span class="k">None</span><span class="p">,</span>

              <span class="n">default_plots</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="k">None</span><span class="p">):</span>

        <span class="ss">&quot;&quot;&quot;Trains a new model using the gym environment passed during instantiation.</span>

<span class="ss">        Args:</span>

<span class="ss">            callbacks: list of callbacks called during training and evaluation</span>

<span class="ss">            num_iterations: number of times the training is repeated (with additional data)</span>

<span class="ss">            num_episodes_per_iteration: number of episodes played per training iteration</span>

<span class="ss">            max_steps_per_episode: maximum number of steps per episode</span>

<span class="ss">            num_epochs_per_iteration: number of times the data collected for the current iteration</span>

<span class="ss">                is used to retrain the current policy</span>

<span class="ss">            num_iterations_between_eval: number of training iterations before the current policy is evaluated.</span>

<span class="ss">                if 0 no evaluation is performed.</span>

<span class="ss">            num_episodes_per_eval: number of episodes played to estimate the average return and steps</span>

<span class="ss">            learning_rate: the learning rate used in the next iteration&#39;s policy training (0,1]</span>

<span class="ss">            train_context: training configuration to be used. if set overrides all other training context arguments.</span>

<span class="ss">            default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...).</span>

<span class="ss">                if None default callbacks are only added if the callbacks list is empty</span>

<span class="ss">        Returns:</span>

<span class="ss">            train_context: the training configuration containing the loss and sum of rewards encountered</span>

<span class="ss">                during training</span>

<span class="ss">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">train_context</span> <span class="k">is</span> <span class="k">None</span><span class="p">:</span>

            <span class="n">train_context</span> <span class="o">=</span> <span class="n">core</span><span class="p">.</span><span class="n">EpisodesTrainContext</span><span class="p">()</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">num_iterations</span> <span class="o">=</span> <span class="n">num_iterations</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">num_episodes_per_iteration</span> <span class="o">=</span> <span class="n">num_episodes_per_iteration</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">max_steps_per_episode</span> <span class="o">=</span> <span class="n">max_steps_per_episode</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">num_epochs_per_iteration</span> <span class="o">=</span> <span class="n">num_epochs_per_iteration</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">num_iterations_between_eval</span> <span class="o">=</span> <span class="n">num_iterations_between_eval</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">num_episodes_per_eval</span> <span class="o">=</span> <span class="n">num_episodes_per_eval</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>

        <span class="n">super</span><span class="p">().</span><span class="n">train</span><span class="p">(</span><span class="n">train_context</span><span class="o">=</span><span class="n">train_context</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span> <span class="n">default_plots</span><span class="o">=</span><span class="n">default_plots</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">train_context</span>
</pre></div>


</details>
<h3 id="sacagent">SacAgent</h3>
<div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">SacAgent</span><span class="p">(</span>
    <span class="n">gym_env_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">fc_layers</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="nb">int</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">backend</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span>
</pre></div>


<p>Agent based on the Soft-Actor-Critic algorithm (https://arxiv.org/abs/1812.05905).</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><span class="k">class</span> <span class="n">SacAgent</span>(<span class="n">DqnAgent</span>):

    <span class="s">&quot;&quot;&quot;Agent based on the Soft-Actor-Critic algorithm (https://arxiv.org/abs/1812.05905).&quot;&quot;&quot;</span>
</pre></div>


</details>
<hr />
<h4 id="ancestors-in-mro_8">Ancestors (in MRO)</h4>
<ul>
<li>easyagents.agents.DqnAgent</li>
<li>easyagents.agents.EasyAgent</li>
<li>abc.ABC</li>
</ul>
<h4 id="methods_8">Methods</h4>
<h5 id="evaluate_8">evaluate</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">num_episodes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span>
<span class="p">)</span>
</pre></div>


<p>Plays num_episodes with the current policy and computes metrics on rewards.</p>
<p>Args:
    num_episodes: number of episodes to play
    max_steps_per_episode: max steps per episode</p>
<p>Returns:
    extensible score metrics</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">evaluate</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="nl">num_episodes</span><span class="p">:</span><span class="w"> </span><span class="nc">int</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">50</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="nl">max_steps_per_episode</span><span class="p">:</span><span class="w"> </span><span class="nc">int</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">50</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">        </span><span class="ss">&quot;&quot;&quot;Plays num_episodes with the current policy and computes metrics on rewards.</span>

<span class="ss">        Args:</span>

<span class="ss">            num_episodes: number of episodes to play</span>

<span class="ss">            max_steps_per_episode: max steps per episode</span>

<span class="ss">        Returns:</span>

<span class="ss">            extensible score metrics</span>

<span class="ss">        &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">        </span><span class="n">play_context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">core</span><span class="p">.</span><span class="n">PlayContext</span><span class="p">()</span><span class="w"></span>

<span class="w">        </span><span class="n">play_context</span><span class="p">.</span><span class="n">max_steps_per_episode</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">max_steps_per_episode</span><span class="w"></span>

<span class="w">        </span><span class="n">play_context</span><span class="p">.</span><span class="n">num_episodes</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">num_episodes</span><span class="w"></span>

<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">play</span><span class="p">(</span><span class="n">play_context</span><span class="o">=</span><span class="n">play_context</span><span class="p">,</span><span class="w"> </span><span class="n">default_plots</span><span class="o">=</span><span class="k">False</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">Metrics</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;Metrics&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;steps rewards&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">Rewards</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;Rewards&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;mean std min max all&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">all_rewards</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">list</span><span class="p">(</span><span class="n">play_context</span><span class="p">.</span><span class="n">sum_of_rewards</span><span class="p">.</span><span class="k">values</span><span class="p">())</span><span class="w"></span>

<span class="w">        </span><span class="n">mean_reward</span><span class="p">,</span><span class="w"> </span><span class="n">std_reward</span><span class="p">,</span><span class="w"> </span><span class="n">min_reward</span><span class="p">,</span><span class="w"> </span><span class="n">max_reward</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">statistics</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">all_rewards</span><span class="p">),</span><span class="w"> </span><span class="k">statistics</span><span class="p">.</span><span class="nf">stdev</span><span class="p">(</span><span class="w"></span>

<span class="w">            </span><span class="n">all_rewards</span><span class="p">),</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">all_rewards</span><span class="p">),</span><span class="w"> </span><span class="nf">max</span><span class="p">(</span><span class="n">all_rewards</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">rewards</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Rewards</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mean_reward</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">=</span><span class="n">std_reward</span><span class="p">,</span><span class="w"> </span><span class="nf">min</span><span class="o">=</span><span class="n">min_reward</span><span class="p">,</span><span class="w"> </span><span class="nf">max</span><span class="o">=</span><span class="n">max_reward</span><span class="p">,</span><span class="w"> </span><span class="ow">all</span><span class="o">=</span><span class="n">all_rewards</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">Steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;Steps&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;mean std min max all&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">all_num_steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">[]</span><span class="w"></span>

<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">play_context</span><span class="p">.</span><span class="n">rewards</span><span class="p">.</span><span class="n">keys</span><span class="p">()</span><span class="err">:</span><span class="w"></span>

<span class="w">            </span><span class="n">all_num_steps</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">play_context</span><span class="p">.</span><span class="n">rewards</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="p">))</span><span class="w"></span>

<span class="w">        </span><span class="n">mean_steps</span><span class="p">,</span><span class="w"> </span><span class="n">std_steps</span><span class="p">,</span><span class="w"> </span><span class="n">min_steps</span><span class="p">,</span><span class="w"> </span><span class="n">max_steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">statistics</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">all_num_steps</span><span class="p">),</span><span class="w"> </span><span class="k">statistics</span><span class="p">.</span><span class="nf">stdev</span><span class="p">(</span><span class="w"></span>

<span class="w">            </span><span class="n">all_num_steps</span><span class="p">),</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">all_num_steps</span><span class="p">),</span><span class="w"> </span><span class="nf">max</span><span class="p">(</span><span class="n">all_num_steps</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Steps</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mean_steps</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">=</span><span class="n">std_steps</span><span class="p">,</span><span class="w"> </span><span class="nf">min</span><span class="o">=</span><span class="n">min_steps</span><span class="p">,</span><span class="w"> </span><span class="nf">max</span><span class="o">=</span><span class="n">max_steps</span><span class="p">,</span><span class="w"> </span><span class="ow">all</span><span class="o">=</span><span class="n">all_num_steps</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">metrics</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Metrics</span><span class="p">(</span><span class="n">rewards</span><span class="o">=</span><span class="n">rewards</span><span class="p">,</span><span class="w"> </span><span class="n">steps</span><span class="o">=</span><span class="n">steps</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">metrics</span><span class="w"></span>
</pre></div>


</details>
<h5 id="play_8">play</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">play</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_episodes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="n">play_context</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">PlayContext</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">default_plots</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span>
</pre></div>


<p>Plays num_episodes with the current policy.</p>
<p>Args:
    callbacks: list of callbacks called during each episode play
    num_episodes: number of episodes to play
    max_steps_per_episode: max steps per episode
    play_context: play configuration to be used. If set override all other play context arguments
    default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...)</p>
<p>Returns:
    play_context containg the actions taken and the rewards received during training</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="n">def</span> <span class="n">play</span><span class="p">(</span><span class="k">self</span><span class="p">,</span>

             <span class="n">callbacks</span><span class="p">:</span> <span class="k">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="k">None</span><span class="p">]</span> <span class="o">=</span> <span class="k">None</span><span class="p">,</span>

             <span class="n">num_episodes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>

             <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>

             <span class="n">play_context</span><span class="p">:</span> <span class="n">core</span><span class="p">.</span><span class="n">PlayContext</span> <span class="o">=</span> <span class="k">None</span><span class="p">,</span>

             <span class="n">default_plots</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="k">None</span><span class="p">):</span>

        <span class="ss">&quot;&quot;&quot;Plays num_episodes with the current policy.</span>

<span class="ss">        Args:</span>

<span class="ss">            callbacks: list of callbacks called during each episode play</span>

<span class="ss">            num_episodes: number of episodes to play</span>

<span class="ss">            max_steps_per_episode: max steps per episode</span>

<span class="ss">            play_context: play configuration to be used. If set override all other play context arguments</span>

<span class="ss">            default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...)</span>

<span class="ss">        Returns:</span>

<span class="ss">            play_context containg the actions taken and the rewards received during training</span>

<span class="ss">        &quot;&quot;&quot;</span>

        <span class="n">assert</span> <span class="k">self</span><span class="p">.</span><span class="n">_backend_agent</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">_is_policy_trained</span><span class="p">,</span> <span class="ss">&quot;No trained policy available. Call train() first.&quot;</span>

        <span class="k">if</span> <span class="n">play_context</span> <span class="k">is</span> <span class="k">None</span><span class="p">:</span>

            <span class="n">play_context</span> <span class="o">=</span> <span class="n">core</span><span class="p">.</span><span class="n">PlayContext</span><span class="p">()</span>

            <span class="n">play_context</span><span class="p">.</span><span class="n">max_steps_per_episode</span> <span class="o">=</span> <span class="n">max_steps_per_episode</span>

            <span class="n">play_context</span><span class="p">.</span><span class="n">num_episodes</span> <span class="o">=</span> <span class="n">num_episodes</span>

        <span class="n">callbacks</span> <span class="o">=</span> <span class="k">self</span><span class="p">.</span><span class="n">_to_callback_list</span><span class="p">(</span><span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span>

        <span class="n">callbacks</span> <span class="o">=</span> <span class="k">self</span><span class="p">.</span><span class="n">_add_plot_callbacks</span><span class="p">(</span><span class="n">callbacks</span><span class="p">,</span> <span class="n">default_plots</span><span class="p">,</span> <span class="p">[</span><span class="n">plot</span><span class="p">.</span><span class="n">Steps</span><span class="p">(),</span> <span class="n">plot</span><span class="p">.</span><span class="n">Rewards</span><span class="p">()])</span>

        <span class="k">self</span><span class="p">.</span><span class="n">_backend_agent</span><span class="p">.</span><span class="n">play</span><span class="p">(</span><span class="n">play_context</span><span class="o">=</span><span class="n">play_context</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">play_context</span>
</pre></div>


</details>
<h5 id="save_8">save</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">save</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span>
</pre></div>


<p>Saves the currently trained actor policy in directory.</p>
<p>If save is called before a trained policy is created, eg by calling train, an exception is raised.</p>
<p>Args:
     directory: the directory to save the policy weights to.
        if the directory does not exist yet, a new directory is created. if None the policy is saved
        in a temp directory.
     callbacks: list of callbacks called during save (eg log.Agent)</p>
<p>Returns:
    the absolute path to the directory containing the saved policy.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="k">save</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="nl">directory</span><span class="p">:</span><span class="w"> </span><span class="n">Optional</span><span class="o">[</span><span class="n">str</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">None</span><span class="p">,</span><span class="w"></span>

<span class="w">             </span><span class="nl">callbacks</span><span class="p">:</span><span class="w"> </span><span class="ow">Union</span><span class="o">[</span><span class="n">List[core.AgentCallback</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="err">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">None</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="nf">str</span><span class="err">:</span><span class="w"></span>

<span class="w">        </span><span class="ss">&quot;&quot;&quot;Saves the currently trained actor policy in directory.</span>

<span class="ss">        If save is called before a trained policy is created, eg by calling train, an exception is raised.</span>

<span class="ss">        Args:</span>

<span class="ss">             directory: the directory to save the policy weights to.</span>

<span class="ss">                if the directory does not exist yet, a new directory is created. if None the policy is saved</span>

<span class="ss">                in a temp directory.</span>

<span class="ss">             callbacks: list of callbacks called during save (eg log.Agent)</span>

<span class="ss">        Returns:</span>

<span class="ss">            the absolute path to the directory containing the saved policy.</span>

<span class="ss">        &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">directory</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">None</span><span class="err">:</span><span class="w"></span>

<span class="w">            </span><span class="n">directory</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bcore</span><span class="p">.</span><span class="n">_get_temp_path</span><span class="p">()</span><span class="w"></span>

<span class="w">        </span><span class="n">assert</span><span class="w"> </span><span class="n">directory</span><span class="w"></span>

<span class="w">        </span><span class="n">assert</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_backend_agent</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">_is_policy_trained</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;No trained policy available. Call train() first.&quot;</span><span class="w"></span>

<span class="w">        </span><span class="n">directory</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bcore</span><span class="p">.</span><span class="n">_mkdir</span><span class="p">(</span><span class="n">directory</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">agent_json_path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">os</span><span class="p">.</span><span class="k">path</span><span class="p">.</span><span class="k">join</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span><span class="w"> </span><span class="n">EasyAgent</span><span class="p">.</span><span class="n">_KEY_EASYAGENT_FILENAME</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">with</span><span class="w"> </span><span class="k">open</span><span class="p">(</span><span class="n">agent_json_path</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;w&#39;</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nl">jsonfile</span><span class="p">:</span><span class="w"></span>

<span class="w">            </span><span class="n">agent_dict</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_to_dict</span><span class="p">()</span><span class="w"></span>

<span class="w">            </span><span class="n">json</span><span class="p">.</span><span class="k">dump</span><span class="p">(</span><span class="n">agent_dict</span><span class="p">,</span><span class="w"> </span><span class="n">jsonfile</span><span class="p">,</span><span class="w"> </span><span class="n">sort_keys</span><span class="o">=</span><span class="k">True</span><span class="p">,</span><span class="w"> </span><span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">callbacks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_to_callback_list</span><span class="p">(</span><span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">policy_directory</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">os</span><span class="p">.</span><span class="k">path</span><span class="p">.</span><span class="k">join</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span><span class="w"> </span><span class="n">EasyAgent</span><span class="p">.</span><span class="n">_KEY_POLICY_DIRECTORY</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">policy_directory</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bcore</span><span class="p">.</span><span class="n">_mkdir</span><span class="p">(</span><span class="n">policy_directory</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">_backend_agent</span><span class="p">.</span><span class="k">save</span><span class="p">(</span><span class="n">directory</span><span class="o">=</span><span class="n">policy_directory</span><span class="p">,</span><span class="w"> </span><span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">directory</span><span class="w"></span>
</pre></div>


</details>
<h5 id="train_8">train</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20000</span><span class="p">,</span>
    <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span>
    <span class="n">num_steps_per_iteration</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">num_steps_buffer_preload</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">num_steps_sampled_from_buffer</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="n">num_iterations_between_eval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="n">num_episodes_per_eval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">,</span>
    <span class="n">train_context</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">StepsTrainContext</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">default_plots</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span>
</pre></div>


<p>Trains a new model using the gym environment passed during instantiation.</p>
<p>Args:
    callbacks: list of callbacks called during training and evaluation
    num_iterations: number of times the training is repeated (with additional data)
    max_steps_per_episode: maximum number of steps per episode
    num_steps_per_iteration: number of steps played per training iteration
    num_steps_buffer_preload: number of initial collect steps to preload the buffer
    num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training
    num_iterations_between_eval: number of training iterations before the current policy is evaluated.
        if 0 no evaluation is performed.
    num_episodes_per_eval: number of episodes played to estimate the average return and steps
    learning_rate: the learning rate used in the next iteration's policy training (0,1]
    train_context: training configuration to be used. if set overrides all other training context arguments.
    default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...).
        if None default callbacks are only added if the callbacks list is empty</p>
<p>Returns:
    train_context: the training configuration containing the loss and sum of rewards encountered
        during training</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="n">def</span> <span class="n">train</span><span class="p">(</span><span class="k">self</span><span class="p">,</span>

              <span class="n">callbacks</span><span class="p">:</span> <span class="k">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">],</span> <span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">,</span> <span class="k">None</span><span class="p">]</span> <span class="o">=</span> <span class="k">None</span><span class="p">,</span>

              <span class="n">num_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20000</span><span class="p">,</span>

              <span class="n">max_steps_per_episode</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span>

              <span class="n">num_steps_per_iteration</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>

              <span class="n">num_steps_buffer_preload</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>

              <span class="n">num_steps_sampled_from_buffer</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>

              <span class="n">num_iterations_between_eval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>

              <span class="n">num_episodes_per_eval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>

              <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">0</span><span class="p">.</span><span class="mi">001</span><span class="p">,</span>

              <span class="n">train_context</span><span class="p">:</span> <span class="n">core</span><span class="p">.</span><span class="n">StepsTrainContext</span> <span class="o">=</span> <span class="k">None</span><span class="p">,</span>

              <span class="n">default_plots</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="k">None</span><span class="p">):</span>

        <span class="ss">&quot;&quot;&quot;Trains a new model using the gym environment passed during instantiation.</span>

<span class="ss">        Args:</span>

<span class="ss">            callbacks: list of callbacks called during training and evaluation</span>

<span class="ss">            num_iterations: number of times the training is repeated (with additional data)</span>

<span class="ss">            max_steps_per_episode: maximum number of steps per episode</span>

<span class="ss">            num_steps_per_iteration: number of steps played per training iteration</span>

<span class="ss">            num_steps_buffer_preload: number of initial collect steps to preload the buffer</span>

<span class="ss">            num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training</span>

<span class="ss">            num_iterations_between_eval: number of training iterations before the current policy is evaluated.</span>

<span class="ss">                if 0 no evaluation is performed.</span>

<span class="ss">            num_episodes_per_eval: number of episodes played to estimate the average return and steps</span>

<span class="ss">            learning_rate: the learning rate used in the next iteration&#39;s policy training (0,1]</span>

<span class="ss">            train_context: training configuration to be used. if set overrides all other training context arguments.</span>

<span class="ss">            default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...).</span>

<span class="ss">                if None default callbacks are only added if the callbacks list is empty</span>

<span class="ss">        Returns:</span>

<span class="ss">            train_context: the training configuration containing the loss and sum of rewards encountered</span>

<span class="ss">                during training</span>

<span class="ss">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">train_context</span> <span class="k">is</span> <span class="k">None</span><span class="p">:</span>

            <span class="n">train_context</span> <span class="o">=</span> <span class="n">core</span><span class="p">.</span><span class="n">StepsTrainContext</span><span class="p">()</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">num_iterations</span> <span class="o">=</span> <span class="n">num_iterations</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">max_steps_per_episode</span> <span class="o">=</span> <span class="n">max_steps_per_episode</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">num_steps_per_iteration</span> <span class="o">=</span> <span class="n">num_steps_per_iteration</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">num_steps_buffer_preload</span> <span class="o">=</span> <span class="n">num_steps_buffer_preload</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">num_steps_sampled_from_buffer</span> <span class="o">=</span> <span class="n">num_steps_sampled_from_buffer</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">num_iterations_between_eval</span> <span class="o">=</span> <span class="n">num_iterations_between_eval</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">num_episodes_per_eval</span> <span class="o">=</span> <span class="n">num_episodes_per_eval</span>

            <span class="n">train_context</span><span class="p">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>

        <span class="n">super</span><span class="p">().</span><span class="n">train</span><span class="p">(</span><span class="n">train_context</span><span class="o">=</span><span class="n">train_context</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span> <span class="n">default_plots</span><span class="o">=</span><span class="n">default_plots</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">train_context</span>
</pre></div>


</details>
                
                  
                
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../../../documentation/Markdown/Release_Notes/" title="Release Notes" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Release Notes
              </span>
            </div>
          </a>
        
        
          <a href="../core/" title="Core" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Core
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        Powered by
        <a href="http://timothycrosley.github.io/portray">portray.</a>
        You too can
        <a href="http://timothycrosley.github.io/portray">
          portray</a>
        your Python project well using automatic documentation.
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../../../assets/javascripts/application.808e90bb.js"></script>
      
      <script>app.initialize({version:"1.0.4",url:{base:"../../.."}})</script>
      
    
  </body>
</html>