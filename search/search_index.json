{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Reinforcement Learning for Practitioners (v1.4.1, 20Q1) Status: under active development, breaking changes may occur. Release notes . EasyAgents is a high level reinforcement learning api focusing on ease of use and simplicity. Written in Python and running on top of established reinforcement learning libraries like tf-Agents , tensorforce or keras-rl . Environments are implemented in OpenAI gym . For an example of an industrial application of reinforcement learning see here . In collaboration with Oliver Zeigermann . Features provides the same, simple api across all libraries . Thus you can easily switch between different implementations and you don't have to learn for each of them a new api. to create and run any algorithm you only need 2 lines of code , all the parameters are named consistently across all algorithms. supports a broad set of different algorithms runs inside jupyter notebooks as well as stand-alone, easy to install requiring only a single 'pip install easyagents'. easy to understand, ready-made plots and logs to investigate the algorithms and environments behaviour Note: keras-rl backend is suspended until support for tensorflow 2.0 is available. Examples from easyagents.agents import PpoAgent from easyagents.callbacks import plot ppoAgent = PpoAgent ( 'CartPole-v0' ) ppoAgent . train ([ plot . State (), plot . Loss (), plot . Rewards ()]) More Detailed from easyagents.agents import PpoAgent from easyagents.callbacks import plot ppoAgent = PpoAgent ( 'Orso-v1' , fc_layers = ( 500 , 500 , 500 )) ppoAgent . train ([ plot . State (), plot . Loss (), plot . Rewards (), plot . Actions (), plot . StepRewards (), plot . Steps (), plot . ToMovie ()], learning_rate = 0.0001 , num_iterations = 500 , max_steps_per_episode = 50 ) Tutorials 1. Introduction (CartPole on colab) : training, plotting, switching algorithms & backends. Based on the classic reinforcement learning example balancing a stick on a cart. 2. Next steps & backend switching (Orso on colab) : custom training, creating a movie & switching backends. gym environment based on a routing problem. 3. Controlling training & evaluation (on colab) : or 'what do all these agent.train(...) args mean ?' 4. Creating your own environment (LineWorld on colab) : implement a gym environment from scratch, workshop example. 5. Saving & loading (on colab) : Once a policy is trained, save it and reload it in a production environment. You may also save intermediate policies as the training proceeds. 6. Switching backends (on colab) : See how you can switch between backend implementations. 7. Api logging, seeding & plot clearing (on colab) : Investigate how easyagent interacts with the backend api and the gym environment; how to set seeds; controlling jupyter output cell clearing Available Algorithms and Backends algorithm tf-Agents tensorforce keras-rl (suspended) easyagents class name CEM not available not available yes CemAgent Dqn yes yes yes DqnAgent Double Dqn open not available yes DoubleDqnAgent Dueling Dqn not available yes yes DuelingDqnAgent Ppo yes yes not available PpoAgent Random yes yes not available RandomAgent REINFORCE yes yes not available ReinforceAgent SAC preview not available not available SacAgent [191001] if you are interested in other algorithms, backends or hyperparameters let us know by creating an issue . We'll try our best to support you. for a documentation of the agents api see here . starting with easyagents 1.3 (191102) the backend for keras-rl is suspended until support for tensorflow 2.0 is available. Industrial Application Geberit - a sanitary technology company with > 12'000 employees - produces in particular pipes and other parts to get rain-water of flat roofs - so called syphonic roof drainage systems . They warrant that large buildings like stadiums, airports or shopping malls do not collapse during heavy rainfalls. However it is surprisingly difficult to find the right dimensions for the pipes. It is actually so difficult, that as of today no feasable, deterministic algorithm is known. Thus traditional heuristics and classic machine learning were used to support the users in finding a suitable solution. Using reinforcement learning the failrate of the previous solution was reduced by 70%, resulting in an end-to-end success-rate of > 98%. For more details take a look at this talk . Installation Install from pypi using pip: pip install easyagents More Documentation for release notes & class diagram , for agents & api . Guiding Principles easily train, evaluate & debug policies for (you own) gym environment over \"designing new algorithms\" simple & consistent over \"flexible & powerful\" inspired by keras: same api across all algorithms support different implementations of the same algorithm extensible (pluggable backends, plots & training schemes) EasyAgents may not be ideal if you would like to leverage implementation specific advantages of an algorithm you want to do distributed or in parallel reinforcement learning Note If you have any difficulties in installing or using easyagents please let us know by creating an issue . We'll try our best to help you. Any ideas, help, suggestions, comments etc in python / open source development / reinforcement learning / whatever are more than welcome.","title":"Home"},{"location":"#reinforcement-learning-for-practitioners-v141-20q1","text":"Status: under active development, breaking changes may occur. Release notes . EasyAgents is a high level reinforcement learning api focusing on ease of use and simplicity. Written in Python and running on top of established reinforcement learning libraries like tf-Agents , tensorforce or keras-rl . Environments are implemented in OpenAI gym . For an example of an industrial application of reinforcement learning see here . In collaboration with Oliver Zeigermann .","title":"Reinforcement Learning for Practitioners (v1.4.1, 20Q1)"},{"location":"#features","text":"provides the same, simple api across all libraries . Thus you can easily switch between different implementations and you don't have to learn for each of them a new api. to create and run any algorithm you only need 2 lines of code , all the parameters are named consistently across all algorithms. supports a broad set of different algorithms runs inside jupyter notebooks as well as stand-alone, easy to install requiring only a single 'pip install easyagents'. easy to understand, ready-made plots and logs to investigate the algorithms and environments behaviour Note: keras-rl backend is suspended until support for tensorflow 2.0 is available.","title":"Features"},{"location":"#examples","text":"from easyagents.agents import PpoAgent from easyagents.callbacks import plot ppoAgent = PpoAgent ( 'CartPole-v0' ) ppoAgent . train ([ plot . State (), plot . Loss (), plot . Rewards ()])","title":"Examples"},{"location":"#more-detailed","text":"from easyagents.agents import PpoAgent from easyagents.callbacks import plot ppoAgent = PpoAgent ( 'Orso-v1' , fc_layers = ( 500 , 500 , 500 )) ppoAgent . train ([ plot . State (), plot . Loss (), plot . Rewards (), plot . Actions (), plot . StepRewards (), plot . Steps (), plot . ToMovie ()], learning_rate = 0.0001 , num_iterations = 500 , max_steps_per_episode = 50 )","title":"More Detailed"},{"location":"#tutorials","text":"1. Introduction (CartPole on colab) : training, plotting, switching algorithms & backends. Based on the classic reinforcement learning example balancing a stick on a cart. 2. Next steps & backend switching (Orso on colab) : custom training, creating a movie & switching backends. gym environment based on a routing problem. 3. Controlling training & evaluation (on colab) : or 'what do all these agent.train(...) args mean ?' 4. Creating your own environment (LineWorld on colab) : implement a gym environment from scratch, workshop example. 5. Saving & loading (on colab) : Once a policy is trained, save it and reload it in a production environment. You may also save intermediate policies as the training proceeds. 6. Switching backends (on colab) : See how you can switch between backend implementations. 7. Api logging, seeding & plot clearing (on colab) : Investigate how easyagent interacts with the backend api and the gym environment; how to set seeds; controlling jupyter output cell clearing","title":"Tutorials"},{"location":"#available-algorithms-and-backends","text":"algorithm tf-Agents tensorforce keras-rl (suspended) easyagents class name CEM not available not available yes CemAgent Dqn yes yes yes DqnAgent Double Dqn open not available yes DoubleDqnAgent Dueling Dqn not available yes yes DuelingDqnAgent Ppo yes yes not available PpoAgent Random yes yes not available RandomAgent REINFORCE yes yes not available ReinforceAgent SAC preview not available not available SacAgent [191001] if you are interested in other algorithms, backends or hyperparameters let us know by creating an issue . We'll try our best to support you. for a documentation of the agents api see here . starting with easyagents 1.3 (191102) the backend for keras-rl is suspended until support for tensorflow 2.0 is available.","title":"Available Algorithms and Backends"},{"location":"#industrial-application","text":"Geberit - a sanitary technology company with > 12'000 employees - produces in particular pipes and other parts to get rain-water of flat roofs - so called syphonic roof drainage systems . They warrant that large buildings like stadiums, airports or shopping malls do not collapse during heavy rainfalls. However it is surprisingly difficult to find the right dimensions for the pipes. It is actually so difficult, that as of today no feasable, deterministic algorithm is known. Thus traditional heuristics and classic machine learning were used to support the users in finding a suitable solution. Using reinforcement learning the failrate of the previous solution was reduced by 70%, resulting in an end-to-end success-rate of > 98%. For more details take a look at this talk .","title":"Industrial Application"},{"location":"#installation","text":"Install from pypi using pip: pip install easyagents","title":"Installation"},{"location":"#more","text":"","title":"More"},{"location":"#documentation","text":"for release notes & class diagram , for agents & api .","title":"Documentation"},{"location":"#guiding-principles","text":"easily train, evaluate & debug policies for (you own) gym environment over \"designing new algorithms\" simple & consistent over \"flexible & powerful\" inspired by keras: same api across all algorithms support different implementations of the same algorithm extensible (pluggable backends, plots & training schemes)","title":"Guiding Principles"},{"location":"#easyagents-may-not-be-ideal-if","text":"you would like to leverage implementation specific advantages of an algorithm you want to do distributed or in parallel reinforcement learning","title":"EasyAgents may not be ideal if"},{"location":"#note","text":"If you have any difficulties in installing or using easyagents please let us know by creating an issue . We'll try our best to help you. Any ideas, help, suggestions, comments etc in python / open source development / reinforcement learning / whatever are more than welcome.","title":"Note"},{"location":"CODE_OF_CONDUCT/","text":"Contributor Covenant Code of Conduct Our Pledge In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. Our Standards Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Our Responsibilities Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful. Scope This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers. Enforcement Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at . All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership. Attribution This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"Code Of Conduct"},{"location":"CODE_OF_CONDUCT/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"CODE_OF_CONDUCT/#our-pledge","text":"In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.","title":"Our Pledge"},{"location":"CODE_OF_CONDUCT/#our-standards","text":"Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"CODE_OF_CONDUCT/#our-responsibilities","text":"Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.","title":"Our Responsibilities"},{"location":"CODE_OF_CONDUCT/#scope","text":"This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.","title":"Scope"},{"location":"CODE_OF_CONDUCT/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at . All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.","title":"Enforcement"},{"location":"CODE_OF_CONDUCT/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"Attribution"},{"location":"CONTRIBUTING/","text":"Contributing Interested in contributing to EasyAgents ? We appreciate all kinds of help. Pull Requests We gladly welcome pull requests . Before making any changes, we recommend opening an issue (if it doesn't already exist) and discussing your proposed changes. This will let us give you advice on the proposed changes. If the changes are minor, then feel free to make them without discussion. General Guidelines While not being dogmatic, we try to adhere to the following * Google Docstring for code documentation * PEP 8 as style guide for code * PyTest for unit testing * Typing to support type hints Supported Platforms Jupyter notebooks on colab using the latest Chrome browser Python 3.7 on Windows and Ubuntu (Xenial)","title":"Contributing"},{"location":"CONTRIBUTING/#contributing","text":"Interested in contributing to EasyAgents ? We appreciate all kinds of help.","title":"Contributing"},{"location":"CONTRIBUTING/#pull-requests","text":"We gladly welcome pull requests . Before making any changes, we recommend opening an issue (if it doesn't already exist) and discussing your proposed changes. This will let us give you advice on the proposed changes. If the changes are minor, then feel free to make them without discussion.","title":"Pull Requests"},{"location":"CONTRIBUTING/#general-guidelines","text":"While not being dogmatic, we try to adhere to the following * Google Docstring for code documentation * PEP 8 as style guide for code * PyTest for unit testing * Typing to support type hints","title":"General Guidelines"},{"location":"CONTRIBUTING/#supported-platforms","text":"Jupyter notebooks on colab using the latest Chrome browser Python 3.7 on Windows and Ubuntu (Xenial)","title":"Supported Platforms"},{"location":"documentation/Markdown/Release_Notes/","text":"Release notes v1.4 [20Q1] 1.4.1: tensorforce reintegrated (due to an incompatibility between tfagents and tensorforce, tensorforce must be explicitely activated by a call to agents.activate_tensorforce() ) upgrade to tfagents 0.3, tensorflow 2.0.1 kwargs for register_with_gym 1.4.0: agent saving & loading (see intro Saving & loading a trained policy ); lineworld as test environment included v1.3 [19Q4] 1.3.1: agent.score substituted by agent.evalute; 1.3.0: migration to tensorflow 2.0; support for tensorforce and keras-rl suspended until support for tf 2.0 is available v1.2 [19Q3] 1.2.2: fix for CemAgent and SacAgent default backend registration 1.2.1: SacAgent for tfagents preview; notebook on 'Agent logging, seeding and jupyter output cells' 1.2.0: Agent.score v1.1 [19Q3] 1.1.23: CemAgent for keras-rl backend; DqnAgent, RandomAgent for tensorforce 1.1.22: DuelingDqnAgent, DoubleDqnAgent with keras-rl backend 1.1.21: keras-rl backend (dqn) 1.1.20: #54 logging in jupyter notebook solved, doc updates 1.1.19: jupyter plotting performance improved plot.ToMovie with support for animated gifs 1.1.18: tensorforce backend (ppo, reinforce) 1.1.11: plot.StepRewards, plot.Actions default_plots parameter (instead of default_callbacks) v1.0.1 [19Q3] api based on pluggable backends and callbacks (for plotting, logging, training durations) backend: tf-agents, default algorithms: dqn, ppo, random plots: State, Loss (including actor-/critic loss), Steps, Rewards support for creating a mp4 movie (plot.ToMovie) v0.1 [19Q2] prototype implementation / proof of concept hard-wired support for Ppo, Reinforce, Dqn on tf-agents hard-wired plots for loss, sum-of-rewards, steps and state rendering hard-wired mp4 rendering Design guidelines separate \"public api\" from concrete implementation using a frontend / backend architecture (inspired by scikit learn, matplotlib, keras) pluggable backends extensible through callbacks (inspired by keras). separate callback types for training, evaluation and monitoring pre-configurable, algorithm specific train & play loops Class diagram","title":"Release Notes"},{"location":"documentation/Markdown/Release_Notes/#release-notes","text":"v1.4 [20Q1] 1.4.1: tensorforce reintegrated (due to an incompatibility between tfagents and tensorforce, tensorforce must be explicitely activated by a call to agents.activate_tensorforce() ) upgrade to tfagents 0.3, tensorflow 2.0.1 kwargs for register_with_gym 1.4.0: agent saving & loading (see intro Saving & loading a trained policy ); lineworld as test environment included v1.3 [19Q4] 1.3.1: agent.score substituted by agent.evalute; 1.3.0: migration to tensorflow 2.0; support for tensorforce and keras-rl suspended until support for tf 2.0 is available v1.2 [19Q3] 1.2.2: fix for CemAgent and SacAgent default backend registration 1.2.1: SacAgent for tfagents preview; notebook on 'Agent logging, seeding and jupyter output cells' 1.2.0: Agent.score v1.1 [19Q3] 1.1.23: CemAgent for keras-rl backend; DqnAgent, RandomAgent for tensorforce 1.1.22: DuelingDqnAgent, DoubleDqnAgent with keras-rl backend 1.1.21: keras-rl backend (dqn) 1.1.20: #54 logging in jupyter notebook solved, doc updates 1.1.19: jupyter plotting performance improved plot.ToMovie with support for animated gifs 1.1.18: tensorforce backend (ppo, reinforce) 1.1.11: plot.StepRewards, plot.Actions default_plots parameter (instead of default_callbacks) v1.0.1 [19Q3] api based on pluggable backends and callbacks (for plotting, logging, training durations) backend: tf-agents, default algorithms: dqn, ppo, random plots: State, Loss (including actor-/critic loss), Steps, Rewards support for creating a mp4 movie (plot.ToMovie) v0.1 [19Q2] prototype implementation / proof of concept hard-wired support for Ppo, Reinforce, Dqn on tf-agents hard-wired plots for loss, sum-of-rewards, steps and state rendering hard-wired mp4 rendering","title":"Release notes"},{"location":"documentation/Markdown/Release_Notes/#design-guidelines","text":"separate \"public api\" from concrete implementation using a frontend / backend architecture (inspired by scikit learn, matplotlib, keras) pluggable backends extensible through callbacks (inspired by keras). separate callback types for training, evaluation and monitoring pre-configurable, algorithm specific train & play loops","title":"Design guidelines"},{"location":"documentation/Markdown/Release_Notes/#class-diagram","text":"","title":"Class diagram"},{"location":"reference/easyagents/agents/","text":"Module easyagents.agents This module contains the public api of the EasyAgents reinforcement learning library. It consist mainly of the class hierarchy of the available agents (algorithms), registrations and the management of the available backends. In their implementation the agents forward their calls to the chosen backend. View Source \"\"\"This module contains the public api of the EasyAgents reinforcement learning library. It consist mainly of the class hierarchy of the available agents (algorithms), registrations and the management of the available backends. In their implementation the agents forward their calls to the chosen backend. \"\"\" from abc import ABC from collections import namedtuple import json import os import statistics from typing import Dict , List , Optional , Tuple , Type , Union from easyagents import core from easyagents.backends import core as bcore from easyagents.callbacks import plot import easyagents.backends.default import easyagents.backends.tfagents import tensorflow as tf # import easyagents.backends.tforce _backends : [ bcore . BackendAgentFactory ] = [] \"\"\"The seed used for all agents and gym environments. If None no seed is set (default).\"\"\" seed : Optional [ int ] = None def load ( directory : str , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None ): \"\"\"Loads an agent from directory. After a successful load play() may be called directly. The agent, model, backend, seed and play policy are restored according to the previously saved agent. Args: directory: the directory containing the previously saved policy. callbacks: list of callbacks called during save (eg log.Agent) Result: a new instance of EasyAgents \"\"\" assert directory agent_json_path = os . path . join ( directory , EasyAgent . _KEY_EASYAGENT_FILENAME ) policy_directory = os . path . join ( directory , EasyAgent . _KEY_POLICY_DIRECTORY ) assert os . path . isdir ( directory ), f 'directory \"{directory}\" not found.' assert os . path . isfile ( agent_json_path ), 'file \"{agent_json_path}\" not found.' assert os . path . isdir ( policy_directory ), f 'directory \"{policy_directory}\" not found.' with open ( agent_json_path ) as jsonfile : agent_dict = json . load ( jsonfile ) result = EasyAgent . _from_dict ( agent_dict ) callbacks = result . _to_callback_list ( callbacks = callbacks ) result . _backend_agent . load ( directory = policy_directory , callbacks = callbacks ) return result def register_backend ( backend : bcore . BackendAgentFactory ): \"\"\"registers a backend as a factory for agent implementations. If another backend with the same name is already registered, the old backend is replaced by backend. \"\"\" assert backend old_backends = [ b for b in _backends if b . backend_name == backend . backend_name ] for old_backend in old_backends : _backends . remove ( old_backend ) _backends . append ( backend ) def activate_tensorforce (): \"\"\"registers the tensorforce backend. Due to an incompatibility between tensorforce and tf-agents, both libraries may not run in the same python instance. Thus - for the time being - once this method is called, the tfagents backend may not be used anymore. \"\"\" import easyagents.backends.tforce global _backends assert easyagents . backends . core . _tf_eager_execution_active is None or \\ easyagents . backends . core . _tf_eager_execution_active == False , \\ \"tensorforce can not be activated, since tensorflow eager execution mode was already actived.\" _backends = [] register_backend ( easyagents . backends . default . DefaultAgentFactory ( register_tensorforce = True )) register_backend ( easyagents . backends . tforce . TensorforceAgentFactory ()) def _activate_tfagents (): \"\"\"registers the tfagents backend. Due to an incompatibility between tensorforce and tf-agents, both libraries may not run in the same python instance. \"\"\" global _backends assert easyagents . backends . core . _tf_eager_execution_active is None or \\ easyagents . backends . core . _tf_eager_execution_active == True , \\ \"tfagents can not be activated, since tensorflow eager execution mode was already disabled.\" _backends = [] register_backend ( easyagents . backends . default . DefaultAgentFactory ( register_tensorforce = False )) register_backend ( easyagents . backends . tfagents . TfAgentAgentFactory ()) _activate_tfagents () class EasyAgent ( ABC ): \"\"\"Abstract base class for all easy reinforcment learning agents. Besides forwarding train and play it implements persistence.\"\"\" _KEY_BACKEND = 'backend' _KEY_EASYAGENT_CLASS = 'easyagent_class' _KEY_EASYAGENT_FILENAME = 'easyagent.json' _KEY_MODEL_CONFIG = 'model_config' _KEY_POLICY_DIRECTORY = 'policy' _KEY_VERSION = 'version' def __init__ ( self , gym_env_name : str , fc_layers : Union [ Tuple [ int , ... ], int , None ] = None , backend : str = None ): \"\"\" Args: gym_env_name: name of an OpenAI gym environment to be used for training and evaluation fc_layers: defines the neural network to be used, a sequence of fully connected layers of the given size. Eg (75,40) yields a neural network consisting out of 2 hidden layers, the first one containing 75 and the second layer containing 40 neurons. backend=the backend to be used (eg 'tfagents'), if None a default implementation is used. call get_backends() to get a list of the available backends. \"\"\" model_config = core . ModelConfig ( gym_env_name = gym_env_name , fc_layers = fc_layers , seed = seed ) self . _initialize ( model_config = model_config , backend_name = backend ) return def _initialize ( self , model_config : core . ModelConfig , backend_name : str = None ): if backend_name is None : backend_name = easyagents . backends . default . DefaultAgentFactory . backend_name backend : bcore . BackendAgentFactory = _get_backend ( backend_name ) assert model_config is not None , \"model_config not set.\" assert backend , f 'Backend \"{backend_name}\" not found. The registered backends are {get_backends()}.' self . _model_config : core . ModelConfig = model_config backend_agent = backend . create_agent ( easyagent_type = type ( self ), model_config = model_config ) assert backend_agent , f 'Backend \"{backend_name}\" does not implement \"{type(self).__name__}\". ' + \\ f 'Choose one of the following backend {get_backends(type(self))}.' self . _backend_agent : Optional [ bcore . _BackendAgent ] = backend_agent self . _backend_name : str = backend_name self . _backend_agent . _agent_context . _agent_saver = self . save return def _add_plot_callbacks ( self , callbacks : List [ core . AgentCallback ], default_plots : Optional [ bool ], default_plot_callbacks : List [ plot . _PlotCallback ] ) -> List [ core . AgentCallback ]: \"\"\"Adds the default callbacks and sorts all callbacks in the order _PreProcessCallbacks, AgentCallbacks, _PostProcessCallbacks. Args: callbacks: existing callbacks to prepare default_plots: if set or if None and callbacks does not contain plots then the default plots are added default_plot_callbacks: plot callbacks to add. \"\"\" pre_process : List [ core . AgentCallback ] = [ plot . _PreProcess ()] agent : List [ core . AgentCallback ] = [] post_process : List [ core . AgentCallback ] = [ plot . _PostProcess ()] if default_plots is None : default_plots = True for c in callbacks : default_plots = default_plots and ( not isinstance ( c , plot . _PlotCallback )) if default_plots : agent = default_plot_callbacks for c in callbacks : if isinstance ( c , core . _PreProcessCallback ): pre_process . append ( c ) else : if isinstance ( c , core . _PostProcessCallback ): post_process . append ( c ) else : agent . append ( c ) result : List [ core . AgentCallback ] = pre_process + agent + post_process return result @staticmethod def _from_dict ( param_dict : Dict [ str , object ]): \"\"\"recreates a new agent instance according to the definition previously created by _to_dict. Returns: new agent instance (excluding any trained policy), the agent type is preserved. \"\"\" assert param_dict mc : core . ModelConfig = core . ModelConfig . _from_dict ( param_dict [ EasyAgent . _KEY_MODEL_CONFIG ]) agent_class = globals ()[ param_dict [ EasyAgent . _KEY_EASYAGENT_CLASS ]] backend : str = param_dict [ EasyAgent . _KEY_BACKEND ] result = agent_class ( gym_env_name = mc . original_env_name , backend = backend ) result . _initialize ( model_config = mc , backend_name = backend ) return result def _to_callback_list ( self , callbacks : Union [ Optional [ core . AgentCallback ], List [ core . AgentCallback ]]) -> List [ core . AgentCallback ]: \"\"\"maps callbacks to an admissible callback list. if callbacks is None an empty list is returned. if callbacks is an AgentCallback a list containing only this callback is returned otherwise callbacks is returned \"\"\" result : List [ core . AgentCallback ] = [] if not callbacks is None : if isinstance ( callbacks , core . AgentCallback ): result = [ callbacks ] else : assert isinstance ( callbacks , list ), \"callback not an AgentCallback or a list thereof.\" result = callbacks return result def _to_dict ( self ) -> Dict [ str , object ]: \"\"\"saves the agent definition to a dict. Returns: dict containing all parameters to recreate the agent (excluding a trained policy) \"\"\" result : Dict [ str , object ] = dict () result [ EasyAgent . _KEY_VERSION ] = easyagents . __version__ result [ EasyAgent . _KEY_EASYAGENT_CLASS ] = self . __class__ . __name__ result [ EasyAgent . _KEY_BACKEND ] = self . _backend_name result [ EasyAgent . _KEY_MODEL_CONFIG ] = self . _model_config . _to_dict () result [ EasyAgent . _KEY_POLICY_DIRECTORY ] = 'policy' return result def evaluate ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ): \"\"\"Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: extensible score metrics \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) Metrics = namedtuple ( 'Metrics' , 'steps rewards' ) Rewards = namedtuple ( 'Rewards' , 'mean std min max all' ) all_rewards = list ( play_context . sum_of_rewards . values ()) mean_reward , std_reward , min_reward , max_reward = statistics . mean ( all_rewards ), statistics . stdev ( all_rewards ), min ( all_rewards ), max ( all_rewards ) rewards = Rewards ( mean = mean_reward , std = std_reward , min = min_reward , max = max_reward , all = all_rewards ) Steps = namedtuple ( 'Steps' , 'mean std min max all' ) all_num_steps = [] for i in play_context . rewards . keys (): all_num_steps . append ( len ( play_context . rewards [ i ])) mean_steps , std_steps , min_steps , max_steps = statistics . mean ( all_num_steps ), statistics . stdev ( all_num_steps ), min ( all_num_steps ), max ( all_num_steps ) steps = Steps ( mean = mean_steps , std = std_steps , min = min_steps , max = max_steps , all = all_num_steps ) metrics = Metrics ( rewards = rewards , steps = steps ) return metrics def play ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : core . PlayContext = None , default_plots : bool = None ): \"\"\"Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training \"\"\" assert self . _backend_agent . _agent_context . _is_policy_trained , \"No trained policy available. Call train() first.\" if play_context is None : play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes callbacks = self . _to_callback_list ( callbacks = callbacks ) callbacks = self . _add_plot_callbacks ( callbacks , default_plots , [ plot . Steps (), plot . Rewards ()]) self . _backend_agent . play ( play_context = play_context , callbacks = callbacks ) return play_context def save ( self , directory : Optional [ str ] = None , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None ) -> str : \"\"\"Saves the currently trained actor policy in directory. If save is called before a trained policy is created, eg by calling train, an exception is raised. Args: directory: the directory to save the policy weights to. if the directory does not exist yet, a new directory is created. if None the policy is saved in a temp directory. callbacks: list of callbacks called during save (eg log.Agent) Returns: the absolute path to the directory containing the saved policy. \"\"\" if directory is None : directory = bcore . _get_temp_path () assert directory assert self . _backend_agent . _agent_context . _is_policy_trained , \"No trained policy available. Call train() first.\" directory = bcore . _mkdir ( directory ) agent_json_path = os . path . join ( directory , EasyAgent . _KEY_EASYAGENT_FILENAME ) with open ( agent_json_path , 'w' ) as jsonfile : agent_dict = self . _to_dict () json . dump ( agent_dict , jsonfile , sort_keys = True , indent = 2 ) callbacks = self . _to_callback_list ( callbacks = callbacks ) policy_directory = os . path . join ( directory , EasyAgent . _KEY_POLICY_DIRECTORY ) policy_directory = bcore . _mkdir ( policy_directory ) self . _backend_agent . save ( directory = policy_directory , callbacks = callbacks ) return directory def train ( self , train_context : core . TrainContext , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ], default_plots : Optional [ bool ]): \"\"\"Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during the training and evaluation train_context: training configuration to be used (num_iterations,num_episodes_per_iteration,...) default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty \"\"\" assert train_context , \"train_context not set.\" callbacks = self . _to_callback_list ( callbacks = callbacks ) callbacks = self . _add_plot_callbacks ( callbacks , default_plots , [ plot . Loss (), plot . Steps (), plot . Rewards ()]) self . _backend_agent . train ( train_context = train_context , callbacks = callbacks ) def get_backends ( agent : Optional [ Type [ EasyAgent ]] = None ): \"\"\"returns a list of all registered backends containing an implementation for the EasyAgent type agent. Args: agent: type deriving from EasyAgent for which the backend identifiers are returned. Returns: a list of admissible values for the 'backend' argument of EazyAgents constructors or a list of all available backends if agent is None. \"\"\" result = [ b . backend_name for b in _backends ] if agent : result = [ b . backend_name for b in _backends if agent in b . get_algorithms ()] return result def _get_backend ( backend_name : str ): \"\"\"Yields the backend with the given name. Returns: the backend instance or None if no backend is found.\"\"\" assert backend_name backends = [ b for b in _backends if b . backend_name == backend_name ] assert len ( backends ) <= 1 , f 'no backend found with name \"{backend_name}\". Available backends = {get_backends()}' result = None if backends : result = backends [ 0 ] return result class CemAgent ( EasyAgent ): \"\"\"creates a new agent based on the cross-entropy-method algorithm. From https://learning.mpi-sws.org/mlss2016/slides/2016-MLSS-RL.pdf: Initialize \u00b5 \u2208Rd,\u03c3 \u2208Rd for iteration = 1,2,... num_iterations do Collect num_episodes_per_iteration samples of \u03b8i \u223c N(\u00b5,diag(\u03c3)) Perform a noisy evaluation Ri \u223c \u03b8i Select the top elite_set_fraction of samples (e.g. p = 0.2), which we\u2019ll call the elite set Fit a Gaussian distribution, with diagonal covariance, to the elite set, obtaining a new \u00b5,\u03c3. end for Return the \ufb01nal \u00b5. see https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.6579&rep=rep1&type=pdf \"\"\" def __init__ ( self , gym_env_name : str , fc_layers : Optional [ Tuple [ int , ... ]] = None , backend : str = None ): super () . __init__ ( gym_env_name , fc_layers , backend ) assert False , \"CemAgent is currently not available (pending migration of keras-rl to tf2.0)\" def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 100 , num_episodes_per_iteration : int = 50 , max_steps_per_episode : int = 500 , elite_set_fraction : float = 0.1 , num_iterations_between_eval : int = 5 , num_episodes_per_eval : int = 10 , train_context : core . CemTrainContext = None , default_plots : bool = None ): \"\"\"Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) num_episodes_per_iteration: number of episodes played in each iteration. for each episode a new policy is sampled from the current weight distribution. max_steps_per_episode: maximum number of steps per episode elite_set_fraction: the fraction of policies which are members of the elite set. These policies are used to fit a new weight distribution in each iteration. num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . CemTrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . elite_set_fraction = elite_set_fraction train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval super () . train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context class DqnAgent ( EasyAgent ): \"\"\"creates a new agent based on the Dqn algorithm. From wikipedia: The DeepMind system used a deep convolutional neural network, with layers of tiled convolutional filters to mimic the effects of receptive fields. Reinforcement learning is unstable or divergent when a nonlinear function approximator such as a neural network is used to represent Q. This instability comes from the correlations present in the sequence of observations, the fact that small updates to Q may significantly change the policy and the data distribution, and the correlations between Q and the target values. The technique used experience replay, a biologically inspired mechanism that uses a random sample of prior actions instead of the most recent action to proceed.[2] This removes correlations in the observation sequence and smooths changes in the data distribution. Iterative update adjusts Q towards target values that are only periodically updated, further reducing correlations with the target.[17] see also: https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning \"\"\" def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 20000 , max_steps_per_episode : int = 500 , num_steps_per_iteration : int = 1 , num_steps_buffer_preload = 1000 , num_steps_sampled_from_buffer = 64 , num_iterations_between_eval : int = 1000 , num_episodes_per_eval : int = 10 , learning_rate : float = 0.001 , train_context : core . StepsTrainContext = None , default_plots : bool = None ): \"\"\"Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) max_steps_per_episode: maximum number of steps per episode num_steps_per_iteration: number of steps played per training iteration num_steps_buffer_preload: number of initial collect steps to preload the buffer num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . StepsTrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . num_steps_per_iteration = num_steps_per_iteration train_context . num_steps_buffer_preload = num_steps_buffer_preload train_context . num_steps_sampled_from_buffer = num_steps_sampled_from_buffer train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super () . train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context class DoubleDqnAgent ( DqnAgent ): \"\"\"Agent based on the Double Dqn algorithm (https://arxiv.org/abs/1509.06461)\"\"\" class DuelingDqnAgent ( DqnAgent ): \"\"\"Agent based on the Dueling Dqn algorithm (https://arxiv.org/abs/1511.06581).\"\"\" class PpoAgent ( EasyAgent ): \"\"\"creates a new agent based on the PPO algorithm. PPO is an actor-critic algorithm using 2 neural networks. The actor network to predict the next action to be taken and the critic network to estimate the value of the game state we are currently in (the expected, discounted sum of future rewards when following the current actor network). see also: https://spinningup.openai.com/en/latest/algorithms/ppo.html \"\"\" def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 100 , num_episodes_per_iteration : int = 10 , max_steps_per_episode : int = 500 , num_epochs_per_iteration : int = 10 , num_iterations_between_eval : int = 5 , num_episodes_per_eval : int = 10 , learning_rate : float = 0.001 , train_context : core . PpoTrainContext = None , default_plots : bool = None ): \"\"\"Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) num_episodes_per_iteration: number of episodes played per training iteration max_steps_per_episode: maximum number of steps per episode num_epochs_per_iteration: number of times the data collected for the current iteration is used to retrain the current policy num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . PpoTrainContext () train_context . num_iterations = num_iterations train_context . num_episodes_per_iteration = num_episodes_per_iteration train_context . max_steps_per_episode = max_steps_per_episode train_context . num_epochs_per_iteration = num_epochs_per_iteration train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super () . train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context class RandomAgent ( EasyAgent ): \"\"\"Agent which always chooses uniform random actions.\"\"\" def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 10 , max_steps_per_episode : int = 1000 , num_episodes_per_eval : int = 10 , train_context : core . TrainContext = None , default_plots : bool = None ): \"\"\"Evaluates the environment using a uniform random policy. The evaluation is performed in batches of num_episodes_per_eval episodes. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times a batch of num_episodes_per_eval episodes is evaluated. max_steps_per_episode: maximum number of steps per episode num_episodes_per_eval: number of episodes played to estimate the average return and steps train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...) Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . TrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . num_epochs_per_iteration = 0 train_context . num_iterations_between_eval = 1 train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = 1 super () . train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context class ReinforceAgent ( EasyAgent ): \"\"\"creates a new agent based on the Reinforce algorithm. Reinforce is a vanilla policy gradient algorithm using a single actor network. see also: www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf \"\"\" def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 100 , num_episodes_per_iteration : int = 10 , max_steps_per_episode : int = 500 , num_epochs_per_iteration : int = 10 , num_iterations_between_eval : int = 5 , num_episodes_per_eval : int = 10 , learning_rate : float = 0.001 , train_context : core . EpisodesTrainContext = None , default_plots : bool = None ): \"\"\"Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) num_episodes_per_iteration: number of episodes played per training iteration max_steps_per_episode: maximum number of steps per episode num_epochs_per_iteration: number of times the data collected for the current iteration is used to retrain the current policy num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . EpisodesTrainContext () train_context . num_iterations = num_iterations train_context . num_episodes_per_iteration = num_episodes_per_iteration train_context . max_steps_per_episode = max_steps_per_episode train_context . num_epochs_per_iteration = num_epochs_per_iteration train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super () . train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context class SacAgent ( DqnAgent ): \"\"\"Agent based on the Soft-Actor-Critic algorithm (https://arxiv.org/abs/1812.05905).\"\"\" Variables seed Functions activate_tensorforce def activate_tensorforce ( ) registers the tensorforce backend. Due to an incompatibility between tensorforce and tf-agents, both libraries may not run in the same python instance. Thus - for the time being - once this method is called, the tfagents backend may not be used anymore. View Source def activate_tensorforce (): \"\"\"registers the tensorforce backend. Due to an incompatibility between tensorforce and tf-agents, both libraries may not run in the same python instance. Thus - for the time being - once this method is called, the tfagents backend may not be used anymore. \"\"\" import easyagents.backends.tforce global _backends assert easyagents . backends . core . _tf_eager_execution_active is None or \\ easyagents . backends . core . _tf_eager_execution_active == False , \\ \"tensorforce can not be activated, since tensorflow eager execution mode was already actived.\" _backends = [] register_backend ( easyagents . backends . default . DefaultAgentFactory ( register_tensorforce = True )) register_backend ( easyagents . backends . tforce . TensorforceAgentFactory ()) get_backends def get_backends ( agent : Union [ Type [ easyagents . agents . EasyAgent ], NoneType ] = None ) returns a list of all registered backends containing an implementation for the EasyAgent type agent. Args: agent: type deriving from EasyAgent for which the backend identifiers are returned. Returns: a list of admissible values for the 'backend' argument of EazyAgents constructors or a list of all available backends if agent is None. View Source def get_backends ( agent : Optional [ Type[EasyAgent ] ] = None ) : \"\"\"returns a list of all registered backends containing an implementation for the EasyAgent type agent. Args: agent: type deriving from EasyAgent for which the backend identifiers are returned. Returns: a list of admissible values for the 'backend' argument of EazyAgents constructors or a list of all available backends if agent is None. \"\"\" result = [ b.backend_name for b in _backends ] if agent : result = [ b.backend_name for b in _backends if agent in b.get_algorithms() ] return result load def load ( directory : str , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None ) Loads an agent from directory. After a successful load play() may be called directly. The agent, model, backend, seed and play policy are restored according to the previously saved agent. Args: directory: the directory containing the previously saved policy. callbacks: list of callbacks called during save (eg log.Agent) Result: a new instance of EasyAgents View Source def load ( directory : str , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None ): \"\"\"Loads an agent from directory. After a successful load play() may be called directly. The agent, model, backend, seed and play policy are restored according to the previously saved agent. Args: directory: the directory containing the previously saved policy. callbacks: list of callbacks called during save (eg log.Agent) Result: a new instance of EasyAgents \"\"\" assert directory agent_json_path = os . path . join ( directory , EasyAgent . _KEY_EASYAGENT_FILENAME ) policy_directory = os . path . join ( directory , EasyAgent . _KEY_POLICY_DIRECTORY ) assert os . path . isdir ( directory ), f 'directory \"{directory}\" not found.' assert os . path . isfile ( agent_json_path ), 'file \"{agent_json_path}\" not found.' assert os . path . isdir ( policy_directory ), f 'directory \"{policy_directory}\" not found.' with open ( agent_json_path ) as jsonfile : agent_dict = json . load ( jsonfile ) result = EasyAgent . _from_dict ( agent_dict ) callbacks = result . _to_callback_list ( callbacks = callbacks ) result . _backend_agent . load ( directory = policy_directory , callbacks = callbacks ) return result register_backend def register_backend ( backend : easyagents . backends . core . BackendAgentFactory ) registers a backend as a factory for agent implementations. If another backend with the same name is already registered, the old backend is replaced by backend. View Source def register_backend ( backend : bcore . BackendAgentFactory ): \"\"\"registers a backend as a factory for agent implementations. If another backend with the same name is already registered, the old backend is replaced by backend. \"\"\" assert backend old_backends = [ b for b in _backends if b . backend_name == backend . backend_name ] for old_backend in old_backends : _backends . remove ( old_backend ) _backends . append ( backend ) Classes CemAgent class CemAgent ( gym_env_name : str , fc_layers : Union [ Tuple [ int , ... ], NoneType ] = None , backend : str = None ) creates a new agent based on the cross-entropy-method algorithm. From https://learning.mpi-sws.org/mlss2016/slides/2016-MLSS-RL.pdf: Initialize \u00b5 \u2208Rd,\u03c3 \u2208Rd for iteration = 1,2,... num_iterations do Collect num_episodes_per_iteration samples of \u03b8i \u223c N(\u00b5,diag(\u03c3)) Perform a noisy evaluation Ri \u223c \u03b8i Select the top elite_set_fraction of samples (e.g. p = 0.2), which we\u2019ll call the elite set Fit a Gaussian distribution, with diagonal covariance, to the elite set, obtaining a new \u00b5,\u03c3. end for Return the \ufb01nal \u00b5. see https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.6579&rep=rep1&type=pdf View Source class CemAgent ( EasyAgent ): \"\"\"creates a new agent based on the cross-entropy-method algorithm. From https://learning.mpi-sws.org/mlss2016/slides/2016-MLSS-RL.pdf: Initialize \u00b5 \u2208Rd,\u03c3 \u2208Rd for iteration = 1,2,... num_iterations do Collect num_episodes_per_iteration samples of \u03b8i \u223c N(\u00b5,diag(\u03c3)) Perform a noisy evaluation Ri \u223c \u03b8i Select the top elite_set_fraction of samples (e.g. p = 0.2), which we\u2019ll call the elite set Fit a Gaussian distribution, with diagonal covariance, to the elite set, obtaining a new \u00b5,\u03c3. end for Return the \ufb01nal \u00b5. see https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.6579&rep=rep1&type=pdf \"\"\" def __init__ ( self , gym_env_name: str , fc_layers: Optional [ Tuple [ int , ...]] = None , backend: str = None ): super (). __init__ ( gym_env_name , fc_layers , backend ) assert False , \"CemAgent is currently not available (pending migration of keras-rl to tf2.0)\" def train ( self , callbacks: Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations: int = 100 , num_episodes_per_iteration: int = 50 , max_steps_per_episode: int = 500 , elite_set_fraction: float = 0.1 , num_iterations_between_eval: int = 5 , num_episodes_per_eval: int = 10 , train_context: core . CemTrainContext = None , default_plots: bool = None ): \"\"\"Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) num_episodes_per_iteration: number of episodes played in each iteration. for each episode a new policy is sampled from the current weight distribution. max_steps_per_episode: maximum number of steps per episode elite_set_fraction: the fraction of policies which are members of the elite set. These policies are used to fit a new weight distribution in each iteration. num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None: train_context = core . CemTrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . elite_set_fraction = elite_set_fraction train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval super (). train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context Ancestors (in MRO) easyagents.agents.EasyAgent abc.ABC Methods evaluate def evaluate ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: extensible score metrics View Source def evaluate ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) : \"\"\"Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: extensible score metrics \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) Metrics = namedtuple ( 'Metrics' , 'steps rewards' ) Rewards = namedtuple ( 'Rewards' , 'mean std min max all' ) all_rewards = list ( play_context . sum_of_rewards . values ()) mean_reward , std_reward , min_reward , max_reward = statistics . mean ( all_rewards ), statistics . stdev ( all_rewards ), min ( all_rewards ), max ( all_rewards ) rewards = Rewards ( mean = mean_reward , std = std_reward , min = min_reward , max = max_reward , all = all_rewards ) Steps = namedtuple ( 'Steps' , 'mean std min max all' ) all_num_steps = [] for i in play_context . rewards . keys () : all_num_steps . append ( len ( play_context . rewards [ i ] )) mean_steps , std_steps , min_steps , max_steps = statistics . mean ( all_num_steps ), statistics . stdev ( all_num_steps ), min ( all_num_steps ), max ( all_num_steps ) steps = Steps ( mean = mean_steps , std = std_steps , min = min_steps , max = max_steps , all = all_num_steps ) metrics = Metrics ( rewards = rewards , steps = steps ) return metrics play def play ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : easyagents . core . PlayContext = None , default_plots : bool = None ) Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training View Source def play ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : core . PlayContext = None , default_plots : bool = None ): \"\"\"Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training \"\"\" assert self . _backend_agent . _agent_context . _is_policy_trained , \"No trained policy available. Call train() first.\" if play_context is None : play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes callbacks = self . _to_callback_list ( callbacks = callbacks ) callbacks = self . _add_plot_callbacks ( callbacks , default_plots , [ plot . Steps (), plot . Rewards ()]) self . _backend_agent . play ( play_context = play_context , callbacks = callbacks ) return play_context save def save ( self , directory : Union [ str , NoneType ] = None , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None ) -> str Saves the currently trained actor policy in directory. If save is called before a trained policy is created, eg by calling train, an exception is raised. Args: directory: the directory to save the policy weights to. if the directory does not exist yet, a new directory is created. if None the policy is saved in a temp directory. callbacks: list of callbacks called during save (eg log.Agent) Returns: the absolute path to the directory containing the saved policy. View Source def save ( self , directory : Optional [ str ] = None , callbacks : Union [ List[core.AgentCallback ] , core . AgentCallback , None ] = None ) -> str : \"\"\"Saves the currently trained actor policy in directory. If save is called before a trained policy is created, eg by calling train, an exception is raised. Args: directory: the directory to save the policy weights to. if the directory does not exist yet, a new directory is created. if None the policy is saved in a temp directory. callbacks: list of callbacks called during save (eg log.Agent) Returns: the absolute path to the directory containing the saved policy. \"\"\" if directory is None : directory = bcore . _get_temp_path () assert directory assert self . _backend_agent . _agent_context . _is_policy_trained , \"No trained policy available. Call train() first.\" directory = bcore . _mkdir ( directory ) agent_json_path = os . path . join ( directory , EasyAgent . _KEY_EASYAGENT_FILENAME ) with open ( agent_json_path , 'w' ) as jsonfile : agent_dict = self . _to_dict () json . dump ( agent_dict , jsonfile , sort_keys = True , indent = 2 ) callbacks = self . _to_callback_list ( callbacks = callbacks ) policy_directory = os . path . join ( directory , EasyAgent . _KEY_POLICY_DIRECTORY ) policy_directory = bcore . _mkdir ( policy_directory ) self . _backend_agent . save ( directory = policy_directory , callbacks = callbacks ) return directory train def train ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_iterations : int = 100 , num_episodes_per_iteration : int = 50 , max_steps_per_episode : int = 500 , elite_set_fraction : float = 0.1 , num_iterations_between_eval : int = 5 , num_episodes_per_eval : int = 10 , train_context : easyagents . core . CemTrainContext = None , default_plots : bool = None ) Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) num_episodes_per_iteration: number of episodes played in each iteration. for each episode a new policy is sampled from the current weight distribution. max_steps_per_episode: maximum number of steps per episode elite_set_fraction: the fraction of policies which are members of the elite set. These policies are used to fit a new weight distribution in each iteration. num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training View Source def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 100 , num_episodes_per_iteration : int = 50 , max_steps_per_episode : int = 500 , elite_set_fraction : float = 0 . 1 , num_iterations_between_eval : int = 5 , num_episodes_per_eval : int = 10 , train_context : core . CemTrainContext = None , default_plots : bool = None ): \"\"\"Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) num_episodes_per_iteration: number of episodes played in each iteration. for each episode a new policy is sampled from the current weight distribution. max_steps_per_episode: maximum number of steps per episode elite_set_fraction: the fraction of policies which are members of the elite set. These policies are used to fit a new weight distribution in each iteration. num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . CemTrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . elite_set_fraction = elite_set_fraction train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval super (). train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context DoubleDqnAgent class DoubleDqnAgent ( gym_env_name : str , fc_layers : Union [ Tuple [ int , ... ], int , NoneType ] = None , backend : str = None ) Agent based on the Double Dqn algorithm (https://arxiv.org/abs/1509.06461) View Source class DoubleDqnAgent ( DqnAgent ): \"\"\"Agent based on the Double Dqn algorithm (https://arxiv.org/abs/1509.06461)\"\"\" Ancestors (in MRO) easyagents.agents.DqnAgent easyagents.agents.EasyAgent abc.ABC Methods evaluate def evaluate ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: extensible score metrics View Source def evaluate ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) : \"\"\"Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: extensible score metrics \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) Metrics = namedtuple ( 'Metrics' , 'steps rewards' ) Rewards = namedtuple ( 'Rewards' , 'mean std min max all' ) all_rewards = list ( play_context . sum_of_rewards . values ()) mean_reward , std_reward , min_reward , max_reward = statistics . mean ( all_rewards ), statistics . stdev ( all_rewards ), min ( all_rewards ), max ( all_rewards ) rewards = Rewards ( mean = mean_reward , std = std_reward , min = min_reward , max = max_reward , all = all_rewards ) Steps = namedtuple ( 'Steps' , 'mean std min max all' ) all_num_steps = [] for i in play_context . rewards . keys () : all_num_steps . append ( len ( play_context . rewards [ i ] )) mean_steps , std_steps , min_steps , max_steps = statistics . mean ( all_num_steps ), statistics . stdev ( all_num_steps ), min ( all_num_steps ), max ( all_num_steps ) steps = Steps ( mean = mean_steps , std = std_steps , min = min_steps , max = max_steps , all = all_num_steps ) metrics = Metrics ( rewards = rewards , steps = steps ) return metrics play def play ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : easyagents . core . PlayContext = None , default_plots : bool = None ) Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training View Source def play ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : core . PlayContext = None , default_plots : bool = None ): \"\"\"Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training \"\"\" assert self . _backend_agent . _agent_context . _is_policy_trained , \"No trained policy available. Call train() first.\" if play_context is None : play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes callbacks = self . _to_callback_list ( callbacks = callbacks ) callbacks = self . _add_plot_callbacks ( callbacks , default_plots , [ plot . Steps (), plot . Rewards ()]) self . _backend_agent . play ( play_context = play_context , callbacks = callbacks ) return play_context save def save ( self , directory : Union [ str , NoneType ] = None , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None ) -> str Saves the currently trained actor policy in directory. If save is called before a trained policy is created, eg by calling train, an exception is raised. Args: directory: the directory to save the policy weights to. if the directory does not exist yet, a new directory is created. if None the policy is saved in a temp directory. callbacks: list of callbacks called during save (eg log.Agent) Returns: the absolute path to the directory containing the saved policy. View Source def save ( self , directory : Optional [ str ] = None , callbacks : Union [ List[core.AgentCallback ] , core . AgentCallback , None ] = None ) -> str : \"\"\"Saves the currently trained actor policy in directory. If save is called before a trained policy is created, eg by calling train, an exception is raised. Args: directory: the directory to save the policy weights to. if the directory does not exist yet, a new directory is created. if None the policy is saved in a temp directory. callbacks: list of callbacks called during save (eg log.Agent) Returns: the absolute path to the directory containing the saved policy. \"\"\" if directory is None : directory = bcore . _get_temp_path () assert directory assert self . _backend_agent . _agent_context . _is_policy_trained , \"No trained policy available. Call train() first.\" directory = bcore . _mkdir ( directory ) agent_json_path = os . path . join ( directory , EasyAgent . _KEY_EASYAGENT_FILENAME ) with open ( agent_json_path , 'w' ) as jsonfile : agent_dict = self . _to_dict () json . dump ( agent_dict , jsonfile , sort_keys = True , indent = 2 ) callbacks = self . _to_callback_list ( callbacks = callbacks ) policy_directory = os . path . join ( directory , EasyAgent . _KEY_POLICY_DIRECTORY ) policy_directory = bcore . _mkdir ( policy_directory ) self . _backend_agent . save ( directory = policy_directory , callbacks = callbacks ) return directory train def train ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_iterations : int = 20000 , max_steps_per_episode : int = 500 , num_steps_per_iteration : int = 1 , num_steps_buffer_preload = 1000 , num_steps_sampled_from_buffer = 64 , num_iterations_between_eval : int = 1000 , num_episodes_per_eval : int = 10 , learning_rate : float = 0.001 , train_context : easyagents . core . StepsTrainContext = None , default_plots : bool = None ) Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) max_steps_per_episode: maximum number of steps per episode num_steps_per_iteration: number of steps played per training iteration num_steps_buffer_preload: number of initial collect steps to preload the buffer num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training View Source def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 20000 , max_steps_per_episode : int = 500 , num_steps_per_iteration : int = 1 , num_steps_buffer_preload = 1000 , num_steps_sampled_from_buffer = 64 , num_iterations_between_eval : int = 1000 , num_episodes_per_eval : int = 10 , learning_rate : float = 0 . 001 , train_context : core . StepsTrainContext = None , default_plots : bool = None ): \"\"\"Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) max_steps_per_episode: maximum number of steps per episode num_steps_per_iteration: number of steps played per training iteration num_steps_buffer_preload: number of initial collect steps to preload the buffer num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . StepsTrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . num_steps_per_iteration = num_steps_per_iteration train_context . num_steps_buffer_preload = num_steps_buffer_preload train_context . num_steps_sampled_from_buffer = num_steps_sampled_from_buffer train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super (). train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context DqnAgent class DqnAgent ( gym_env_name : str , fc_layers : Union [ Tuple [ int , ... ], int , NoneType ] = None , backend : str = None ) creates a new agent based on the Dqn algorithm. From wikipedia: The DeepMind system used a deep convolutional neural network, with layers of tiled convolutional filters to mimic the effects of receptive fields. Reinforcement learning is unstable or divergent when a nonlinear function approximator such as a neural network is used to represent Q. This instability comes from the correlations present in the sequence of observations, the fact that small updates to Q may significantly change the policy and the data distribution, and the correlations between Q and the target values. The technique used experience replay, a biologically inspired mechanism that uses a random sample of prior actions instead of the most recent action to proceed.[2] This removes correlations in the observation sequence and smooths changes in the data distribution. Iterative update adjusts Q towards target values that are only periodically updated, further reducing correlations with the target.[17] see also: https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning View Source class DqnAgent ( EasyAgent ): \"\"\"creates a new agent based on the Dqn algorithm. From wikipedia: The DeepMind system used a deep convolutional neural network, with layers of tiled convolutional filters to mimic the effects of receptive fields. Reinforcement learning is unstable or divergent when a nonlinear function approximator such as a neural network is used to represent Q. This instability comes from the correlations present in the sequence of observations, the fact that small updates to Q may significantly change the policy and the data distribution, and the correlations between Q and the target values. The technique used experience replay, a biologically inspired mechanism that uses a random sample of prior actions instead of the most recent action to proceed.[2] This removes correlations in the observation sequence and smooths changes in the data distribution. Iterative update adjusts Q towards target values that are only periodically updated, further reducing correlations with the target.[17] see also: https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning \"\"\" def train ( self , callbacks: Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations: int = 20000 , max_steps_per_episode: int = 500 , num_steps_per_iteration: int = 1 , num_steps_buffer_preload = 1000 , num_steps_sampled_from_buffer = 64 , num_iterations_between_eval: int = 1000 , num_episodes_per_eval: int = 10 , learning_rate: float = 0.001 , train_context: core . StepsTrainContext = None , default_plots: bool = None ): \"\"\"Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) max_steps_per_episode: maximum number of steps per episode num_steps_per_iteration: number of steps played per training iteration num_steps_buffer_preload: number of initial collect steps to preload the buffer num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None: train_context = core . StepsTrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . num_steps_per_iteration = num_steps_per_iteration train_context . num_steps_buffer_preload = num_steps_buffer_preload train_context . num_steps_sampled_from_buffer = num_steps_sampled_from_buffer train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super (). train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context Ancestors (in MRO) easyagents.agents.EasyAgent abc.ABC Descendants easyagents.agents.DoubleDqnAgent easyagents.agents.DuelingDqnAgent easyagents.agents.SacAgent Methods evaluate def evaluate ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: extensible score metrics View Source def evaluate ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) : \"\"\"Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: extensible score metrics \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) Metrics = namedtuple ( 'Metrics' , 'steps rewards' ) Rewards = namedtuple ( 'Rewards' , 'mean std min max all' ) all_rewards = list ( play_context . sum_of_rewards . values ()) mean_reward , std_reward , min_reward , max_reward = statistics . mean ( all_rewards ), statistics . stdev ( all_rewards ), min ( all_rewards ), max ( all_rewards ) rewards = Rewards ( mean = mean_reward , std = std_reward , min = min_reward , max = max_reward , all = all_rewards ) Steps = namedtuple ( 'Steps' , 'mean std min max all' ) all_num_steps = [] for i in play_context . rewards . keys () : all_num_steps . append ( len ( play_context . rewards [ i ] )) mean_steps , std_steps , min_steps , max_steps = statistics . mean ( all_num_steps ), statistics . stdev ( all_num_steps ), min ( all_num_steps ), max ( all_num_steps ) steps = Steps ( mean = mean_steps , std = std_steps , min = min_steps , max = max_steps , all = all_num_steps ) metrics = Metrics ( rewards = rewards , steps = steps ) return metrics play def play ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : easyagents . core . PlayContext = None , default_plots : bool = None ) Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training View Source def play ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : core . PlayContext = None , default_plots : bool = None ): \"\"\"Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training \"\"\" assert self . _backend_agent . _agent_context . _is_policy_trained , \"No trained policy available. Call train() first.\" if play_context is None : play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes callbacks = self . _to_callback_list ( callbacks = callbacks ) callbacks = self . _add_plot_callbacks ( callbacks , default_plots , [ plot . Steps (), plot . Rewards ()]) self . _backend_agent . play ( play_context = play_context , callbacks = callbacks ) return play_context save def save ( self , directory : Union [ str , NoneType ] = None , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None ) -> str Saves the currently trained actor policy in directory. If save is called before a trained policy is created, eg by calling train, an exception is raised. Args: directory: the directory to save the policy weights to. if the directory does not exist yet, a new directory is created. if None the policy is saved in a temp directory. callbacks: list of callbacks called during save (eg log.Agent) Returns: the absolute path to the directory containing the saved policy. View Source def save ( self , directory : Optional [ str ] = None , callbacks : Union [ List[core.AgentCallback ] , core . AgentCallback , None ] = None ) -> str : \"\"\"Saves the currently trained actor policy in directory. If save is called before a trained policy is created, eg by calling train, an exception is raised. Args: directory: the directory to save the policy weights to. if the directory does not exist yet, a new directory is created. if None the policy is saved in a temp directory. callbacks: list of callbacks called during save (eg log.Agent) Returns: the absolute path to the directory containing the saved policy. \"\"\" if directory is None : directory = bcore . _get_temp_path () assert directory assert self . _backend_agent . _agent_context . _is_policy_trained , \"No trained policy available. Call train() first.\" directory = bcore . _mkdir ( directory ) agent_json_path = os . path . join ( directory , EasyAgent . _KEY_EASYAGENT_FILENAME ) with open ( agent_json_path , 'w' ) as jsonfile : agent_dict = self . _to_dict () json . dump ( agent_dict , jsonfile , sort_keys = True , indent = 2 ) callbacks = self . _to_callback_list ( callbacks = callbacks ) policy_directory = os . path . join ( directory , EasyAgent . _KEY_POLICY_DIRECTORY ) policy_directory = bcore . _mkdir ( policy_directory ) self . _backend_agent . save ( directory = policy_directory , callbacks = callbacks ) return directory train def train ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_iterations : int = 20000 , max_steps_per_episode : int = 500 , num_steps_per_iteration : int = 1 , num_steps_buffer_preload = 1000 , num_steps_sampled_from_buffer = 64 , num_iterations_between_eval : int = 1000 , num_episodes_per_eval : int = 10 , learning_rate : float = 0.001 , train_context : easyagents . core . StepsTrainContext = None , default_plots : bool = None ) Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) max_steps_per_episode: maximum number of steps per episode num_steps_per_iteration: number of steps played per training iteration num_steps_buffer_preload: number of initial collect steps to preload the buffer num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training View Source def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 20000 , max_steps_per_episode : int = 500 , num_steps_per_iteration : int = 1 , num_steps_buffer_preload = 1000 , num_steps_sampled_from_buffer = 64 , num_iterations_between_eval : int = 1000 , num_episodes_per_eval : int = 10 , learning_rate : float = 0 . 001 , train_context : core . StepsTrainContext = None , default_plots : bool = None ): \"\"\"Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) max_steps_per_episode: maximum number of steps per episode num_steps_per_iteration: number of steps played per training iteration num_steps_buffer_preload: number of initial collect steps to preload the buffer num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . StepsTrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . num_steps_per_iteration = num_steps_per_iteration train_context . num_steps_buffer_preload = num_steps_buffer_preload train_context . num_steps_sampled_from_buffer = num_steps_sampled_from_buffer train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super (). train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context DuelingDqnAgent class DuelingDqnAgent ( gym_env_name : str , fc_layers : Union [ Tuple [ int , ... ], int , NoneType ] = None , backend : str = None ) Agent based on the Dueling Dqn algorithm (https://arxiv.org/abs/1511.06581). View Source class DuelingDqnAgent ( DqnAgent ): \"\"\"Agent based on the Dueling Dqn algorithm (https://arxiv.org/abs/1511.06581).\"\"\" Ancestors (in MRO) easyagents.agents.DqnAgent easyagents.agents.EasyAgent abc.ABC Methods evaluate def evaluate ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: extensible score metrics View Source def evaluate ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) : \"\"\"Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: extensible score metrics \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) Metrics = namedtuple ( 'Metrics' , 'steps rewards' ) Rewards = namedtuple ( 'Rewards' , 'mean std min max all' ) all_rewards = list ( play_context . sum_of_rewards . values ()) mean_reward , std_reward , min_reward , max_reward = statistics . mean ( all_rewards ), statistics . stdev ( all_rewards ), min ( all_rewards ), max ( all_rewards ) rewards = Rewards ( mean = mean_reward , std = std_reward , min = min_reward , max = max_reward , all = all_rewards ) Steps = namedtuple ( 'Steps' , 'mean std min max all' ) all_num_steps = [] for i in play_context . rewards . keys () : all_num_steps . append ( len ( play_context . rewards [ i ] )) mean_steps , std_steps , min_steps , max_steps = statistics . mean ( all_num_steps ), statistics . stdev ( all_num_steps ), min ( all_num_steps ), max ( all_num_steps ) steps = Steps ( mean = mean_steps , std = std_steps , min = min_steps , max = max_steps , all = all_num_steps ) metrics = Metrics ( rewards = rewards , steps = steps ) return metrics play def play ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : easyagents . core . PlayContext = None , default_plots : bool = None ) Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training View Source def play ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : core . PlayContext = None , default_plots : bool = None ): \"\"\"Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training \"\"\" assert self . _backend_agent . _agent_context . _is_policy_trained , \"No trained policy available. Call train() first.\" if play_context is None : play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes callbacks = self . _to_callback_list ( callbacks = callbacks ) callbacks = self . _add_plot_callbacks ( callbacks , default_plots , [ plot . Steps (), plot . Rewards ()]) self . _backend_agent . play ( play_context = play_context , callbacks = callbacks ) return play_context save def save ( self , directory : Union [ str , NoneType ] = None , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None ) -> str Saves the currently trained actor policy in directory. If save is called before a trained policy is created, eg by calling train, an exception is raised. Args: directory: the directory to save the policy weights to. if the directory does not exist yet, a new directory is created. if None the policy is saved in a temp directory. callbacks: list of callbacks called during save (eg log.Agent) Returns: the absolute path to the directory containing the saved policy. View Source def save ( self , directory : Optional [ str ] = None , callbacks : Union [ List[core.AgentCallback ] , core . AgentCallback , None ] = None ) -> str : \"\"\"Saves the currently trained actor policy in directory. If save is called before a trained policy is created, eg by calling train, an exception is raised. Args: directory: the directory to save the policy weights to. if the directory does not exist yet, a new directory is created. if None the policy is saved in a temp directory. callbacks: list of callbacks called during save (eg log.Agent) Returns: the absolute path to the directory containing the saved policy. \"\"\" if directory is None : directory = bcore . _get_temp_path () assert directory assert self . _backend_agent . _agent_context . _is_policy_trained , \"No trained policy available. Call train() first.\" directory = bcore . _mkdir ( directory ) agent_json_path = os . path . join ( directory , EasyAgent . _KEY_EASYAGENT_FILENAME ) with open ( agent_json_path , 'w' ) as jsonfile : agent_dict = self . _to_dict () json . dump ( agent_dict , jsonfile , sort_keys = True , indent = 2 ) callbacks = self . _to_callback_list ( callbacks = callbacks ) policy_directory = os . path . join ( directory , EasyAgent . _KEY_POLICY_DIRECTORY ) policy_directory = bcore . _mkdir ( policy_directory ) self . _backend_agent . save ( directory = policy_directory , callbacks = callbacks ) return directory train def train ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_iterations : int = 20000 , max_steps_per_episode : int = 500 , num_steps_per_iteration : int = 1 , num_steps_buffer_preload = 1000 , num_steps_sampled_from_buffer = 64 , num_iterations_between_eval : int = 1000 , num_episodes_per_eval : int = 10 , learning_rate : float = 0.001 , train_context : easyagents . core . StepsTrainContext = None , default_plots : bool = None ) Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) max_steps_per_episode: maximum number of steps per episode num_steps_per_iteration: number of steps played per training iteration num_steps_buffer_preload: number of initial collect steps to preload the buffer num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training View Source def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 20000 , max_steps_per_episode : int = 500 , num_steps_per_iteration : int = 1 , num_steps_buffer_preload = 1000 , num_steps_sampled_from_buffer = 64 , num_iterations_between_eval : int = 1000 , num_episodes_per_eval : int = 10 , learning_rate : float = 0 . 001 , train_context : core . StepsTrainContext = None , default_plots : bool = None ): \"\"\"Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) max_steps_per_episode: maximum number of steps per episode num_steps_per_iteration: number of steps played per training iteration num_steps_buffer_preload: number of initial collect steps to preload the buffer num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . StepsTrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . num_steps_per_iteration = num_steps_per_iteration train_context . num_steps_buffer_preload = num_steps_buffer_preload train_context . num_steps_sampled_from_buffer = num_steps_sampled_from_buffer train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super (). train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context EasyAgent class EasyAgent ( gym_env_name : str , fc_layers : Union [ Tuple [ int , ... ], int , NoneType ] = None , backend : str = None ) Abstract base class for all easy reinforcment learning agents. Besides forwarding train and play it implements persistence. View Source class EasyAgent ( ABC ) : \"\"\"Abstract base class for all easy reinforcment learning agents. Besides forwarding train and play it implements persistence.\"\"\" _KEY_BACKEND = 'backend' _KEY_EASYAGENT_CLASS = 'easyagent_class' _KEY_EASYAGENT_FILENAME = 'easyagent.json' _KEY_MODEL_CONFIG = 'model_config' _KEY_POLICY_DIRECTORY = 'policy' _KEY_VERSION = 'version' def __init__ ( self , gym_env_name : str , fc_layers : Union [ Tuple[int, ... ] , int , None ] = None , backend : str = None ) : \"\"\" Args: gym_env_name: name of an OpenAI gym environment to be used for training and evaluation fc_layers: defines the neural network to be used, a sequence of fully connected layers of the given size. Eg (75,40) yields a neural network consisting out of 2 hidden layers, the first one containing 75 and the second layer containing 40 neurons. backend=the backend to be used (eg 'tfagents'), if None a default implementation is used. call get_backends() to get a list of the available backends. \"\"\" model_config = core . ModelConfig ( gym_env_name = gym_env_name , fc_layers = fc_layers , seed = seed ) self . _initialize ( model_config = model_config , backend_name = backend ) return def _initialize ( self , model_config : core . ModelConfig , backend_name : str = None ) : if backend_name is None : backend_name = easyagents . backends . default . DefaultAgentFactory . backend_name backend : bcore . BackendAgentFactory = _get_backend ( backend_name ) assert model_config is not None , \"model_config not set.\" assert backend , f 'Backend \"{backend_name}\" not found. The registered backends are {get_backends()}.' self . _model_config : core . ModelConfig = model_config backend_agent = backend . create_agent ( easyagent_type = type ( self ), model_config = model_config ) assert backend_agent , f 'Backend \"{backend_name}\" does not implement \"{type(self).__name__}\". ' + \\ f 'Choose one of the following backend {get_backends(type(self))}.' self . _backend_agent : Optional [ bcore._BackendAgent ] = backend_agent self . _backend_name : str = backend_name self . _backend_agent . _agent_context . _agent_saver = self . save return def _add_plot_callbacks ( self , callbacks : List [ core.AgentCallback ] , default_plots : Optional [ bool ] , default_plot_callbacks : List [ plot._PlotCallback ] ) -> List [ core.AgentCallback ] : \"\"\"Adds the default callbacks and sorts all callbacks in the order _PreProcessCallbacks, AgentCallbacks, _PostProcessCallbacks. Args: callbacks: existing callbacks to prepare default_plots: if set or if None and callbacks does not contain plots then the default plots are added default_plot_callbacks: plot callbacks to add. \"\"\" pre_process : List [ core.AgentCallback ] = [ plot._PreProcess() ] agent : List [ core.AgentCallback ] = [] post_process : List [ core.AgentCallback ] = [ plot._PostProcess() ] if default_plots is None : default_plots = True for c in callbacks : default_plots = default_plots and ( not isinstance ( c , plot . _PlotCallback )) if default_plots : agent = default_plot_callbacks for c in callbacks : if isinstance ( c , core . _PreProcessCallback ) : pre_process . append ( c ) else : if isinstance ( c , core . _PostProcessCallback ) : post_process . append ( c ) else : agent . append ( c ) result : List [ core.AgentCallback ] = pre_process + agent + post_process return result @staticmethod def _from_dict ( param_dict : Dict [ str, object ] ) : \"\"\"recreates a new agent instance according to the definition previously created by _to_dict. Returns: new agent instance (excluding any trained policy), the agent type is preserved. \"\"\" assert param_dict mc : core . ModelConfig = core . ModelConfig . _from_dict ( param_dict [ EasyAgent._KEY_MODEL_CONFIG ] ) agent_class = globals () [ param_dict[EasyAgent._KEY_EASYAGENT_CLASS ] ] backend : str = param_dict [ EasyAgent._KEY_BACKEND ] result = agent_class ( gym_env_name = mc . original_env_name , backend = backend ) result . _initialize ( model_config = mc , backend_name = backend ) return result def _to_callback_list ( self , callbacks : Union [ Optional[core.AgentCallback ] , List [ core.AgentCallback ] ] ) -> List [ core.AgentCallback ] : \"\"\"maps callbacks to an admissible callback list. if callbacks is None an empty list is returned. if callbacks is an AgentCallback a list containing only this callback is returned otherwise callbacks is returned \"\"\" result : List [ core.AgentCallback ] = [] if not callbacks is None : if isinstance ( callbacks , core . AgentCallback ) : result = [ callbacks ] else : assert isinstance ( callbacks , list ), \"callback not an AgentCallback or a list thereof.\" result = callbacks return result def _to_dict ( self ) -> Dict [ str, object ] : \"\"\"saves the agent definition to a dict. Returns: dict containing all parameters to recreate the agent (excluding a trained policy) \"\"\" result : Dict [ str, object ] = dict () result [ EasyAgent._KEY_VERSION ] = easyagents . __version__ result [ EasyAgent._KEY_EASYAGENT_CLASS ] = self . __class__ . __name__ result [ EasyAgent._KEY_BACKEND ] = self . _backend_name result [ EasyAgent._KEY_MODEL_CONFIG ] = self . _model_config . _to_dict () result [ EasyAgent._KEY_POLICY_DIRECTORY ] = 'policy' return result def evaluate ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) : \"\"\"Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: extensible score metrics \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) Metrics = namedtuple ( 'Metrics' , 'steps rewards' ) Rewards = namedtuple ( 'Rewards' , 'mean std min max all' ) all_rewards = list ( play_context . sum_of_rewards . values ()) mean_reward , std_reward , min_reward , max_reward = statistics . mean ( all_rewards ), statistics . stdev ( all_rewards ), min ( all_rewards ), max ( all_rewards ) rewards = Rewards ( mean = mean_reward , std = std_reward , min = min_reward , max = max_reward , all = all_rewards ) Steps = namedtuple ( 'Steps' , 'mean std min max all' ) all_num_steps = [] for i in play_context . rewards . keys () : all_num_steps . append ( len ( play_context . rewards [ i ] )) mean_steps , std_steps , min_steps , max_steps = statistics . mean ( all_num_steps ), statistics . stdev ( all_num_steps ), min ( all_num_steps ), max ( all_num_steps ) steps = Steps ( mean = mean_steps , std = std_steps , min = min_steps , max = max_steps , all = all_num_steps ) metrics = Metrics ( rewards = rewards , steps = steps ) return metrics def play ( self , callbacks : Union [ List[core.AgentCallback ] , core . AgentCallback , None ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : core . PlayContext = None , default_plots : bool = None ) : \"\"\"Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training \"\"\" assert self . _backend_agent . _agent_context . _is_policy_trained , \"No trained policy available. Call train() first.\" if play_context is None : play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes callbacks = self . _to_callback_list ( callbacks = callbacks ) callbacks = self . _add_plot_callbacks ( callbacks , default_plots , [ plot.Steps(), plot.Rewards() ] ) self . _backend_agent . play ( play_context = play_context , callbacks = callbacks ) return play_context def save ( self , directory : Optional [ str ] = None , callbacks : Union [ List[core.AgentCallback ] , core . AgentCallback , None ] = None ) -> str : \"\"\"Saves the currently trained actor policy in directory. If save is called before a trained policy is created, eg by calling train, an exception is raised. Args: directory: the directory to save the policy weights to. if the directory does not exist yet, a new directory is created. if None the policy is saved in a temp directory. callbacks: list of callbacks called during save (eg log.Agent) Returns: the absolute path to the directory containing the saved policy. \"\"\" if directory is None : directory = bcore . _get_temp_path () assert directory assert self . _backend_agent . _agent_context . _is_policy_trained , \"No trained policy available. Call train() first.\" directory = bcore . _mkdir ( directory ) agent_json_path = os . path . join ( directory , EasyAgent . _KEY_EASYAGENT_FILENAME ) with open ( agent_json_path , 'w' ) as jsonfile : agent_dict = self . _to_dict () json . dump ( agent_dict , jsonfile , sort_keys = True , indent = 2 ) callbacks = self . _to_callback_list ( callbacks = callbacks ) policy_directory = os . path . join ( directory , EasyAgent . _KEY_POLICY_DIRECTORY ) policy_directory = bcore . _mkdir ( policy_directory ) self . _backend_agent . save ( directory = policy_directory , callbacks = callbacks ) return directory def train ( self , train_context : core . TrainContext , callbacks : Union [ List[core.AgentCallback ] , core . AgentCallback , None ] , default_plots : Optional [ bool ] ) : \"\"\"Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during the training and evaluation train_context: training configuration to be used (num_iterations,num_episodes_per_iteration,...) default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty \"\"\" assert train_context , \"train_context not set.\" callbacks = self . _to_callback_list ( callbacks = callbacks ) callbacks = self . _add_plot_callbacks ( callbacks , default_plots , [ plot.Loss(), plot.Steps(), plot.Rewards() ] ) self . _backend_agent . train ( train_context = train_context , callbacks = callbacks ) Ancestors (in MRO) abc.ABC Descendants easyagents.agents.CemAgent easyagents.agents.DqnAgent easyagents.agents.PpoAgent easyagents.agents.RandomAgent easyagents.agents.ReinforceAgent Methods evaluate def evaluate ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: extensible score metrics View Source def evaluate ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) : \"\"\"Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: extensible score metrics \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) Metrics = namedtuple ( 'Metrics' , 'steps rewards' ) Rewards = namedtuple ( 'Rewards' , 'mean std min max all' ) all_rewards = list ( play_context . sum_of_rewards . values ()) mean_reward , std_reward , min_reward , max_reward = statistics . mean ( all_rewards ), statistics . stdev ( all_rewards ), min ( all_rewards ), max ( all_rewards ) rewards = Rewards ( mean = mean_reward , std = std_reward , min = min_reward , max = max_reward , all = all_rewards ) Steps = namedtuple ( 'Steps' , 'mean std min max all' ) all_num_steps = [] for i in play_context . rewards . keys () : all_num_steps . append ( len ( play_context . rewards [ i ] )) mean_steps , std_steps , min_steps , max_steps = statistics . mean ( all_num_steps ), statistics . stdev ( all_num_steps ), min ( all_num_steps ), max ( all_num_steps ) steps = Steps ( mean = mean_steps , std = std_steps , min = min_steps , max = max_steps , all = all_num_steps ) metrics = Metrics ( rewards = rewards , steps = steps ) return metrics play def play ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : easyagents . core . PlayContext = None , default_plots : bool = None ) Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training View Source def play ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : core . PlayContext = None , default_plots : bool = None ): \"\"\"Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training \"\"\" assert self . _backend_agent . _agent_context . _is_policy_trained , \"No trained policy available. Call train() first.\" if play_context is None : play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes callbacks = self . _to_callback_list ( callbacks = callbacks ) callbacks = self . _add_plot_callbacks ( callbacks , default_plots , [ plot . Steps (), plot . Rewards ()]) self . _backend_agent . play ( play_context = play_context , callbacks = callbacks ) return play_context save def save ( self , directory : Union [ str , NoneType ] = None , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None ) -> str Saves the currently trained actor policy in directory. If save is called before a trained policy is created, eg by calling train, an exception is raised. Args: directory: the directory to save the policy weights to. if the directory does not exist yet, a new directory is created. if None the policy is saved in a temp directory. callbacks: list of callbacks called during save (eg log.Agent) Returns: the absolute path to the directory containing the saved policy. View Source def save ( self , directory : Optional [ str ] = None , callbacks : Union [ List[core.AgentCallback ] , core . AgentCallback , None ] = None ) -> str : \"\"\"Saves the currently trained actor policy in directory. If save is called before a trained policy is created, eg by calling train, an exception is raised. Args: directory: the directory to save the policy weights to. if the directory does not exist yet, a new directory is created. if None the policy is saved in a temp directory. callbacks: list of callbacks called during save (eg log.Agent) Returns: the absolute path to the directory containing the saved policy. \"\"\" if directory is None : directory = bcore . _get_temp_path () assert directory assert self . _backend_agent . _agent_context . _is_policy_trained , \"No trained policy available. Call train() first.\" directory = bcore . _mkdir ( directory ) agent_json_path = os . path . join ( directory , EasyAgent . _KEY_EASYAGENT_FILENAME ) with open ( agent_json_path , 'w' ) as jsonfile : agent_dict = self . _to_dict () json . dump ( agent_dict , jsonfile , sort_keys = True , indent = 2 ) callbacks = self . _to_callback_list ( callbacks = callbacks ) policy_directory = os . path . join ( directory , EasyAgent . _KEY_POLICY_DIRECTORY ) policy_directory = bcore . _mkdir ( policy_directory ) self . _backend_agent . save ( directory = policy_directory , callbacks = callbacks ) return directory train def train ( self , train_context : easyagents . core . TrainContext , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ], default_plots : Union [ bool , NoneType ] ) Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during the training and evaluation train_context: training configuration to be used (num_iterations,num_episodes_per_iteration,...) default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty View Source def train ( self , train_context : core . TrainContext , callbacks : Union [ List[core.AgentCallback ] , core . AgentCallback , None ] , default_plots : Optional [ bool ] ) : \"\"\"Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during the training and evaluation train_context: training configuration to be used (num_iterations,num_episodes_per_iteration,...) default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty \"\"\" assert train_context , \"train_context not set.\" callbacks = self . _to_callback_list ( callbacks = callbacks ) callbacks = self . _add_plot_callbacks ( callbacks , default_plots , [ plot.Loss(), plot.Steps(), plot.Rewards() ] ) self . _backend_agent . train ( train_context = train_context , callbacks = callbacks ) PpoAgent class PpoAgent ( gym_env_name : str , fc_layers : Union [ Tuple [ int , ... ], int , NoneType ] = None , backend : str = None ) creates a new agent based on the PPO algorithm. PPO is an actor-critic algorithm using 2 neural networks. The actor network to predict the next action to be taken and the critic network to estimate the value of the game state we are currently in (the expected, discounted sum of future rewards when following the current actor network). see also: https://spinningup.openai.com/en/latest/algorithms/ppo.html View Source class PpoAgent ( EasyAgent ): \"\"\"creates a new agent based on the PPO algorithm. PPO is an actor-critic algorithm using 2 neural networks. The actor network to predict the next action to be taken and the critic network to estimate the value of the game state we are currently in (the expected, discounted sum of future rewards when following the current actor network). see also: https://spinningup.openai.com/en/latest/algorithms/ppo.html \"\"\" def train ( self , callbacks: Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations: int = 100 , num_episodes_per_iteration: int = 10 , max_steps_per_episode: int = 500 , num_epochs_per_iteration: int = 10 , num_iterations_between_eval: int = 5 , num_episodes_per_eval: int = 10 , learning_rate: float = 0.001 , train_context: core . PpoTrainContext = None , default_plots: bool = None ): \"\"\"Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) num_episodes_per_iteration: number of episodes played per training iteration max_steps_per_episode: maximum number of steps per episode num_epochs_per_iteration: number of times the data collected for the current iteration is used to retrain the current policy num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None: train_context = core . PpoTrainContext () train_context . num_iterations = num_iterations train_context . num_episodes_per_iteration = num_episodes_per_iteration train_context . max_steps_per_episode = max_steps_per_episode train_context . num_epochs_per_iteration = num_epochs_per_iteration train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super (). train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context Ancestors (in MRO) easyagents.agents.EasyAgent abc.ABC Methods evaluate def evaluate ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: extensible score metrics View Source def evaluate ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) : \"\"\"Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: extensible score metrics \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) Metrics = namedtuple ( 'Metrics' , 'steps rewards' ) Rewards = namedtuple ( 'Rewards' , 'mean std min max all' ) all_rewards = list ( play_context . sum_of_rewards . values ()) mean_reward , std_reward , min_reward , max_reward = statistics . mean ( all_rewards ), statistics . stdev ( all_rewards ), min ( all_rewards ), max ( all_rewards ) rewards = Rewards ( mean = mean_reward , std = std_reward , min = min_reward , max = max_reward , all = all_rewards ) Steps = namedtuple ( 'Steps' , 'mean std min max all' ) all_num_steps = [] for i in play_context . rewards . keys () : all_num_steps . append ( len ( play_context . rewards [ i ] )) mean_steps , std_steps , min_steps , max_steps = statistics . mean ( all_num_steps ), statistics . stdev ( all_num_steps ), min ( all_num_steps ), max ( all_num_steps ) steps = Steps ( mean = mean_steps , std = std_steps , min = min_steps , max = max_steps , all = all_num_steps ) metrics = Metrics ( rewards = rewards , steps = steps ) return metrics play def play ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : easyagents . core . PlayContext = None , default_plots : bool = None ) Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training View Source def play ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : core . PlayContext = None , default_plots : bool = None ): \"\"\"Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training \"\"\" assert self . _backend_agent . _agent_context . _is_policy_trained , \"No trained policy available. Call train() first.\" if play_context is None : play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes callbacks = self . _to_callback_list ( callbacks = callbacks ) callbacks = self . _add_plot_callbacks ( callbacks , default_plots , [ plot . Steps (), plot . Rewards ()]) self . _backend_agent . play ( play_context = play_context , callbacks = callbacks ) return play_context save def save ( self , directory : Union [ str , NoneType ] = None , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None ) -> str Saves the currently trained actor policy in directory. If save is called before a trained policy is created, eg by calling train, an exception is raised. Args: directory: the directory to save the policy weights to. if the directory does not exist yet, a new directory is created. if None the policy is saved in a temp directory. callbacks: list of callbacks called during save (eg log.Agent) Returns: the absolute path to the directory containing the saved policy. View Source def save ( self , directory : Optional [ str ] = None , callbacks : Union [ List[core.AgentCallback ] , core . AgentCallback , None ] = None ) -> str : \"\"\"Saves the currently trained actor policy in directory. If save is called before a trained policy is created, eg by calling train, an exception is raised. Args: directory: the directory to save the policy weights to. if the directory does not exist yet, a new directory is created. if None the policy is saved in a temp directory. callbacks: list of callbacks called during save (eg log.Agent) Returns: the absolute path to the directory containing the saved policy. \"\"\" if directory is None : directory = bcore . _get_temp_path () assert directory assert self . _backend_agent . _agent_context . _is_policy_trained , \"No trained policy available. Call train() first.\" directory = bcore . _mkdir ( directory ) agent_json_path = os . path . join ( directory , EasyAgent . _KEY_EASYAGENT_FILENAME ) with open ( agent_json_path , 'w' ) as jsonfile : agent_dict = self . _to_dict () json . dump ( agent_dict , jsonfile , sort_keys = True , indent = 2 ) callbacks = self . _to_callback_list ( callbacks = callbacks ) policy_directory = os . path . join ( directory , EasyAgent . _KEY_POLICY_DIRECTORY ) policy_directory = bcore . _mkdir ( policy_directory ) self . _backend_agent . save ( directory = policy_directory , callbacks = callbacks ) return directory train def train ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_iterations : int = 100 , num_episodes_per_iteration : int = 10 , max_steps_per_episode : int = 500 , num_epochs_per_iteration : int = 10 , num_iterations_between_eval : int = 5 , num_episodes_per_eval : int = 10 , learning_rate : float = 0.001 , train_context : easyagents . core . PpoTrainContext = None , default_plots : bool = None ) Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) num_episodes_per_iteration: number of episodes played per training iteration max_steps_per_episode: maximum number of steps per episode num_epochs_per_iteration: number of times the data collected for the current iteration is used to retrain the current policy num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training View Source def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 100 , num_episodes_per_iteration : int = 10 , max_steps_per_episode : int = 500 , num_epochs_per_iteration : int = 10 , num_iterations_between_eval : int = 5 , num_episodes_per_eval : int = 10 , learning_rate : float = 0 . 001 , train_context : core . PpoTrainContext = None , default_plots : bool = None ): \"\"\"Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) num_episodes_per_iteration: number of episodes played per training iteration max_steps_per_episode: maximum number of steps per episode num_epochs_per_iteration: number of times the data collected for the current iteration is used to retrain the current policy num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . PpoTrainContext () train_context . num_iterations = num_iterations train_context . num_episodes_per_iteration = num_episodes_per_iteration train_context . max_steps_per_episode = max_steps_per_episode train_context . num_epochs_per_iteration = num_epochs_per_iteration train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super (). train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context RandomAgent class RandomAgent ( gym_env_name : str , fc_layers : Union [ Tuple [ int , ... ], int , NoneType ] = None , backend : str = None ) Agent which always chooses uniform random actions. View Source class RandomAgent ( EasyAgent ): \"\"\"Agent which always chooses uniform random actions.\"\"\" def train ( self , callbacks: Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations: int = 10 , max_steps_per_episode: int = 1000 , num_episodes_per_eval: int = 10 , train_context: core . TrainContext = None , default_plots: bool = None ): \"\"\"Evaluates the environment using a uniform random policy. The evaluation is performed in batches of num_episodes_per_eval episodes. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times a batch of num_episodes_per_eval episodes is evaluated. max_steps_per_episode: maximum number of steps per episode num_episodes_per_eval: number of episodes played to estimate the average return and steps train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...) Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None: train_context = core . TrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . num_epochs_per_iteration = 0 train_context . num_iterations_between_eval = 1 train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = 1 super (). train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context Ancestors (in MRO) easyagents.agents.EasyAgent abc.ABC Methods evaluate def evaluate ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: extensible score metrics View Source def evaluate ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) : \"\"\"Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: extensible score metrics \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) Metrics = namedtuple ( 'Metrics' , 'steps rewards' ) Rewards = namedtuple ( 'Rewards' , 'mean std min max all' ) all_rewards = list ( play_context . sum_of_rewards . values ()) mean_reward , std_reward , min_reward , max_reward = statistics . mean ( all_rewards ), statistics . stdev ( all_rewards ), min ( all_rewards ), max ( all_rewards ) rewards = Rewards ( mean = mean_reward , std = std_reward , min = min_reward , max = max_reward , all = all_rewards ) Steps = namedtuple ( 'Steps' , 'mean std min max all' ) all_num_steps = [] for i in play_context . rewards . keys () : all_num_steps . append ( len ( play_context . rewards [ i ] )) mean_steps , std_steps , min_steps , max_steps = statistics . mean ( all_num_steps ), statistics . stdev ( all_num_steps ), min ( all_num_steps ), max ( all_num_steps ) steps = Steps ( mean = mean_steps , std = std_steps , min = min_steps , max = max_steps , all = all_num_steps ) metrics = Metrics ( rewards = rewards , steps = steps ) return metrics play def play ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : easyagents . core . PlayContext = None , default_plots : bool = None ) Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training View Source def play ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : core . PlayContext = None , default_plots : bool = None ): \"\"\"Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training \"\"\" assert self . _backend_agent . _agent_context . _is_policy_trained , \"No trained policy available. Call train() first.\" if play_context is None : play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes callbacks = self . _to_callback_list ( callbacks = callbacks ) callbacks = self . _add_plot_callbacks ( callbacks , default_plots , [ plot . Steps (), plot . Rewards ()]) self . _backend_agent . play ( play_context = play_context , callbacks = callbacks ) return play_context save def save ( self , directory : Union [ str , NoneType ] = None , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None ) -> str Saves the currently trained actor policy in directory. If save is called before a trained policy is created, eg by calling train, an exception is raised. Args: directory: the directory to save the policy weights to. if the directory does not exist yet, a new directory is created. if None the policy is saved in a temp directory. callbacks: list of callbacks called during save (eg log.Agent) Returns: the absolute path to the directory containing the saved policy. View Source def save ( self , directory : Optional [ str ] = None , callbacks : Union [ List[core.AgentCallback ] , core . AgentCallback , None ] = None ) -> str : \"\"\"Saves the currently trained actor policy in directory. If save is called before a trained policy is created, eg by calling train, an exception is raised. Args: directory: the directory to save the policy weights to. if the directory does not exist yet, a new directory is created. if None the policy is saved in a temp directory. callbacks: list of callbacks called during save (eg log.Agent) Returns: the absolute path to the directory containing the saved policy. \"\"\" if directory is None : directory = bcore . _get_temp_path () assert directory assert self . _backend_agent . _agent_context . _is_policy_trained , \"No trained policy available. Call train() first.\" directory = bcore . _mkdir ( directory ) agent_json_path = os . path . join ( directory , EasyAgent . _KEY_EASYAGENT_FILENAME ) with open ( agent_json_path , 'w' ) as jsonfile : agent_dict = self . _to_dict () json . dump ( agent_dict , jsonfile , sort_keys = True , indent = 2 ) callbacks = self . _to_callback_list ( callbacks = callbacks ) policy_directory = os . path . join ( directory , EasyAgent . _KEY_POLICY_DIRECTORY ) policy_directory = bcore . _mkdir ( policy_directory ) self . _backend_agent . save ( directory = policy_directory , callbacks = callbacks ) return directory train def train ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_iterations : int = 10 , max_steps_per_episode : int = 1000 , num_episodes_per_eval : int = 10 , train_context : easyagents . core . TrainContext = None , default_plots : bool = None ) Evaluates the environment using a uniform random policy. The evaluation is performed in batches of num_episodes_per_eval episodes. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times a batch of num_episodes_per_eval episodes is evaluated. max_steps_per_episode: maximum number of steps per episode num_episodes_per_eval: number of episodes played to estimate the average return and steps train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...) Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training View Source def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 10 , max_steps_per_episode : int = 1000 , num_episodes_per_eval : int = 10 , train_context : core . TrainContext = None , default_plots : bool = None ): \"\"\"Evaluates the environment using a uniform random policy. The evaluation is performed in batches of num_episodes_per_eval episodes. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times a batch of num_episodes_per_eval episodes is evaluated. max_steps_per_episode: maximum number of steps per episode num_episodes_per_eval: number of episodes played to estimate the average return and steps train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...) Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . TrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . num_epochs_per_iteration = 0 train_context . num_iterations_between_eval = 1 train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = 1 super (). train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context ReinforceAgent class ReinforceAgent ( gym_env_name : str , fc_layers : Union [ Tuple [ int , ... ], int , NoneType ] = None , backend : str = None ) creates a new agent based on the Reinforce algorithm. Reinforce is a vanilla policy gradient algorithm using a single actor network. see also: www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf View Source class ReinforceAgent ( EasyAgent ): \"\"\"creates a new agent based on the Reinforce algorithm. Reinforce is a vanilla policy gradient algorithm using a single actor network. see also: www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf \"\"\" def train ( self , callbacks: Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations: int = 100 , num_episodes_per_iteration: int = 10 , max_steps_per_episode: int = 500 , num_epochs_per_iteration: int = 10 , num_iterations_between_eval: int = 5 , num_episodes_per_eval: int = 10 , learning_rate: float = 0.001 , train_context: core . EpisodesTrainContext = None , default_plots: bool = None ): \"\"\"Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) num_episodes_per_iteration: number of episodes played per training iteration max_steps_per_episode: maximum number of steps per episode num_epochs_per_iteration: number of times the data collected for the current iteration is used to retrain the current policy num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None: train_context = core . EpisodesTrainContext () train_context . num_iterations = num_iterations train_context . num_episodes_per_iteration = num_episodes_per_iteration train_context . max_steps_per_episode = max_steps_per_episode train_context . num_epochs_per_iteration = num_epochs_per_iteration train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super (). train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context Ancestors (in MRO) easyagents.agents.EasyAgent abc.ABC Methods evaluate def evaluate ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: extensible score metrics View Source def evaluate ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) : \"\"\"Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: extensible score metrics \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) Metrics = namedtuple ( 'Metrics' , 'steps rewards' ) Rewards = namedtuple ( 'Rewards' , 'mean std min max all' ) all_rewards = list ( play_context . sum_of_rewards . values ()) mean_reward , std_reward , min_reward , max_reward = statistics . mean ( all_rewards ), statistics . stdev ( all_rewards ), min ( all_rewards ), max ( all_rewards ) rewards = Rewards ( mean = mean_reward , std = std_reward , min = min_reward , max = max_reward , all = all_rewards ) Steps = namedtuple ( 'Steps' , 'mean std min max all' ) all_num_steps = [] for i in play_context . rewards . keys () : all_num_steps . append ( len ( play_context . rewards [ i ] )) mean_steps , std_steps , min_steps , max_steps = statistics . mean ( all_num_steps ), statistics . stdev ( all_num_steps ), min ( all_num_steps ), max ( all_num_steps ) steps = Steps ( mean = mean_steps , std = std_steps , min = min_steps , max = max_steps , all = all_num_steps ) metrics = Metrics ( rewards = rewards , steps = steps ) return metrics play def play ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : easyagents . core . PlayContext = None , default_plots : bool = None ) Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training View Source def play ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : core . PlayContext = None , default_plots : bool = None ): \"\"\"Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training \"\"\" assert self . _backend_agent . _agent_context . _is_policy_trained , \"No trained policy available. Call train() first.\" if play_context is None : play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes callbacks = self . _to_callback_list ( callbacks = callbacks ) callbacks = self . _add_plot_callbacks ( callbacks , default_plots , [ plot . Steps (), plot . Rewards ()]) self . _backend_agent . play ( play_context = play_context , callbacks = callbacks ) return play_context save def save ( self , directory : Union [ str , NoneType ] = None , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None ) -> str Saves the currently trained actor policy in directory. If save is called before a trained policy is created, eg by calling train, an exception is raised. Args: directory: the directory to save the policy weights to. if the directory does not exist yet, a new directory is created. if None the policy is saved in a temp directory. callbacks: list of callbacks called during save (eg log.Agent) Returns: the absolute path to the directory containing the saved policy. View Source def save ( self , directory : Optional [ str ] = None , callbacks : Union [ List[core.AgentCallback ] , core . AgentCallback , None ] = None ) -> str : \"\"\"Saves the currently trained actor policy in directory. If save is called before a trained policy is created, eg by calling train, an exception is raised. Args: directory: the directory to save the policy weights to. if the directory does not exist yet, a new directory is created. if None the policy is saved in a temp directory. callbacks: list of callbacks called during save (eg log.Agent) Returns: the absolute path to the directory containing the saved policy. \"\"\" if directory is None : directory = bcore . _get_temp_path () assert directory assert self . _backend_agent . _agent_context . _is_policy_trained , \"No trained policy available. Call train() first.\" directory = bcore . _mkdir ( directory ) agent_json_path = os . path . join ( directory , EasyAgent . _KEY_EASYAGENT_FILENAME ) with open ( agent_json_path , 'w' ) as jsonfile : agent_dict = self . _to_dict () json . dump ( agent_dict , jsonfile , sort_keys = True , indent = 2 ) callbacks = self . _to_callback_list ( callbacks = callbacks ) policy_directory = os . path . join ( directory , EasyAgent . _KEY_POLICY_DIRECTORY ) policy_directory = bcore . _mkdir ( policy_directory ) self . _backend_agent . save ( directory = policy_directory , callbacks = callbacks ) return directory train def train ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_iterations : int = 100 , num_episodes_per_iteration : int = 10 , max_steps_per_episode : int = 500 , num_epochs_per_iteration : int = 10 , num_iterations_between_eval : int = 5 , num_episodes_per_eval : int = 10 , learning_rate : float = 0.001 , train_context : easyagents . core . EpisodesTrainContext = None , default_plots : bool = None ) Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) num_episodes_per_iteration: number of episodes played per training iteration max_steps_per_episode: maximum number of steps per episode num_epochs_per_iteration: number of times the data collected for the current iteration is used to retrain the current policy num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training View Source def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 100 , num_episodes_per_iteration : int = 10 , max_steps_per_episode : int = 500 , num_epochs_per_iteration : int = 10 , num_iterations_between_eval : int = 5 , num_episodes_per_eval : int = 10 , learning_rate : float = 0 . 001 , train_context : core . EpisodesTrainContext = None , default_plots : bool = None ): \"\"\"Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) num_episodes_per_iteration: number of episodes played per training iteration max_steps_per_episode: maximum number of steps per episode num_epochs_per_iteration: number of times the data collected for the current iteration is used to retrain the current policy num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . EpisodesTrainContext () train_context . num_iterations = num_iterations train_context . num_episodes_per_iteration = num_episodes_per_iteration train_context . max_steps_per_episode = max_steps_per_episode train_context . num_epochs_per_iteration = num_epochs_per_iteration train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super (). train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context SacAgent class SacAgent ( gym_env_name : str , fc_layers : Union [ Tuple [ int , ... ], int , NoneType ] = None , backend : str = None ) Agent based on the Soft-Actor-Critic algorithm (https://arxiv.org/abs/1812.05905). View Source class SacAgent ( DqnAgent ): \"\"\"Agent based on the Soft-Actor-Critic algorithm (https://arxiv.org/abs/1812.05905).\"\"\" Ancestors (in MRO) easyagents.agents.DqnAgent easyagents.agents.EasyAgent abc.ABC Methods evaluate def evaluate ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: extensible score metrics View Source def evaluate ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) : \"\"\"Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: extensible score metrics \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) Metrics = namedtuple ( 'Metrics' , 'steps rewards' ) Rewards = namedtuple ( 'Rewards' , 'mean std min max all' ) all_rewards = list ( play_context . sum_of_rewards . values ()) mean_reward , std_reward , min_reward , max_reward = statistics . mean ( all_rewards ), statistics . stdev ( all_rewards ), min ( all_rewards ), max ( all_rewards ) rewards = Rewards ( mean = mean_reward , std = std_reward , min = min_reward , max = max_reward , all = all_rewards ) Steps = namedtuple ( 'Steps' , 'mean std min max all' ) all_num_steps = [] for i in play_context . rewards . keys () : all_num_steps . append ( len ( play_context . rewards [ i ] )) mean_steps , std_steps , min_steps , max_steps = statistics . mean ( all_num_steps ), statistics . stdev ( all_num_steps ), min ( all_num_steps ), max ( all_num_steps ) steps = Steps ( mean = mean_steps , std = std_steps , min = min_steps , max = max_steps , all = all_num_steps ) metrics = Metrics ( rewards = rewards , steps = steps ) return metrics play def play ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : easyagents . core . PlayContext = None , default_plots : bool = None ) Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training View Source def play ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : core . PlayContext = None , default_plots : bool = None ): \"\"\"Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training \"\"\" assert self . _backend_agent . _agent_context . _is_policy_trained , \"No trained policy available. Call train() first.\" if play_context is None : play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes callbacks = self . _to_callback_list ( callbacks = callbacks ) callbacks = self . _add_plot_callbacks ( callbacks , default_plots , [ plot . Steps (), plot . Rewards ()]) self . _backend_agent . play ( play_context = play_context , callbacks = callbacks ) return play_context save def save ( self , directory : Union [ str , NoneType ] = None , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None ) -> str Saves the currently trained actor policy in directory. If save is called before a trained policy is created, eg by calling train, an exception is raised. Args: directory: the directory to save the policy weights to. if the directory does not exist yet, a new directory is created. if None the policy is saved in a temp directory. callbacks: list of callbacks called during save (eg log.Agent) Returns: the absolute path to the directory containing the saved policy. View Source def save ( self , directory : Optional [ str ] = None , callbacks : Union [ List[core.AgentCallback ] , core . AgentCallback , None ] = None ) -> str : \"\"\"Saves the currently trained actor policy in directory. If save is called before a trained policy is created, eg by calling train, an exception is raised. Args: directory: the directory to save the policy weights to. if the directory does not exist yet, a new directory is created. if None the policy is saved in a temp directory. callbacks: list of callbacks called during save (eg log.Agent) Returns: the absolute path to the directory containing the saved policy. \"\"\" if directory is None : directory = bcore . _get_temp_path () assert directory assert self . _backend_agent . _agent_context . _is_policy_trained , \"No trained policy available. Call train() first.\" directory = bcore . _mkdir ( directory ) agent_json_path = os . path . join ( directory , EasyAgent . _KEY_EASYAGENT_FILENAME ) with open ( agent_json_path , 'w' ) as jsonfile : agent_dict = self . _to_dict () json . dump ( agent_dict , jsonfile , sort_keys = True , indent = 2 ) callbacks = self . _to_callback_list ( callbacks = callbacks ) policy_directory = os . path . join ( directory , EasyAgent . _KEY_POLICY_DIRECTORY ) policy_directory = bcore . _mkdir ( policy_directory ) self . _backend_agent . save ( directory = policy_directory , callbacks = callbacks ) return directory train def train ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_iterations : int = 20000 , max_steps_per_episode : int = 500 , num_steps_per_iteration : int = 1 , num_steps_buffer_preload = 1000 , num_steps_sampled_from_buffer = 64 , num_iterations_between_eval : int = 1000 , num_episodes_per_eval : int = 10 , learning_rate : float = 0.001 , train_context : easyagents . core . StepsTrainContext = None , default_plots : bool = None ) Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) max_steps_per_episode: maximum number of steps per episode num_steps_per_iteration: number of steps played per training iteration num_steps_buffer_preload: number of initial collect steps to preload the buffer num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training View Source def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 20000 , max_steps_per_episode : int = 500 , num_steps_per_iteration : int = 1 , num_steps_buffer_preload = 1000 , num_steps_sampled_from_buffer = 64 , num_iterations_between_eval : int = 1000 , num_episodes_per_eval : int = 10 , learning_rate : float = 0 . 001 , train_context : core . StepsTrainContext = None , default_plots : bool = None ): \"\"\"Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) max_steps_per_episode: maximum number of steps per episode num_steps_per_iteration: number of steps played per training iteration num_steps_buffer_preload: number of initial collect steps to preload the buffer num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . StepsTrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . num_steps_per_iteration = num_steps_per_iteration train_context . num_steps_buffer_preload = num_steps_buffer_preload train_context . num_steps_sampled_from_buffer = num_steps_sampled_from_buffer train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super (). train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context","title":"Agents"},{"location":"reference/easyagents/agents/#module-easyagentsagents","text":"This module contains the public api of the EasyAgents reinforcement learning library. It consist mainly of the class hierarchy of the available agents (algorithms), registrations and the management of the available backends. In their implementation the agents forward their calls to the chosen backend. View Source \"\"\"This module contains the public api of the EasyAgents reinforcement learning library. It consist mainly of the class hierarchy of the available agents (algorithms), registrations and the management of the available backends. In their implementation the agents forward their calls to the chosen backend. \"\"\" from abc import ABC from collections import namedtuple import json import os import statistics from typing import Dict , List , Optional , Tuple , Type , Union from easyagents import core from easyagents.backends import core as bcore from easyagents.callbacks import plot import easyagents.backends.default import easyagents.backends.tfagents import tensorflow as tf # import easyagents.backends.tforce _backends : [ bcore . BackendAgentFactory ] = [] \"\"\"The seed used for all agents and gym environments. If None no seed is set (default).\"\"\" seed : Optional [ int ] = None def load ( directory : str , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None ): \"\"\"Loads an agent from directory. After a successful load play() may be called directly. The agent, model, backend, seed and play policy are restored according to the previously saved agent. Args: directory: the directory containing the previously saved policy. callbacks: list of callbacks called during save (eg log.Agent) Result: a new instance of EasyAgents \"\"\" assert directory agent_json_path = os . path . join ( directory , EasyAgent . _KEY_EASYAGENT_FILENAME ) policy_directory = os . path . join ( directory , EasyAgent . _KEY_POLICY_DIRECTORY ) assert os . path . isdir ( directory ), f 'directory \"{directory}\" not found.' assert os . path . isfile ( agent_json_path ), 'file \"{agent_json_path}\" not found.' assert os . path . isdir ( policy_directory ), f 'directory \"{policy_directory}\" not found.' with open ( agent_json_path ) as jsonfile : agent_dict = json . load ( jsonfile ) result = EasyAgent . _from_dict ( agent_dict ) callbacks = result . _to_callback_list ( callbacks = callbacks ) result . _backend_agent . load ( directory = policy_directory , callbacks = callbacks ) return result def register_backend ( backend : bcore . BackendAgentFactory ): \"\"\"registers a backend as a factory for agent implementations. If another backend with the same name is already registered, the old backend is replaced by backend. \"\"\" assert backend old_backends = [ b for b in _backends if b . backend_name == backend . backend_name ] for old_backend in old_backends : _backends . remove ( old_backend ) _backends . append ( backend ) def activate_tensorforce (): \"\"\"registers the tensorforce backend. Due to an incompatibility between tensorforce and tf-agents, both libraries may not run in the same python instance. Thus - for the time being - once this method is called, the tfagents backend may not be used anymore. \"\"\" import easyagents.backends.tforce global _backends assert easyagents . backends . core . _tf_eager_execution_active is None or \\ easyagents . backends . core . _tf_eager_execution_active == False , \\ \"tensorforce can not be activated, since tensorflow eager execution mode was already actived.\" _backends = [] register_backend ( easyagents . backends . default . DefaultAgentFactory ( register_tensorforce = True )) register_backend ( easyagents . backends . tforce . TensorforceAgentFactory ()) def _activate_tfagents (): \"\"\"registers the tfagents backend. Due to an incompatibility between tensorforce and tf-agents, both libraries may not run in the same python instance. \"\"\" global _backends assert easyagents . backends . core . _tf_eager_execution_active is None or \\ easyagents . backends . core . _tf_eager_execution_active == True , \\ \"tfagents can not be activated, since tensorflow eager execution mode was already disabled.\" _backends = [] register_backend ( easyagents . backends . default . DefaultAgentFactory ( register_tensorforce = False )) register_backend ( easyagents . backends . tfagents . TfAgentAgentFactory ()) _activate_tfagents () class EasyAgent ( ABC ): \"\"\"Abstract base class for all easy reinforcment learning agents. Besides forwarding train and play it implements persistence.\"\"\" _KEY_BACKEND = 'backend' _KEY_EASYAGENT_CLASS = 'easyagent_class' _KEY_EASYAGENT_FILENAME = 'easyagent.json' _KEY_MODEL_CONFIG = 'model_config' _KEY_POLICY_DIRECTORY = 'policy' _KEY_VERSION = 'version' def __init__ ( self , gym_env_name : str , fc_layers : Union [ Tuple [ int , ... ], int , None ] = None , backend : str = None ): \"\"\" Args: gym_env_name: name of an OpenAI gym environment to be used for training and evaluation fc_layers: defines the neural network to be used, a sequence of fully connected layers of the given size. Eg (75,40) yields a neural network consisting out of 2 hidden layers, the first one containing 75 and the second layer containing 40 neurons. backend=the backend to be used (eg 'tfagents'), if None a default implementation is used. call get_backends() to get a list of the available backends. \"\"\" model_config = core . ModelConfig ( gym_env_name = gym_env_name , fc_layers = fc_layers , seed = seed ) self . _initialize ( model_config = model_config , backend_name = backend ) return def _initialize ( self , model_config : core . ModelConfig , backend_name : str = None ): if backend_name is None : backend_name = easyagents . backends . default . DefaultAgentFactory . backend_name backend : bcore . BackendAgentFactory = _get_backend ( backend_name ) assert model_config is not None , \"model_config not set.\" assert backend , f 'Backend \"{backend_name}\" not found. The registered backends are {get_backends()}.' self . _model_config : core . ModelConfig = model_config backend_agent = backend . create_agent ( easyagent_type = type ( self ), model_config = model_config ) assert backend_agent , f 'Backend \"{backend_name}\" does not implement \"{type(self).__name__}\". ' + \\ f 'Choose one of the following backend {get_backends(type(self))}.' self . _backend_agent : Optional [ bcore . _BackendAgent ] = backend_agent self . _backend_name : str = backend_name self . _backend_agent . _agent_context . _agent_saver = self . save return def _add_plot_callbacks ( self , callbacks : List [ core . AgentCallback ], default_plots : Optional [ bool ], default_plot_callbacks : List [ plot . _PlotCallback ] ) -> List [ core . AgentCallback ]: \"\"\"Adds the default callbacks and sorts all callbacks in the order _PreProcessCallbacks, AgentCallbacks, _PostProcessCallbacks. Args: callbacks: existing callbacks to prepare default_plots: if set or if None and callbacks does not contain plots then the default plots are added default_plot_callbacks: plot callbacks to add. \"\"\" pre_process : List [ core . AgentCallback ] = [ plot . _PreProcess ()] agent : List [ core . AgentCallback ] = [] post_process : List [ core . AgentCallback ] = [ plot . _PostProcess ()] if default_plots is None : default_plots = True for c in callbacks : default_plots = default_plots and ( not isinstance ( c , plot . _PlotCallback )) if default_plots : agent = default_plot_callbacks for c in callbacks : if isinstance ( c , core . _PreProcessCallback ): pre_process . append ( c ) else : if isinstance ( c , core . _PostProcessCallback ): post_process . append ( c ) else : agent . append ( c ) result : List [ core . AgentCallback ] = pre_process + agent + post_process return result @staticmethod def _from_dict ( param_dict : Dict [ str , object ]): \"\"\"recreates a new agent instance according to the definition previously created by _to_dict. Returns: new agent instance (excluding any trained policy), the agent type is preserved. \"\"\" assert param_dict mc : core . ModelConfig = core . ModelConfig . _from_dict ( param_dict [ EasyAgent . _KEY_MODEL_CONFIG ]) agent_class = globals ()[ param_dict [ EasyAgent . _KEY_EASYAGENT_CLASS ]] backend : str = param_dict [ EasyAgent . _KEY_BACKEND ] result = agent_class ( gym_env_name = mc . original_env_name , backend = backend ) result . _initialize ( model_config = mc , backend_name = backend ) return result def _to_callback_list ( self , callbacks : Union [ Optional [ core . AgentCallback ], List [ core . AgentCallback ]]) -> List [ core . AgentCallback ]: \"\"\"maps callbacks to an admissible callback list. if callbacks is None an empty list is returned. if callbacks is an AgentCallback a list containing only this callback is returned otherwise callbacks is returned \"\"\" result : List [ core . AgentCallback ] = [] if not callbacks is None : if isinstance ( callbacks , core . AgentCallback ): result = [ callbacks ] else : assert isinstance ( callbacks , list ), \"callback not an AgentCallback or a list thereof.\" result = callbacks return result def _to_dict ( self ) -> Dict [ str , object ]: \"\"\"saves the agent definition to a dict. Returns: dict containing all parameters to recreate the agent (excluding a trained policy) \"\"\" result : Dict [ str , object ] = dict () result [ EasyAgent . _KEY_VERSION ] = easyagents . __version__ result [ EasyAgent . _KEY_EASYAGENT_CLASS ] = self . __class__ . __name__ result [ EasyAgent . _KEY_BACKEND ] = self . _backend_name result [ EasyAgent . _KEY_MODEL_CONFIG ] = self . _model_config . _to_dict () result [ EasyAgent . _KEY_POLICY_DIRECTORY ] = 'policy' return result def evaluate ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ): \"\"\"Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: extensible score metrics \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) Metrics = namedtuple ( 'Metrics' , 'steps rewards' ) Rewards = namedtuple ( 'Rewards' , 'mean std min max all' ) all_rewards = list ( play_context . sum_of_rewards . values ()) mean_reward , std_reward , min_reward , max_reward = statistics . mean ( all_rewards ), statistics . stdev ( all_rewards ), min ( all_rewards ), max ( all_rewards ) rewards = Rewards ( mean = mean_reward , std = std_reward , min = min_reward , max = max_reward , all = all_rewards ) Steps = namedtuple ( 'Steps' , 'mean std min max all' ) all_num_steps = [] for i in play_context . rewards . keys (): all_num_steps . append ( len ( play_context . rewards [ i ])) mean_steps , std_steps , min_steps , max_steps = statistics . mean ( all_num_steps ), statistics . stdev ( all_num_steps ), min ( all_num_steps ), max ( all_num_steps ) steps = Steps ( mean = mean_steps , std = std_steps , min = min_steps , max = max_steps , all = all_num_steps ) metrics = Metrics ( rewards = rewards , steps = steps ) return metrics def play ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : core . PlayContext = None , default_plots : bool = None ): \"\"\"Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training \"\"\" assert self . _backend_agent . _agent_context . _is_policy_trained , \"No trained policy available. Call train() first.\" if play_context is None : play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes callbacks = self . _to_callback_list ( callbacks = callbacks ) callbacks = self . _add_plot_callbacks ( callbacks , default_plots , [ plot . Steps (), plot . Rewards ()]) self . _backend_agent . play ( play_context = play_context , callbacks = callbacks ) return play_context def save ( self , directory : Optional [ str ] = None , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None ) -> str : \"\"\"Saves the currently trained actor policy in directory. If save is called before a trained policy is created, eg by calling train, an exception is raised. Args: directory: the directory to save the policy weights to. if the directory does not exist yet, a new directory is created. if None the policy is saved in a temp directory. callbacks: list of callbacks called during save (eg log.Agent) Returns: the absolute path to the directory containing the saved policy. \"\"\" if directory is None : directory = bcore . _get_temp_path () assert directory assert self . _backend_agent . _agent_context . _is_policy_trained , \"No trained policy available. Call train() first.\" directory = bcore . _mkdir ( directory ) agent_json_path = os . path . join ( directory , EasyAgent . _KEY_EASYAGENT_FILENAME ) with open ( agent_json_path , 'w' ) as jsonfile : agent_dict = self . _to_dict () json . dump ( agent_dict , jsonfile , sort_keys = True , indent = 2 ) callbacks = self . _to_callback_list ( callbacks = callbacks ) policy_directory = os . path . join ( directory , EasyAgent . _KEY_POLICY_DIRECTORY ) policy_directory = bcore . _mkdir ( policy_directory ) self . _backend_agent . save ( directory = policy_directory , callbacks = callbacks ) return directory def train ( self , train_context : core . TrainContext , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ], default_plots : Optional [ bool ]): \"\"\"Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during the training and evaluation train_context: training configuration to be used (num_iterations,num_episodes_per_iteration,...) default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty \"\"\" assert train_context , \"train_context not set.\" callbacks = self . _to_callback_list ( callbacks = callbacks ) callbacks = self . _add_plot_callbacks ( callbacks , default_plots , [ plot . Loss (), plot . Steps (), plot . Rewards ()]) self . _backend_agent . train ( train_context = train_context , callbacks = callbacks ) def get_backends ( agent : Optional [ Type [ EasyAgent ]] = None ): \"\"\"returns a list of all registered backends containing an implementation for the EasyAgent type agent. Args: agent: type deriving from EasyAgent for which the backend identifiers are returned. Returns: a list of admissible values for the 'backend' argument of EazyAgents constructors or a list of all available backends if agent is None. \"\"\" result = [ b . backend_name for b in _backends ] if agent : result = [ b . backend_name for b in _backends if agent in b . get_algorithms ()] return result def _get_backend ( backend_name : str ): \"\"\"Yields the backend with the given name. Returns: the backend instance or None if no backend is found.\"\"\" assert backend_name backends = [ b for b in _backends if b . backend_name == backend_name ] assert len ( backends ) <= 1 , f 'no backend found with name \"{backend_name}\". Available backends = {get_backends()}' result = None if backends : result = backends [ 0 ] return result class CemAgent ( EasyAgent ): \"\"\"creates a new agent based on the cross-entropy-method algorithm. From https://learning.mpi-sws.org/mlss2016/slides/2016-MLSS-RL.pdf: Initialize \u00b5 \u2208Rd,\u03c3 \u2208Rd for iteration = 1,2,... num_iterations do Collect num_episodes_per_iteration samples of \u03b8i \u223c N(\u00b5,diag(\u03c3)) Perform a noisy evaluation Ri \u223c \u03b8i Select the top elite_set_fraction of samples (e.g. p = 0.2), which we\u2019ll call the elite set Fit a Gaussian distribution, with diagonal covariance, to the elite set, obtaining a new \u00b5,\u03c3. end for Return the \ufb01nal \u00b5. see https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.6579&rep=rep1&type=pdf \"\"\" def __init__ ( self , gym_env_name : str , fc_layers : Optional [ Tuple [ int , ... ]] = None , backend : str = None ): super () . __init__ ( gym_env_name , fc_layers , backend ) assert False , \"CemAgent is currently not available (pending migration of keras-rl to tf2.0)\" def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 100 , num_episodes_per_iteration : int = 50 , max_steps_per_episode : int = 500 , elite_set_fraction : float = 0.1 , num_iterations_between_eval : int = 5 , num_episodes_per_eval : int = 10 , train_context : core . CemTrainContext = None , default_plots : bool = None ): \"\"\"Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) num_episodes_per_iteration: number of episodes played in each iteration. for each episode a new policy is sampled from the current weight distribution. max_steps_per_episode: maximum number of steps per episode elite_set_fraction: the fraction of policies which are members of the elite set. These policies are used to fit a new weight distribution in each iteration. num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . CemTrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . elite_set_fraction = elite_set_fraction train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval super () . train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context class DqnAgent ( EasyAgent ): \"\"\"creates a new agent based on the Dqn algorithm. From wikipedia: The DeepMind system used a deep convolutional neural network, with layers of tiled convolutional filters to mimic the effects of receptive fields. Reinforcement learning is unstable or divergent when a nonlinear function approximator such as a neural network is used to represent Q. This instability comes from the correlations present in the sequence of observations, the fact that small updates to Q may significantly change the policy and the data distribution, and the correlations between Q and the target values. The technique used experience replay, a biologically inspired mechanism that uses a random sample of prior actions instead of the most recent action to proceed.[2] This removes correlations in the observation sequence and smooths changes in the data distribution. Iterative update adjusts Q towards target values that are only periodically updated, further reducing correlations with the target.[17] see also: https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning \"\"\" def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 20000 , max_steps_per_episode : int = 500 , num_steps_per_iteration : int = 1 , num_steps_buffer_preload = 1000 , num_steps_sampled_from_buffer = 64 , num_iterations_between_eval : int = 1000 , num_episodes_per_eval : int = 10 , learning_rate : float = 0.001 , train_context : core . StepsTrainContext = None , default_plots : bool = None ): \"\"\"Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) max_steps_per_episode: maximum number of steps per episode num_steps_per_iteration: number of steps played per training iteration num_steps_buffer_preload: number of initial collect steps to preload the buffer num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . StepsTrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . num_steps_per_iteration = num_steps_per_iteration train_context . num_steps_buffer_preload = num_steps_buffer_preload train_context . num_steps_sampled_from_buffer = num_steps_sampled_from_buffer train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super () . train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context class DoubleDqnAgent ( DqnAgent ): \"\"\"Agent based on the Double Dqn algorithm (https://arxiv.org/abs/1509.06461)\"\"\" class DuelingDqnAgent ( DqnAgent ): \"\"\"Agent based on the Dueling Dqn algorithm (https://arxiv.org/abs/1511.06581).\"\"\" class PpoAgent ( EasyAgent ): \"\"\"creates a new agent based on the PPO algorithm. PPO is an actor-critic algorithm using 2 neural networks. The actor network to predict the next action to be taken and the critic network to estimate the value of the game state we are currently in (the expected, discounted sum of future rewards when following the current actor network). see also: https://spinningup.openai.com/en/latest/algorithms/ppo.html \"\"\" def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 100 , num_episodes_per_iteration : int = 10 , max_steps_per_episode : int = 500 , num_epochs_per_iteration : int = 10 , num_iterations_between_eval : int = 5 , num_episodes_per_eval : int = 10 , learning_rate : float = 0.001 , train_context : core . PpoTrainContext = None , default_plots : bool = None ): \"\"\"Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) num_episodes_per_iteration: number of episodes played per training iteration max_steps_per_episode: maximum number of steps per episode num_epochs_per_iteration: number of times the data collected for the current iteration is used to retrain the current policy num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . PpoTrainContext () train_context . num_iterations = num_iterations train_context . num_episodes_per_iteration = num_episodes_per_iteration train_context . max_steps_per_episode = max_steps_per_episode train_context . num_epochs_per_iteration = num_epochs_per_iteration train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super () . train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context class RandomAgent ( EasyAgent ): \"\"\"Agent which always chooses uniform random actions.\"\"\" def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 10 , max_steps_per_episode : int = 1000 , num_episodes_per_eval : int = 10 , train_context : core . TrainContext = None , default_plots : bool = None ): \"\"\"Evaluates the environment using a uniform random policy. The evaluation is performed in batches of num_episodes_per_eval episodes. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times a batch of num_episodes_per_eval episodes is evaluated. max_steps_per_episode: maximum number of steps per episode num_episodes_per_eval: number of episodes played to estimate the average return and steps train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...) Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . TrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . num_epochs_per_iteration = 0 train_context . num_iterations_between_eval = 1 train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = 1 super () . train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context class ReinforceAgent ( EasyAgent ): \"\"\"creates a new agent based on the Reinforce algorithm. Reinforce is a vanilla policy gradient algorithm using a single actor network. see also: www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf \"\"\" def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 100 , num_episodes_per_iteration : int = 10 , max_steps_per_episode : int = 500 , num_epochs_per_iteration : int = 10 , num_iterations_between_eval : int = 5 , num_episodes_per_eval : int = 10 , learning_rate : float = 0.001 , train_context : core . EpisodesTrainContext = None , default_plots : bool = None ): \"\"\"Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) num_episodes_per_iteration: number of episodes played per training iteration max_steps_per_episode: maximum number of steps per episode num_epochs_per_iteration: number of times the data collected for the current iteration is used to retrain the current policy num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . EpisodesTrainContext () train_context . num_iterations = num_iterations train_context . num_episodes_per_iteration = num_episodes_per_iteration train_context . max_steps_per_episode = max_steps_per_episode train_context . num_epochs_per_iteration = num_epochs_per_iteration train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super () . train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context class SacAgent ( DqnAgent ): \"\"\"Agent based on the Soft-Actor-Critic algorithm (https://arxiv.org/abs/1812.05905).\"\"\"","title":"Module easyagents.agents"},{"location":"reference/easyagents/agents/#variables","text":"seed","title":"Variables"},{"location":"reference/easyagents/agents/#functions","text":"","title":"Functions"},{"location":"reference/easyagents/agents/#activate_tensorforce","text":"def activate_tensorforce ( ) registers the tensorforce backend. Due to an incompatibility between tensorforce and tf-agents, both libraries may not run in the same python instance. Thus - for the time being - once this method is called, the tfagents backend may not be used anymore. View Source def activate_tensorforce (): \"\"\"registers the tensorforce backend. Due to an incompatibility between tensorforce and tf-agents, both libraries may not run in the same python instance. Thus - for the time being - once this method is called, the tfagents backend may not be used anymore. \"\"\" import easyagents.backends.tforce global _backends assert easyagents . backends . core . _tf_eager_execution_active is None or \\ easyagents . backends . core . _tf_eager_execution_active == False , \\ \"tensorforce can not be activated, since tensorflow eager execution mode was already actived.\" _backends = [] register_backend ( easyagents . backends . default . DefaultAgentFactory ( register_tensorforce = True )) register_backend ( easyagents . backends . tforce . TensorforceAgentFactory ())","title":"activate_tensorforce"},{"location":"reference/easyagents/agents/#get_backends","text":"def get_backends ( agent : Union [ Type [ easyagents . agents . EasyAgent ], NoneType ] = None ) returns a list of all registered backends containing an implementation for the EasyAgent type agent. Args: agent: type deriving from EasyAgent for which the backend identifiers are returned. Returns: a list of admissible values for the 'backend' argument of EazyAgents constructors or a list of all available backends if agent is None. View Source def get_backends ( agent : Optional [ Type[EasyAgent ] ] = None ) : \"\"\"returns a list of all registered backends containing an implementation for the EasyAgent type agent. Args: agent: type deriving from EasyAgent for which the backend identifiers are returned. Returns: a list of admissible values for the 'backend' argument of EazyAgents constructors or a list of all available backends if agent is None. \"\"\" result = [ b.backend_name for b in _backends ] if agent : result = [ b.backend_name for b in _backends if agent in b.get_algorithms() ] return result","title":"get_backends"},{"location":"reference/easyagents/agents/#load","text":"def load ( directory : str , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None ) Loads an agent from directory. After a successful load play() may be called directly. The agent, model, backend, seed and play policy are restored according to the previously saved agent. Args: directory: the directory containing the previously saved policy. callbacks: list of callbacks called during save (eg log.Agent) Result: a new instance of EasyAgents View Source def load ( directory : str , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None ): \"\"\"Loads an agent from directory. After a successful load play() may be called directly. The agent, model, backend, seed and play policy are restored according to the previously saved agent. Args: directory: the directory containing the previously saved policy. callbacks: list of callbacks called during save (eg log.Agent) Result: a new instance of EasyAgents \"\"\" assert directory agent_json_path = os . path . join ( directory , EasyAgent . _KEY_EASYAGENT_FILENAME ) policy_directory = os . path . join ( directory , EasyAgent . _KEY_POLICY_DIRECTORY ) assert os . path . isdir ( directory ), f 'directory \"{directory}\" not found.' assert os . path . isfile ( agent_json_path ), 'file \"{agent_json_path}\" not found.' assert os . path . isdir ( policy_directory ), f 'directory \"{policy_directory}\" not found.' with open ( agent_json_path ) as jsonfile : agent_dict = json . load ( jsonfile ) result = EasyAgent . _from_dict ( agent_dict ) callbacks = result . _to_callback_list ( callbacks = callbacks ) result . _backend_agent . load ( directory = policy_directory , callbacks = callbacks ) return result","title":"load"},{"location":"reference/easyagents/agents/#register_backend","text":"def register_backend ( backend : easyagents . backends . core . BackendAgentFactory ) registers a backend as a factory for agent implementations. If another backend with the same name is already registered, the old backend is replaced by backend. View Source def register_backend ( backend : bcore . BackendAgentFactory ): \"\"\"registers a backend as a factory for agent implementations. If another backend with the same name is already registered, the old backend is replaced by backend. \"\"\" assert backend old_backends = [ b for b in _backends if b . backend_name == backend . backend_name ] for old_backend in old_backends : _backends . remove ( old_backend ) _backends . append ( backend )","title":"register_backend"},{"location":"reference/easyagents/agents/#classes","text":"","title":"Classes"},{"location":"reference/easyagents/agents/#cemagent","text":"class CemAgent ( gym_env_name : str , fc_layers : Union [ Tuple [ int , ... ], NoneType ] = None , backend : str = None ) creates a new agent based on the cross-entropy-method algorithm. From https://learning.mpi-sws.org/mlss2016/slides/2016-MLSS-RL.pdf: Initialize \u00b5 \u2208Rd,\u03c3 \u2208Rd for iteration = 1,2,... num_iterations do Collect num_episodes_per_iteration samples of \u03b8i \u223c N(\u00b5,diag(\u03c3)) Perform a noisy evaluation Ri \u223c \u03b8i Select the top elite_set_fraction of samples (e.g. p = 0.2), which we\u2019ll call the elite set Fit a Gaussian distribution, with diagonal covariance, to the elite set, obtaining a new \u00b5,\u03c3. end for Return the \ufb01nal \u00b5. see https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.6579&rep=rep1&type=pdf View Source class CemAgent ( EasyAgent ): \"\"\"creates a new agent based on the cross-entropy-method algorithm. From https://learning.mpi-sws.org/mlss2016/slides/2016-MLSS-RL.pdf: Initialize \u00b5 \u2208Rd,\u03c3 \u2208Rd for iteration = 1,2,... num_iterations do Collect num_episodes_per_iteration samples of \u03b8i \u223c N(\u00b5,diag(\u03c3)) Perform a noisy evaluation Ri \u223c \u03b8i Select the top elite_set_fraction of samples (e.g. p = 0.2), which we\u2019ll call the elite set Fit a Gaussian distribution, with diagonal covariance, to the elite set, obtaining a new \u00b5,\u03c3. end for Return the \ufb01nal \u00b5. see https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.6579&rep=rep1&type=pdf \"\"\" def __init__ ( self , gym_env_name: str , fc_layers: Optional [ Tuple [ int , ...]] = None , backend: str = None ): super (). __init__ ( gym_env_name , fc_layers , backend ) assert False , \"CemAgent is currently not available (pending migration of keras-rl to tf2.0)\" def train ( self , callbacks: Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations: int = 100 , num_episodes_per_iteration: int = 50 , max_steps_per_episode: int = 500 , elite_set_fraction: float = 0.1 , num_iterations_between_eval: int = 5 , num_episodes_per_eval: int = 10 , train_context: core . CemTrainContext = None , default_plots: bool = None ): \"\"\"Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) num_episodes_per_iteration: number of episodes played in each iteration. for each episode a new policy is sampled from the current weight distribution. max_steps_per_episode: maximum number of steps per episode elite_set_fraction: the fraction of policies which are members of the elite set. These policies are used to fit a new weight distribution in each iteration. num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None: train_context = core . CemTrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . elite_set_fraction = elite_set_fraction train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval super (). train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context","title":"CemAgent"},{"location":"reference/easyagents/agents/#ancestors-in-mro","text":"easyagents.agents.EasyAgent abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/agents/#methods","text":"","title":"Methods"},{"location":"reference/easyagents/agents/#evaluate","text":"def evaluate ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: extensible score metrics View Source def evaluate ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) : \"\"\"Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: extensible score metrics \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) Metrics = namedtuple ( 'Metrics' , 'steps rewards' ) Rewards = namedtuple ( 'Rewards' , 'mean std min max all' ) all_rewards = list ( play_context . sum_of_rewards . values ()) mean_reward , std_reward , min_reward , max_reward = statistics . mean ( all_rewards ), statistics . stdev ( all_rewards ), min ( all_rewards ), max ( all_rewards ) rewards = Rewards ( mean = mean_reward , std = std_reward , min = min_reward , max = max_reward , all = all_rewards ) Steps = namedtuple ( 'Steps' , 'mean std min max all' ) all_num_steps = [] for i in play_context . rewards . keys () : all_num_steps . append ( len ( play_context . rewards [ i ] )) mean_steps , std_steps , min_steps , max_steps = statistics . mean ( all_num_steps ), statistics . stdev ( all_num_steps ), min ( all_num_steps ), max ( all_num_steps ) steps = Steps ( mean = mean_steps , std = std_steps , min = min_steps , max = max_steps , all = all_num_steps ) metrics = Metrics ( rewards = rewards , steps = steps ) return metrics","title":"evaluate"},{"location":"reference/easyagents/agents/#play","text":"def play ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : easyagents . core . PlayContext = None , default_plots : bool = None ) Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training View Source def play ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : core . PlayContext = None , default_plots : bool = None ): \"\"\"Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training \"\"\" assert self . _backend_agent . _agent_context . _is_policy_trained , \"No trained policy available. Call train() first.\" if play_context is None : play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes callbacks = self . _to_callback_list ( callbacks = callbacks ) callbacks = self . _add_plot_callbacks ( callbacks , default_plots , [ plot . Steps (), plot . Rewards ()]) self . _backend_agent . play ( play_context = play_context , callbacks = callbacks ) return play_context","title":"play"},{"location":"reference/easyagents/agents/#save","text":"def save ( self , directory : Union [ str , NoneType ] = None , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None ) -> str Saves the currently trained actor policy in directory. If save is called before a trained policy is created, eg by calling train, an exception is raised. Args: directory: the directory to save the policy weights to. if the directory does not exist yet, a new directory is created. if None the policy is saved in a temp directory. callbacks: list of callbacks called during save (eg log.Agent) Returns: the absolute path to the directory containing the saved policy. View Source def save ( self , directory : Optional [ str ] = None , callbacks : Union [ List[core.AgentCallback ] , core . AgentCallback , None ] = None ) -> str : \"\"\"Saves the currently trained actor policy in directory. If save is called before a trained policy is created, eg by calling train, an exception is raised. Args: directory: the directory to save the policy weights to. if the directory does not exist yet, a new directory is created. if None the policy is saved in a temp directory. callbacks: list of callbacks called during save (eg log.Agent) Returns: the absolute path to the directory containing the saved policy. \"\"\" if directory is None : directory = bcore . _get_temp_path () assert directory assert self . _backend_agent . _agent_context . _is_policy_trained , \"No trained policy available. Call train() first.\" directory = bcore . _mkdir ( directory ) agent_json_path = os . path . join ( directory , EasyAgent . _KEY_EASYAGENT_FILENAME ) with open ( agent_json_path , 'w' ) as jsonfile : agent_dict = self . _to_dict () json . dump ( agent_dict , jsonfile , sort_keys = True , indent = 2 ) callbacks = self . _to_callback_list ( callbacks = callbacks ) policy_directory = os . path . join ( directory , EasyAgent . _KEY_POLICY_DIRECTORY ) policy_directory = bcore . _mkdir ( policy_directory ) self . _backend_agent . save ( directory = policy_directory , callbacks = callbacks ) return directory","title":"save"},{"location":"reference/easyagents/agents/#train","text":"def train ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_iterations : int = 100 , num_episodes_per_iteration : int = 50 , max_steps_per_episode : int = 500 , elite_set_fraction : float = 0.1 , num_iterations_between_eval : int = 5 , num_episodes_per_eval : int = 10 , train_context : easyagents . core . CemTrainContext = None , default_plots : bool = None ) Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) num_episodes_per_iteration: number of episodes played in each iteration. for each episode a new policy is sampled from the current weight distribution. max_steps_per_episode: maximum number of steps per episode elite_set_fraction: the fraction of policies which are members of the elite set. These policies are used to fit a new weight distribution in each iteration. num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training View Source def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 100 , num_episodes_per_iteration : int = 50 , max_steps_per_episode : int = 500 , elite_set_fraction : float = 0 . 1 , num_iterations_between_eval : int = 5 , num_episodes_per_eval : int = 10 , train_context : core . CemTrainContext = None , default_plots : bool = None ): \"\"\"Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) num_episodes_per_iteration: number of episodes played in each iteration. for each episode a new policy is sampled from the current weight distribution. max_steps_per_episode: maximum number of steps per episode elite_set_fraction: the fraction of policies which are members of the elite set. These policies are used to fit a new weight distribution in each iteration. num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . CemTrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . elite_set_fraction = elite_set_fraction train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval super (). train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context","title":"train"},{"location":"reference/easyagents/agents/#doubledqnagent","text":"class DoubleDqnAgent ( gym_env_name : str , fc_layers : Union [ Tuple [ int , ... ], int , NoneType ] = None , backend : str = None ) Agent based on the Double Dqn algorithm (https://arxiv.org/abs/1509.06461) View Source class DoubleDqnAgent ( DqnAgent ): \"\"\"Agent based on the Double Dqn algorithm (https://arxiv.org/abs/1509.06461)\"\"\"","title":"DoubleDqnAgent"},{"location":"reference/easyagents/agents/#ancestors-in-mro_1","text":"easyagents.agents.DqnAgent easyagents.agents.EasyAgent abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/agents/#methods_1","text":"","title":"Methods"},{"location":"reference/easyagents/agents/#evaluate_1","text":"def evaluate ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: extensible score metrics View Source def evaluate ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) : \"\"\"Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: extensible score metrics \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) Metrics = namedtuple ( 'Metrics' , 'steps rewards' ) Rewards = namedtuple ( 'Rewards' , 'mean std min max all' ) all_rewards = list ( play_context . sum_of_rewards . values ()) mean_reward , std_reward , min_reward , max_reward = statistics . mean ( all_rewards ), statistics . stdev ( all_rewards ), min ( all_rewards ), max ( all_rewards ) rewards = Rewards ( mean = mean_reward , std = std_reward , min = min_reward , max = max_reward , all = all_rewards ) Steps = namedtuple ( 'Steps' , 'mean std min max all' ) all_num_steps = [] for i in play_context . rewards . keys () : all_num_steps . append ( len ( play_context . rewards [ i ] )) mean_steps , std_steps , min_steps , max_steps = statistics . mean ( all_num_steps ), statistics . stdev ( all_num_steps ), min ( all_num_steps ), max ( all_num_steps ) steps = Steps ( mean = mean_steps , std = std_steps , min = min_steps , max = max_steps , all = all_num_steps ) metrics = Metrics ( rewards = rewards , steps = steps ) return metrics","title":"evaluate"},{"location":"reference/easyagents/agents/#play_1","text":"def play ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : easyagents . core . PlayContext = None , default_plots : bool = None ) Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training View Source def play ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : core . PlayContext = None , default_plots : bool = None ): \"\"\"Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training \"\"\" assert self . _backend_agent . _agent_context . _is_policy_trained , \"No trained policy available. Call train() first.\" if play_context is None : play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes callbacks = self . _to_callback_list ( callbacks = callbacks ) callbacks = self . _add_plot_callbacks ( callbacks , default_plots , [ plot . Steps (), plot . Rewards ()]) self . _backend_agent . play ( play_context = play_context , callbacks = callbacks ) return play_context","title":"play"},{"location":"reference/easyagents/agents/#save_1","text":"def save ( self , directory : Union [ str , NoneType ] = None , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None ) -> str Saves the currently trained actor policy in directory. If save is called before a trained policy is created, eg by calling train, an exception is raised. Args: directory: the directory to save the policy weights to. if the directory does not exist yet, a new directory is created. if None the policy is saved in a temp directory. callbacks: list of callbacks called during save (eg log.Agent) Returns: the absolute path to the directory containing the saved policy. View Source def save ( self , directory : Optional [ str ] = None , callbacks : Union [ List[core.AgentCallback ] , core . AgentCallback , None ] = None ) -> str : \"\"\"Saves the currently trained actor policy in directory. If save is called before a trained policy is created, eg by calling train, an exception is raised. Args: directory: the directory to save the policy weights to. if the directory does not exist yet, a new directory is created. if None the policy is saved in a temp directory. callbacks: list of callbacks called during save (eg log.Agent) Returns: the absolute path to the directory containing the saved policy. \"\"\" if directory is None : directory = bcore . _get_temp_path () assert directory assert self . _backend_agent . _agent_context . _is_policy_trained , \"No trained policy available. Call train() first.\" directory = bcore . _mkdir ( directory ) agent_json_path = os . path . join ( directory , EasyAgent . _KEY_EASYAGENT_FILENAME ) with open ( agent_json_path , 'w' ) as jsonfile : agent_dict = self . _to_dict () json . dump ( agent_dict , jsonfile , sort_keys = True , indent = 2 ) callbacks = self . _to_callback_list ( callbacks = callbacks ) policy_directory = os . path . join ( directory , EasyAgent . _KEY_POLICY_DIRECTORY ) policy_directory = bcore . _mkdir ( policy_directory ) self . _backend_agent . save ( directory = policy_directory , callbacks = callbacks ) return directory","title":"save"},{"location":"reference/easyagents/agents/#train_1","text":"def train ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_iterations : int = 20000 , max_steps_per_episode : int = 500 , num_steps_per_iteration : int = 1 , num_steps_buffer_preload = 1000 , num_steps_sampled_from_buffer = 64 , num_iterations_between_eval : int = 1000 , num_episodes_per_eval : int = 10 , learning_rate : float = 0.001 , train_context : easyagents . core . StepsTrainContext = None , default_plots : bool = None ) Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) max_steps_per_episode: maximum number of steps per episode num_steps_per_iteration: number of steps played per training iteration num_steps_buffer_preload: number of initial collect steps to preload the buffer num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training View Source def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 20000 , max_steps_per_episode : int = 500 , num_steps_per_iteration : int = 1 , num_steps_buffer_preload = 1000 , num_steps_sampled_from_buffer = 64 , num_iterations_between_eval : int = 1000 , num_episodes_per_eval : int = 10 , learning_rate : float = 0 . 001 , train_context : core . StepsTrainContext = None , default_plots : bool = None ): \"\"\"Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) max_steps_per_episode: maximum number of steps per episode num_steps_per_iteration: number of steps played per training iteration num_steps_buffer_preload: number of initial collect steps to preload the buffer num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . StepsTrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . num_steps_per_iteration = num_steps_per_iteration train_context . num_steps_buffer_preload = num_steps_buffer_preload train_context . num_steps_sampled_from_buffer = num_steps_sampled_from_buffer train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super (). train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context","title":"train"},{"location":"reference/easyagents/agents/#dqnagent","text":"class DqnAgent ( gym_env_name : str , fc_layers : Union [ Tuple [ int , ... ], int , NoneType ] = None , backend : str = None ) creates a new agent based on the Dqn algorithm. From wikipedia: The DeepMind system used a deep convolutional neural network, with layers of tiled convolutional filters to mimic the effects of receptive fields. Reinforcement learning is unstable or divergent when a nonlinear function approximator such as a neural network is used to represent Q. This instability comes from the correlations present in the sequence of observations, the fact that small updates to Q may significantly change the policy and the data distribution, and the correlations between Q and the target values. The technique used experience replay, a biologically inspired mechanism that uses a random sample of prior actions instead of the most recent action to proceed.[2] This removes correlations in the observation sequence and smooths changes in the data distribution. Iterative update adjusts Q towards target values that are only periodically updated, further reducing correlations with the target.[17] see also: https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning View Source class DqnAgent ( EasyAgent ): \"\"\"creates a new agent based on the Dqn algorithm. From wikipedia: The DeepMind system used a deep convolutional neural network, with layers of tiled convolutional filters to mimic the effects of receptive fields. Reinforcement learning is unstable or divergent when a nonlinear function approximator such as a neural network is used to represent Q. This instability comes from the correlations present in the sequence of observations, the fact that small updates to Q may significantly change the policy and the data distribution, and the correlations between Q and the target values. The technique used experience replay, a biologically inspired mechanism that uses a random sample of prior actions instead of the most recent action to proceed.[2] This removes correlations in the observation sequence and smooths changes in the data distribution. Iterative update adjusts Q towards target values that are only periodically updated, further reducing correlations with the target.[17] see also: https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning \"\"\" def train ( self , callbacks: Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations: int = 20000 , max_steps_per_episode: int = 500 , num_steps_per_iteration: int = 1 , num_steps_buffer_preload = 1000 , num_steps_sampled_from_buffer = 64 , num_iterations_between_eval: int = 1000 , num_episodes_per_eval: int = 10 , learning_rate: float = 0.001 , train_context: core . StepsTrainContext = None , default_plots: bool = None ): \"\"\"Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) max_steps_per_episode: maximum number of steps per episode num_steps_per_iteration: number of steps played per training iteration num_steps_buffer_preload: number of initial collect steps to preload the buffer num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None: train_context = core . StepsTrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . num_steps_per_iteration = num_steps_per_iteration train_context . num_steps_buffer_preload = num_steps_buffer_preload train_context . num_steps_sampled_from_buffer = num_steps_sampled_from_buffer train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super (). train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context","title":"DqnAgent"},{"location":"reference/easyagents/agents/#ancestors-in-mro_2","text":"easyagents.agents.EasyAgent abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/agents/#descendants","text":"easyagents.agents.DoubleDqnAgent easyagents.agents.DuelingDqnAgent easyagents.agents.SacAgent","title":"Descendants"},{"location":"reference/easyagents/agents/#methods_2","text":"","title":"Methods"},{"location":"reference/easyagents/agents/#evaluate_2","text":"def evaluate ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: extensible score metrics View Source def evaluate ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) : \"\"\"Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: extensible score metrics \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) Metrics = namedtuple ( 'Metrics' , 'steps rewards' ) Rewards = namedtuple ( 'Rewards' , 'mean std min max all' ) all_rewards = list ( play_context . sum_of_rewards . values ()) mean_reward , std_reward , min_reward , max_reward = statistics . mean ( all_rewards ), statistics . stdev ( all_rewards ), min ( all_rewards ), max ( all_rewards ) rewards = Rewards ( mean = mean_reward , std = std_reward , min = min_reward , max = max_reward , all = all_rewards ) Steps = namedtuple ( 'Steps' , 'mean std min max all' ) all_num_steps = [] for i in play_context . rewards . keys () : all_num_steps . append ( len ( play_context . rewards [ i ] )) mean_steps , std_steps , min_steps , max_steps = statistics . mean ( all_num_steps ), statistics . stdev ( all_num_steps ), min ( all_num_steps ), max ( all_num_steps ) steps = Steps ( mean = mean_steps , std = std_steps , min = min_steps , max = max_steps , all = all_num_steps ) metrics = Metrics ( rewards = rewards , steps = steps ) return metrics","title":"evaluate"},{"location":"reference/easyagents/agents/#play_2","text":"def play ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : easyagents . core . PlayContext = None , default_plots : bool = None ) Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training View Source def play ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : core . PlayContext = None , default_plots : bool = None ): \"\"\"Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training \"\"\" assert self . _backend_agent . _agent_context . _is_policy_trained , \"No trained policy available. Call train() first.\" if play_context is None : play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes callbacks = self . _to_callback_list ( callbacks = callbacks ) callbacks = self . _add_plot_callbacks ( callbacks , default_plots , [ plot . Steps (), plot . Rewards ()]) self . _backend_agent . play ( play_context = play_context , callbacks = callbacks ) return play_context","title":"play"},{"location":"reference/easyagents/agents/#save_2","text":"def save ( self , directory : Union [ str , NoneType ] = None , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None ) -> str Saves the currently trained actor policy in directory. If save is called before a trained policy is created, eg by calling train, an exception is raised. Args: directory: the directory to save the policy weights to. if the directory does not exist yet, a new directory is created. if None the policy is saved in a temp directory. callbacks: list of callbacks called during save (eg log.Agent) Returns: the absolute path to the directory containing the saved policy. View Source def save ( self , directory : Optional [ str ] = None , callbacks : Union [ List[core.AgentCallback ] , core . AgentCallback , None ] = None ) -> str : \"\"\"Saves the currently trained actor policy in directory. If save is called before a trained policy is created, eg by calling train, an exception is raised. Args: directory: the directory to save the policy weights to. if the directory does not exist yet, a new directory is created. if None the policy is saved in a temp directory. callbacks: list of callbacks called during save (eg log.Agent) Returns: the absolute path to the directory containing the saved policy. \"\"\" if directory is None : directory = bcore . _get_temp_path () assert directory assert self . _backend_agent . _agent_context . _is_policy_trained , \"No trained policy available. Call train() first.\" directory = bcore . _mkdir ( directory ) agent_json_path = os . path . join ( directory , EasyAgent . _KEY_EASYAGENT_FILENAME ) with open ( agent_json_path , 'w' ) as jsonfile : agent_dict = self . _to_dict () json . dump ( agent_dict , jsonfile , sort_keys = True , indent = 2 ) callbacks = self . _to_callback_list ( callbacks = callbacks ) policy_directory = os . path . join ( directory , EasyAgent . _KEY_POLICY_DIRECTORY ) policy_directory = bcore . _mkdir ( policy_directory ) self . _backend_agent . save ( directory = policy_directory , callbacks = callbacks ) return directory","title":"save"},{"location":"reference/easyagents/agents/#train_2","text":"def train ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_iterations : int = 20000 , max_steps_per_episode : int = 500 , num_steps_per_iteration : int = 1 , num_steps_buffer_preload = 1000 , num_steps_sampled_from_buffer = 64 , num_iterations_between_eval : int = 1000 , num_episodes_per_eval : int = 10 , learning_rate : float = 0.001 , train_context : easyagents . core . StepsTrainContext = None , default_plots : bool = None ) Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) max_steps_per_episode: maximum number of steps per episode num_steps_per_iteration: number of steps played per training iteration num_steps_buffer_preload: number of initial collect steps to preload the buffer num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training View Source def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 20000 , max_steps_per_episode : int = 500 , num_steps_per_iteration : int = 1 , num_steps_buffer_preload = 1000 , num_steps_sampled_from_buffer = 64 , num_iterations_between_eval : int = 1000 , num_episodes_per_eval : int = 10 , learning_rate : float = 0 . 001 , train_context : core . StepsTrainContext = None , default_plots : bool = None ): \"\"\"Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) max_steps_per_episode: maximum number of steps per episode num_steps_per_iteration: number of steps played per training iteration num_steps_buffer_preload: number of initial collect steps to preload the buffer num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . StepsTrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . num_steps_per_iteration = num_steps_per_iteration train_context . num_steps_buffer_preload = num_steps_buffer_preload train_context . num_steps_sampled_from_buffer = num_steps_sampled_from_buffer train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super (). train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context","title":"train"},{"location":"reference/easyagents/agents/#duelingdqnagent","text":"class DuelingDqnAgent ( gym_env_name : str , fc_layers : Union [ Tuple [ int , ... ], int , NoneType ] = None , backend : str = None ) Agent based on the Dueling Dqn algorithm (https://arxiv.org/abs/1511.06581). View Source class DuelingDqnAgent ( DqnAgent ): \"\"\"Agent based on the Dueling Dqn algorithm (https://arxiv.org/abs/1511.06581).\"\"\"","title":"DuelingDqnAgent"},{"location":"reference/easyagents/agents/#ancestors-in-mro_3","text":"easyagents.agents.DqnAgent easyagents.agents.EasyAgent abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/agents/#methods_3","text":"","title":"Methods"},{"location":"reference/easyagents/agents/#evaluate_3","text":"def evaluate ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: extensible score metrics View Source def evaluate ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) : \"\"\"Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: extensible score metrics \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) Metrics = namedtuple ( 'Metrics' , 'steps rewards' ) Rewards = namedtuple ( 'Rewards' , 'mean std min max all' ) all_rewards = list ( play_context . sum_of_rewards . values ()) mean_reward , std_reward , min_reward , max_reward = statistics . mean ( all_rewards ), statistics . stdev ( all_rewards ), min ( all_rewards ), max ( all_rewards ) rewards = Rewards ( mean = mean_reward , std = std_reward , min = min_reward , max = max_reward , all = all_rewards ) Steps = namedtuple ( 'Steps' , 'mean std min max all' ) all_num_steps = [] for i in play_context . rewards . keys () : all_num_steps . append ( len ( play_context . rewards [ i ] )) mean_steps , std_steps , min_steps , max_steps = statistics . mean ( all_num_steps ), statistics . stdev ( all_num_steps ), min ( all_num_steps ), max ( all_num_steps ) steps = Steps ( mean = mean_steps , std = std_steps , min = min_steps , max = max_steps , all = all_num_steps ) metrics = Metrics ( rewards = rewards , steps = steps ) return metrics","title":"evaluate"},{"location":"reference/easyagents/agents/#play_3","text":"def play ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : easyagents . core . PlayContext = None , default_plots : bool = None ) Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training View Source def play ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : core . PlayContext = None , default_plots : bool = None ): \"\"\"Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training \"\"\" assert self . _backend_agent . _agent_context . _is_policy_trained , \"No trained policy available. Call train() first.\" if play_context is None : play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes callbacks = self . _to_callback_list ( callbacks = callbacks ) callbacks = self . _add_plot_callbacks ( callbacks , default_plots , [ plot . Steps (), plot . Rewards ()]) self . _backend_agent . play ( play_context = play_context , callbacks = callbacks ) return play_context","title":"play"},{"location":"reference/easyagents/agents/#save_3","text":"def save ( self , directory : Union [ str , NoneType ] = None , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None ) -> str Saves the currently trained actor policy in directory. If save is called before a trained policy is created, eg by calling train, an exception is raised. Args: directory: the directory to save the policy weights to. if the directory does not exist yet, a new directory is created. if None the policy is saved in a temp directory. callbacks: list of callbacks called during save (eg log.Agent) Returns: the absolute path to the directory containing the saved policy. View Source def save ( self , directory : Optional [ str ] = None , callbacks : Union [ List[core.AgentCallback ] , core . AgentCallback , None ] = None ) -> str : \"\"\"Saves the currently trained actor policy in directory. If save is called before a trained policy is created, eg by calling train, an exception is raised. Args: directory: the directory to save the policy weights to. if the directory does not exist yet, a new directory is created. if None the policy is saved in a temp directory. callbacks: list of callbacks called during save (eg log.Agent) Returns: the absolute path to the directory containing the saved policy. \"\"\" if directory is None : directory = bcore . _get_temp_path () assert directory assert self . _backend_agent . _agent_context . _is_policy_trained , \"No trained policy available. Call train() first.\" directory = bcore . _mkdir ( directory ) agent_json_path = os . path . join ( directory , EasyAgent . _KEY_EASYAGENT_FILENAME ) with open ( agent_json_path , 'w' ) as jsonfile : agent_dict = self . _to_dict () json . dump ( agent_dict , jsonfile , sort_keys = True , indent = 2 ) callbacks = self . _to_callback_list ( callbacks = callbacks ) policy_directory = os . path . join ( directory , EasyAgent . _KEY_POLICY_DIRECTORY ) policy_directory = bcore . _mkdir ( policy_directory ) self . _backend_agent . save ( directory = policy_directory , callbacks = callbacks ) return directory","title":"save"},{"location":"reference/easyagents/agents/#train_3","text":"def train ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_iterations : int = 20000 , max_steps_per_episode : int = 500 , num_steps_per_iteration : int = 1 , num_steps_buffer_preload = 1000 , num_steps_sampled_from_buffer = 64 , num_iterations_between_eval : int = 1000 , num_episodes_per_eval : int = 10 , learning_rate : float = 0.001 , train_context : easyagents . core . StepsTrainContext = None , default_plots : bool = None ) Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) max_steps_per_episode: maximum number of steps per episode num_steps_per_iteration: number of steps played per training iteration num_steps_buffer_preload: number of initial collect steps to preload the buffer num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training View Source def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 20000 , max_steps_per_episode : int = 500 , num_steps_per_iteration : int = 1 , num_steps_buffer_preload = 1000 , num_steps_sampled_from_buffer = 64 , num_iterations_between_eval : int = 1000 , num_episodes_per_eval : int = 10 , learning_rate : float = 0 . 001 , train_context : core . StepsTrainContext = None , default_plots : bool = None ): \"\"\"Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) max_steps_per_episode: maximum number of steps per episode num_steps_per_iteration: number of steps played per training iteration num_steps_buffer_preload: number of initial collect steps to preload the buffer num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . StepsTrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . num_steps_per_iteration = num_steps_per_iteration train_context . num_steps_buffer_preload = num_steps_buffer_preload train_context . num_steps_sampled_from_buffer = num_steps_sampled_from_buffer train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super (). train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context","title":"train"},{"location":"reference/easyagents/agents/#easyagent","text":"class EasyAgent ( gym_env_name : str , fc_layers : Union [ Tuple [ int , ... ], int , NoneType ] = None , backend : str = None ) Abstract base class for all easy reinforcment learning agents. Besides forwarding train and play it implements persistence. View Source class EasyAgent ( ABC ) : \"\"\"Abstract base class for all easy reinforcment learning agents. Besides forwarding train and play it implements persistence.\"\"\" _KEY_BACKEND = 'backend' _KEY_EASYAGENT_CLASS = 'easyagent_class' _KEY_EASYAGENT_FILENAME = 'easyagent.json' _KEY_MODEL_CONFIG = 'model_config' _KEY_POLICY_DIRECTORY = 'policy' _KEY_VERSION = 'version' def __init__ ( self , gym_env_name : str , fc_layers : Union [ Tuple[int, ... ] , int , None ] = None , backend : str = None ) : \"\"\" Args: gym_env_name: name of an OpenAI gym environment to be used for training and evaluation fc_layers: defines the neural network to be used, a sequence of fully connected layers of the given size. Eg (75,40) yields a neural network consisting out of 2 hidden layers, the first one containing 75 and the second layer containing 40 neurons. backend=the backend to be used (eg 'tfagents'), if None a default implementation is used. call get_backends() to get a list of the available backends. \"\"\" model_config = core . ModelConfig ( gym_env_name = gym_env_name , fc_layers = fc_layers , seed = seed ) self . _initialize ( model_config = model_config , backend_name = backend ) return def _initialize ( self , model_config : core . ModelConfig , backend_name : str = None ) : if backend_name is None : backend_name = easyagents . backends . default . DefaultAgentFactory . backend_name backend : bcore . BackendAgentFactory = _get_backend ( backend_name ) assert model_config is not None , \"model_config not set.\" assert backend , f 'Backend \"{backend_name}\" not found. The registered backends are {get_backends()}.' self . _model_config : core . ModelConfig = model_config backend_agent = backend . create_agent ( easyagent_type = type ( self ), model_config = model_config ) assert backend_agent , f 'Backend \"{backend_name}\" does not implement \"{type(self).__name__}\". ' + \\ f 'Choose one of the following backend {get_backends(type(self))}.' self . _backend_agent : Optional [ bcore._BackendAgent ] = backend_agent self . _backend_name : str = backend_name self . _backend_agent . _agent_context . _agent_saver = self . save return def _add_plot_callbacks ( self , callbacks : List [ core.AgentCallback ] , default_plots : Optional [ bool ] , default_plot_callbacks : List [ plot._PlotCallback ] ) -> List [ core.AgentCallback ] : \"\"\"Adds the default callbacks and sorts all callbacks in the order _PreProcessCallbacks, AgentCallbacks, _PostProcessCallbacks. Args: callbacks: existing callbacks to prepare default_plots: if set or if None and callbacks does not contain plots then the default plots are added default_plot_callbacks: plot callbacks to add. \"\"\" pre_process : List [ core.AgentCallback ] = [ plot._PreProcess() ] agent : List [ core.AgentCallback ] = [] post_process : List [ core.AgentCallback ] = [ plot._PostProcess() ] if default_plots is None : default_plots = True for c in callbacks : default_plots = default_plots and ( not isinstance ( c , plot . _PlotCallback )) if default_plots : agent = default_plot_callbacks for c in callbacks : if isinstance ( c , core . _PreProcessCallback ) : pre_process . append ( c ) else : if isinstance ( c , core . _PostProcessCallback ) : post_process . append ( c ) else : agent . append ( c ) result : List [ core.AgentCallback ] = pre_process + agent + post_process return result @staticmethod def _from_dict ( param_dict : Dict [ str, object ] ) : \"\"\"recreates a new agent instance according to the definition previously created by _to_dict. Returns: new agent instance (excluding any trained policy), the agent type is preserved. \"\"\" assert param_dict mc : core . ModelConfig = core . ModelConfig . _from_dict ( param_dict [ EasyAgent._KEY_MODEL_CONFIG ] ) agent_class = globals () [ param_dict[EasyAgent._KEY_EASYAGENT_CLASS ] ] backend : str = param_dict [ EasyAgent._KEY_BACKEND ] result = agent_class ( gym_env_name = mc . original_env_name , backend = backend ) result . _initialize ( model_config = mc , backend_name = backend ) return result def _to_callback_list ( self , callbacks : Union [ Optional[core.AgentCallback ] , List [ core.AgentCallback ] ] ) -> List [ core.AgentCallback ] : \"\"\"maps callbacks to an admissible callback list. if callbacks is None an empty list is returned. if callbacks is an AgentCallback a list containing only this callback is returned otherwise callbacks is returned \"\"\" result : List [ core.AgentCallback ] = [] if not callbacks is None : if isinstance ( callbacks , core . AgentCallback ) : result = [ callbacks ] else : assert isinstance ( callbacks , list ), \"callback not an AgentCallback or a list thereof.\" result = callbacks return result def _to_dict ( self ) -> Dict [ str, object ] : \"\"\"saves the agent definition to a dict. Returns: dict containing all parameters to recreate the agent (excluding a trained policy) \"\"\" result : Dict [ str, object ] = dict () result [ EasyAgent._KEY_VERSION ] = easyagents . __version__ result [ EasyAgent._KEY_EASYAGENT_CLASS ] = self . __class__ . __name__ result [ EasyAgent._KEY_BACKEND ] = self . _backend_name result [ EasyAgent._KEY_MODEL_CONFIG ] = self . _model_config . _to_dict () result [ EasyAgent._KEY_POLICY_DIRECTORY ] = 'policy' return result def evaluate ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) : \"\"\"Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: extensible score metrics \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) Metrics = namedtuple ( 'Metrics' , 'steps rewards' ) Rewards = namedtuple ( 'Rewards' , 'mean std min max all' ) all_rewards = list ( play_context . sum_of_rewards . values ()) mean_reward , std_reward , min_reward , max_reward = statistics . mean ( all_rewards ), statistics . stdev ( all_rewards ), min ( all_rewards ), max ( all_rewards ) rewards = Rewards ( mean = mean_reward , std = std_reward , min = min_reward , max = max_reward , all = all_rewards ) Steps = namedtuple ( 'Steps' , 'mean std min max all' ) all_num_steps = [] for i in play_context . rewards . keys () : all_num_steps . append ( len ( play_context . rewards [ i ] )) mean_steps , std_steps , min_steps , max_steps = statistics . mean ( all_num_steps ), statistics . stdev ( all_num_steps ), min ( all_num_steps ), max ( all_num_steps ) steps = Steps ( mean = mean_steps , std = std_steps , min = min_steps , max = max_steps , all = all_num_steps ) metrics = Metrics ( rewards = rewards , steps = steps ) return metrics def play ( self , callbacks : Union [ List[core.AgentCallback ] , core . AgentCallback , None ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : core . PlayContext = None , default_plots : bool = None ) : \"\"\"Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training \"\"\" assert self . _backend_agent . _agent_context . _is_policy_trained , \"No trained policy available. Call train() first.\" if play_context is None : play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes callbacks = self . _to_callback_list ( callbacks = callbacks ) callbacks = self . _add_plot_callbacks ( callbacks , default_plots , [ plot.Steps(), plot.Rewards() ] ) self . _backend_agent . play ( play_context = play_context , callbacks = callbacks ) return play_context def save ( self , directory : Optional [ str ] = None , callbacks : Union [ List[core.AgentCallback ] , core . AgentCallback , None ] = None ) -> str : \"\"\"Saves the currently trained actor policy in directory. If save is called before a trained policy is created, eg by calling train, an exception is raised. Args: directory: the directory to save the policy weights to. if the directory does not exist yet, a new directory is created. if None the policy is saved in a temp directory. callbacks: list of callbacks called during save (eg log.Agent) Returns: the absolute path to the directory containing the saved policy. \"\"\" if directory is None : directory = bcore . _get_temp_path () assert directory assert self . _backend_agent . _agent_context . _is_policy_trained , \"No trained policy available. Call train() first.\" directory = bcore . _mkdir ( directory ) agent_json_path = os . path . join ( directory , EasyAgent . _KEY_EASYAGENT_FILENAME ) with open ( agent_json_path , 'w' ) as jsonfile : agent_dict = self . _to_dict () json . dump ( agent_dict , jsonfile , sort_keys = True , indent = 2 ) callbacks = self . _to_callback_list ( callbacks = callbacks ) policy_directory = os . path . join ( directory , EasyAgent . _KEY_POLICY_DIRECTORY ) policy_directory = bcore . _mkdir ( policy_directory ) self . _backend_agent . save ( directory = policy_directory , callbacks = callbacks ) return directory def train ( self , train_context : core . TrainContext , callbacks : Union [ List[core.AgentCallback ] , core . AgentCallback , None ] , default_plots : Optional [ bool ] ) : \"\"\"Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during the training and evaluation train_context: training configuration to be used (num_iterations,num_episodes_per_iteration,...) default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty \"\"\" assert train_context , \"train_context not set.\" callbacks = self . _to_callback_list ( callbacks = callbacks ) callbacks = self . _add_plot_callbacks ( callbacks , default_plots , [ plot.Loss(), plot.Steps(), plot.Rewards() ] ) self . _backend_agent . train ( train_context = train_context , callbacks = callbacks )","title":"EasyAgent"},{"location":"reference/easyagents/agents/#ancestors-in-mro_4","text":"abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/agents/#descendants_1","text":"easyagents.agents.CemAgent easyagents.agents.DqnAgent easyagents.agents.PpoAgent easyagents.agents.RandomAgent easyagents.agents.ReinforceAgent","title":"Descendants"},{"location":"reference/easyagents/agents/#methods_4","text":"","title":"Methods"},{"location":"reference/easyagents/agents/#evaluate_4","text":"def evaluate ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: extensible score metrics View Source def evaluate ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) : \"\"\"Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: extensible score metrics \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) Metrics = namedtuple ( 'Metrics' , 'steps rewards' ) Rewards = namedtuple ( 'Rewards' , 'mean std min max all' ) all_rewards = list ( play_context . sum_of_rewards . values ()) mean_reward , std_reward , min_reward , max_reward = statistics . mean ( all_rewards ), statistics . stdev ( all_rewards ), min ( all_rewards ), max ( all_rewards ) rewards = Rewards ( mean = mean_reward , std = std_reward , min = min_reward , max = max_reward , all = all_rewards ) Steps = namedtuple ( 'Steps' , 'mean std min max all' ) all_num_steps = [] for i in play_context . rewards . keys () : all_num_steps . append ( len ( play_context . rewards [ i ] )) mean_steps , std_steps , min_steps , max_steps = statistics . mean ( all_num_steps ), statistics . stdev ( all_num_steps ), min ( all_num_steps ), max ( all_num_steps ) steps = Steps ( mean = mean_steps , std = std_steps , min = min_steps , max = max_steps , all = all_num_steps ) metrics = Metrics ( rewards = rewards , steps = steps ) return metrics","title":"evaluate"},{"location":"reference/easyagents/agents/#play_4","text":"def play ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : easyagents . core . PlayContext = None , default_plots : bool = None ) Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training View Source def play ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : core . PlayContext = None , default_plots : bool = None ): \"\"\"Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training \"\"\" assert self . _backend_agent . _agent_context . _is_policy_trained , \"No trained policy available. Call train() first.\" if play_context is None : play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes callbacks = self . _to_callback_list ( callbacks = callbacks ) callbacks = self . _add_plot_callbacks ( callbacks , default_plots , [ plot . Steps (), plot . Rewards ()]) self . _backend_agent . play ( play_context = play_context , callbacks = callbacks ) return play_context","title":"play"},{"location":"reference/easyagents/agents/#save_4","text":"def save ( self , directory : Union [ str , NoneType ] = None , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None ) -> str Saves the currently trained actor policy in directory. If save is called before a trained policy is created, eg by calling train, an exception is raised. Args: directory: the directory to save the policy weights to. if the directory does not exist yet, a new directory is created. if None the policy is saved in a temp directory. callbacks: list of callbacks called during save (eg log.Agent) Returns: the absolute path to the directory containing the saved policy. View Source def save ( self , directory : Optional [ str ] = None , callbacks : Union [ List[core.AgentCallback ] , core . AgentCallback , None ] = None ) -> str : \"\"\"Saves the currently trained actor policy in directory. If save is called before a trained policy is created, eg by calling train, an exception is raised. Args: directory: the directory to save the policy weights to. if the directory does not exist yet, a new directory is created. if None the policy is saved in a temp directory. callbacks: list of callbacks called during save (eg log.Agent) Returns: the absolute path to the directory containing the saved policy. \"\"\" if directory is None : directory = bcore . _get_temp_path () assert directory assert self . _backend_agent . _agent_context . _is_policy_trained , \"No trained policy available. Call train() first.\" directory = bcore . _mkdir ( directory ) agent_json_path = os . path . join ( directory , EasyAgent . _KEY_EASYAGENT_FILENAME ) with open ( agent_json_path , 'w' ) as jsonfile : agent_dict = self . _to_dict () json . dump ( agent_dict , jsonfile , sort_keys = True , indent = 2 ) callbacks = self . _to_callback_list ( callbacks = callbacks ) policy_directory = os . path . join ( directory , EasyAgent . _KEY_POLICY_DIRECTORY ) policy_directory = bcore . _mkdir ( policy_directory ) self . _backend_agent . save ( directory = policy_directory , callbacks = callbacks ) return directory","title":"save"},{"location":"reference/easyagents/agents/#train_4","text":"def train ( self , train_context : easyagents . core . TrainContext , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ], default_plots : Union [ bool , NoneType ] ) Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during the training and evaluation train_context: training configuration to be used (num_iterations,num_episodes_per_iteration,...) default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty View Source def train ( self , train_context : core . TrainContext , callbacks : Union [ List[core.AgentCallback ] , core . AgentCallback , None ] , default_plots : Optional [ bool ] ) : \"\"\"Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during the training and evaluation train_context: training configuration to be used (num_iterations,num_episodes_per_iteration,...) default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty \"\"\" assert train_context , \"train_context not set.\" callbacks = self . _to_callback_list ( callbacks = callbacks ) callbacks = self . _add_plot_callbacks ( callbacks , default_plots , [ plot.Loss(), plot.Steps(), plot.Rewards() ] ) self . _backend_agent . train ( train_context = train_context , callbacks = callbacks )","title":"train"},{"location":"reference/easyagents/agents/#ppoagent","text":"class PpoAgent ( gym_env_name : str , fc_layers : Union [ Tuple [ int , ... ], int , NoneType ] = None , backend : str = None ) creates a new agent based on the PPO algorithm. PPO is an actor-critic algorithm using 2 neural networks. The actor network to predict the next action to be taken and the critic network to estimate the value of the game state we are currently in (the expected, discounted sum of future rewards when following the current actor network). see also: https://spinningup.openai.com/en/latest/algorithms/ppo.html View Source class PpoAgent ( EasyAgent ): \"\"\"creates a new agent based on the PPO algorithm. PPO is an actor-critic algorithm using 2 neural networks. The actor network to predict the next action to be taken and the critic network to estimate the value of the game state we are currently in (the expected, discounted sum of future rewards when following the current actor network). see also: https://spinningup.openai.com/en/latest/algorithms/ppo.html \"\"\" def train ( self , callbacks: Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations: int = 100 , num_episodes_per_iteration: int = 10 , max_steps_per_episode: int = 500 , num_epochs_per_iteration: int = 10 , num_iterations_between_eval: int = 5 , num_episodes_per_eval: int = 10 , learning_rate: float = 0.001 , train_context: core . PpoTrainContext = None , default_plots: bool = None ): \"\"\"Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) num_episodes_per_iteration: number of episodes played per training iteration max_steps_per_episode: maximum number of steps per episode num_epochs_per_iteration: number of times the data collected for the current iteration is used to retrain the current policy num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None: train_context = core . PpoTrainContext () train_context . num_iterations = num_iterations train_context . num_episodes_per_iteration = num_episodes_per_iteration train_context . max_steps_per_episode = max_steps_per_episode train_context . num_epochs_per_iteration = num_epochs_per_iteration train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super (). train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context","title":"PpoAgent"},{"location":"reference/easyagents/agents/#ancestors-in-mro_5","text":"easyagents.agents.EasyAgent abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/agents/#methods_5","text":"","title":"Methods"},{"location":"reference/easyagents/agents/#evaluate_5","text":"def evaluate ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: extensible score metrics View Source def evaluate ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) : \"\"\"Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: extensible score metrics \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) Metrics = namedtuple ( 'Metrics' , 'steps rewards' ) Rewards = namedtuple ( 'Rewards' , 'mean std min max all' ) all_rewards = list ( play_context . sum_of_rewards . values ()) mean_reward , std_reward , min_reward , max_reward = statistics . mean ( all_rewards ), statistics . stdev ( all_rewards ), min ( all_rewards ), max ( all_rewards ) rewards = Rewards ( mean = mean_reward , std = std_reward , min = min_reward , max = max_reward , all = all_rewards ) Steps = namedtuple ( 'Steps' , 'mean std min max all' ) all_num_steps = [] for i in play_context . rewards . keys () : all_num_steps . append ( len ( play_context . rewards [ i ] )) mean_steps , std_steps , min_steps , max_steps = statistics . mean ( all_num_steps ), statistics . stdev ( all_num_steps ), min ( all_num_steps ), max ( all_num_steps ) steps = Steps ( mean = mean_steps , std = std_steps , min = min_steps , max = max_steps , all = all_num_steps ) metrics = Metrics ( rewards = rewards , steps = steps ) return metrics","title":"evaluate"},{"location":"reference/easyagents/agents/#play_5","text":"def play ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : easyagents . core . PlayContext = None , default_plots : bool = None ) Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training View Source def play ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : core . PlayContext = None , default_plots : bool = None ): \"\"\"Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training \"\"\" assert self . _backend_agent . _agent_context . _is_policy_trained , \"No trained policy available. Call train() first.\" if play_context is None : play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes callbacks = self . _to_callback_list ( callbacks = callbacks ) callbacks = self . _add_plot_callbacks ( callbacks , default_plots , [ plot . Steps (), plot . Rewards ()]) self . _backend_agent . play ( play_context = play_context , callbacks = callbacks ) return play_context","title":"play"},{"location":"reference/easyagents/agents/#save_5","text":"def save ( self , directory : Union [ str , NoneType ] = None , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None ) -> str Saves the currently trained actor policy in directory. If save is called before a trained policy is created, eg by calling train, an exception is raised. Args: directory: the directory to save the policy weights to. if the directory does not exist yet, a new directory is created. if None the policy is saved in a temp directory. callbacks: list of callbacks called during save (eg log.Agent) Returns: the absolute path to the directory containing the saved policy. View Source def save ( self , directory : Optional [ str ] = None , callbacks : Union [ List[core.AgentCallback ] , core . AgentCallback , None ] = None ) -> str : \"\"\"Saves the currently trained actor policy in directory. If save is called before a trained policy is created, eg by calling train, an exception is raised. Args: directory: the directory to save the policy weights to. if the directory does not exist yet, a new directory is created. if None the policy is saved in a temp directory. callbacks: list of callbacks called during save (eg log.Agent) Returns: the absolute path to the directory containing the saved policy. \"\"\" if directory is None : directory = bcore . _get_temp_path () assert directory assert self . _backend_agent . _agent_context . _is_policy_trained , \"No trained policy available. Call train() first.\" directory = bcore . _mkdir ( directory ) agent_json_path = os . path . join ( directory , EasyAgent . _KEY_EASYAGENT_FILENAME ) with open ( agent_json_path , 'w' ) as jsonfile : agent_dict = self . _to_dict () json . dump ( agent_dict , jsonfile , sort_keys = True , indent = 2 ) callbacks = self . _to_callback_list ( callbacks = callbacks ) policy_directory = os . path . join ( directory , EasyAgent . _KEY_POLICY_DIRECTORY ) policy_directory = bcore . _mkdir ( policy_directory ) self . _backend_agent . save ( directory = policy_directory , callbacks = callbacks ) return directory","title":"save"},{"location":"reference/easyagents/agents/#train_5","text":"def train ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_iterations : int = 100 , num_episodes_per_iteration : int = 10 , max_steps_per_episode : int = 500 , num_epochs_per_iteration : int = 10 , num_iterations_between_eval : int = 5 , num_episodes_per_eval : int = 10 , learning_rate : float = 0.001 , train_context : easyagents . core . PpoTrainContext = None , default_plots : bool = None ) Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) num_episodes_per_iteration: number of episodes played per training iteration max_steps_per_episode: maximum number of steps per episode num_epochs_per_iteration: number of times the data collected for the current iteration is used to retrain the current policy num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training View Source def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 100 , num_episodes_per_iteration : int = 10 , max_steps_per_episode : int = 500 , num_epochs_per_iteration : int = 10 , num_iterations_between_eval : int = 5 , num_episodes_per_eval : int = 10 , learning_rate : float = 0 . 001 , train_context : core . PpoTrainContext = None , default_plots : bool = None ): \"\"\"Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) num_episodes_per_iteration: number of episodes played per training iteration max_steps_per_episode: maximum number of steps per episode num_epochs_per_iteration: number of times the data collected for the current iteration is used to retrain the current policy num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . PpoTrainContext () train_context . num_iterations = num_iterations train_context . num_episodes_per_iteration = num_episodes_per_iteration train_context . max_steps_per_episode = max_steps_per_episode train_context . num_epochs_per_iteration = num_epochs_per_iteration train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super (). train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context","title":"train"},{"location":"reference/easyagents/agents/#randomagent","text":"class RandomAgent ( gym_env_name : str , fc_layers : Union [ Tuple [ int , ... ], int , NoneType ] = None , backend : str = None ) Agent which always chooses uniform random actions. View Source class RandomAgent ( EasyAgent ): \"\"\"Agent which always chooses uniform random actions.\"\"\" def train ( self , callbacks: Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations: int = 10 , max_steps_per_episode: int = 1000 , num_episodes_per_eval: int = 10 , train_context: core . TrainContext = None , default_plots: bool = None ): \"\"\"Evaluates the environment using a uniform random policy. The evaluation is performed in batches of num_episodes_per_eval episodes. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times a batch of num_episodes_per_eval episodes is evaluated. max_steps_per_episode: maximum number of steps per episode num_episodes_per_eval: number of episodes played to estimate the average return and steps train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...) Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None: train_context = core . TrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . num_epochs_per_iteration = 0 train_context . num_iterations_between_eval = 1 train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = 1 super (). train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context","title":"RandomAgent"},{"location":"reference/easyagents/agents/#ancestors-in-mro_6","text":"easyagents.agents.EasyAgent abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/agents/#methods_6","text":"","title":"Methods"},{"location":"reference/easyagents/agents/#evaluate_6","text":"def evaluate ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: extensible score metrics View Source def evaluate ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) : \"\"\"Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: extensible score metrics \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) Metrics = namedtuple ( 'Metrics' , 'steps rewards' ) Rewards = namedtuple ( 'Rewards' , 'mean std min max all' ) all_rewards = list ( play_context . sum_of_rewards . values ()) mean_reward , std_reward , min_reward , max_reward = statistics . mean ( all_rewards ), statistics . stdev ( all_rewards ), min ( all_rewards ), max ( all_rewards ) rewards = Rewards ( mean = mean_reward , std = std_reward , min = min_reward , max = max_reward , all = all_rewards ) Steps = namedtuple ( 'Steps' , 'mean std min max all' ) all_num_steps = [] for i in play_context . rewards . keys () : all_num_steps . append ( len ( play_context . rewards [ i ] )) mean_steps , std_steps , min_steps , max_steps = statistics . mean ( all_num_steps ), statistics . stdev ( all_num_steps ), min ( all_num_steps ), max ( all_num_steps ) steps = Steps ( mean = mean_steps , std = std_steps , min = min_steps , max = max_steps , all = all_num_steps ) metrics = Metrics ( rewards = rewards , steps = steps ) return metrics","title":"evaluate"},{"location":"reference/easyagents/agents/#play_6","text":"def play ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : easyagents . core . PlayContext = None , default_plots : bool = None ) Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training View Source def play ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : core . PlayContext = None , default_plots : bool = None ): \"\"\"Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training \"\"\" assert self . _backend_agent . _agent_context . _is_policy_trained , \"No trained policy available. Call train() first.\" if play_context is None : play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes callbacks = self . _to_callback_list ( callbacks = callbacks ) callbacks = self . _add_plot_callbacks ( callbacks , default_plots , [ plot . Steps (), plot . Rewards ()]) self . _backend_agent . play ( play_context = play_context , callbacks = callbacks ) return play_context","title":"play"},{"location":"reference/easyagents/agents/#save_6","text":"def save ( self , directory : Union [ str , NoneType ] = None , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None ) -> str Saves the currently trained actor policy in directory. If save is called before a trained policy is created, eg by calling train, an exception is raised. Args: directory: the directory to save the policy weights to. if the directory does not exist yet, a new directory is created. if None the policy is saved in a temp directory. callbacks: list of callbacks called during save (eg log.Agent) Returns: the absolute path to the directory containing the saved policy. View Source def save ( self , directory : Optional [ str ] = None , callbacks : Union [ List[core.AgentCallback ] , core . AgentCallback , None ] = None ) -> str : \"\"\"Saves the currently trained actor policy in directory. If save is called before a trained policy is created, eg by calling train, an exception is raised. Args: directory: the directory to save the policy weights to. if the directory does not exist yet, a new directory is created. if None the policy is saved in a temp directory. callbacks: list of callbacks called during save (eg log.Agent) Returns: the absolute path to the directory containing the saved policy. \"\"\" if directory is None : directory = bcore . _get_temp_path () assert directory assert self . _backend_agent . _agent_context . _is_policy_trained , \"No trained policy available. Call train() first.\" directory = bcore . _mkdir ( directory ) agent_json_path = os . path . join ( directory , EasyAgent . _KEY_EASYAGENT_FILENAME ) with open ( agent_json_path , 'w' ) as jsonfile : agent_dict = self . _to_dict () json . dump ( agent_dict , jsonfile , sort_keys = True , indent = 2 ) callbacks = self . _to_callback_list ( callbacks = callbacks ) policy_directory = os . path . join ( directory , EasyAgent . _KEY_POLICY_DIRECTORY ) policy_directory = bcore . _mkdir ( policy_directory ) self . _backend_agent . save ( directory = policy_directory , callbacks = callbacks ) return directory","title":"save"},{"location":"reference/easyagents/agents/#train_6","text":"def train ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_iterations : int = 10 , max_steps_per_episode : int = 1000 , num_episodes_per_eval : int = 10 , train_context : easyagents . core . TrainContext = None , default_plots : bool = None ) Evaluates the environment using a uniform random policy. The evaluation is performed in batches of num_episodes_per_eval episodes. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times a batch of num_episodes_per_eval episodes is evaluated. max_steps_per_episode: maximum number of steps per episode num_episodes_per_eval: number of episodes played to estimate the average return and steps train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...) Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training View Source def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 10 , max_steps_per_episode : int = 1000 , num_episodes_per_eval : int = 10 , train_context : core . TrainContext = None , default_plots : bool = None ): \"\"\"Evaluates the environment using a uniform random policy. The evaluation is performed in batches of num_episodes_per_eval episodes. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times a batch of num_episodes_per_eval episodes is evaluated. max_steps_per_episode: maximum number of steps per episode num_episodes_per_eval: number of episodes played to estimate the average return and steps train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...) Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . TrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . num_epochs_per_iteration = 0 train_context . num_iterations_between_eval = 1 train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = 1 super (). train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context","title":"train"},{"location":"reference/easyagents/agents/#reinforceagent","text":"class ReinforceAgent ( gym_env_name : str , fc_layers : Union [ Tuple [ int , ... ], int , NoneType ] = None , backend : str = None ) creates a new agent based on the Reinforce algorithm. Reinforce is a vanilla policy gradient algorithm using a single actor network. see also: www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf View Source class ReinforceAgent ( EasyAgent ): \"\"\"creates a new agent based on the Reinforce algorithm. Reinforce is a vanilla policy gradient algorithm using a single actor network. see also: www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf \"\"\" def train ( self , callbacks: Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations: int = 100 , num_episodes_per_iteration: int = 10 , max_steps_per_episode: int = 500 , num_epochs_per_iteration: int = 10 , num_iterations_between_eval: int = 5 , num_episodes_per_eval: int = 10 , learning_rate: float = 0.001 , train_context: core . EpisodesTrainContext = None , default_plots: bool = None ): \"\"\"Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) num_episodes_per_iteration: number of episodes played per training iteration max_steps_per_episode: maximum number of steps per episode num_epochs_per_iteration: number of times the data collected for the current iteration is used to retrain the current policy num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None: train_context = core . EpisodesTrainContext () train_context . num_iterations = num_iterations train_context . num_episodes_per_iteration = num_episodes_per_iteration train_context . max_steps_per_episode = max_steps_per_episode train_context . num_epochs_per_iteration = num_epochs_per_iteration train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super (). train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context","title":"ReinforceAgent"},{"location":"reference/easyagents/agents/#ancestors-in-mro_7","text":"easyagents.agents.EasyAgent abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/agents/#methods_7","text":"","title":"Methods"},{"location":"reference/easyagents/agents/#evaluate_7","text":"def evaluate ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: extensible score metrics View Source def evaluate ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) : \"\"\"Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: extensible score metrics \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) Metrics = namedtuple ( 'Metrics' , 'steps rewards' ) Rewards = namedtuple ( 'Rewards' , 'mean std min max all' ) all_rewards = list ( play_context . sum_of_rewards . values ()) mean_reward , std_reward , min_reward , max_reward = statistics . mean ( all_rewards ), statistics . stdev ( all_rewards ), min ( all_rewards ), max ( all_rewards ) rewards = Rewards ( mean = mean_reward , std = std_reward , min = min_reward , max = max_reward , all = all_rewards ) Steps = namedtuple ( 'Steps' , 'mean std min max all' ) all_num_steps = [] for i in play_context . rewards . keys () : all_num_steps . append ( len ( play_context . rewards [ i ] )) mean_steps , std_steps , min_steps , max_steps = statistics . mean ( all_num_steps ), statistics . stdev ( all_num_steps ), min ( all_num_steps ), max ( all_num_steps ) steps = Steps ( mean = mean_steps , std = std_steps , min = min_steps , max = max_steps , all = all_num_steps ) metrics = Metrics ( rewards = rewards , steps = steps ) return metrics","title":"evaluate"},{"location":"reference/easyagents/agents/#play_7","text":"def play ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : easyagents . core . PlayContext = None , default_plots : bool = None ) Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training View Source def play ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : core . PlayContext = None , default_plots : bool = None ): \"\"\"Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training \"\"\" assert self . _backend_agent . _agent_context . _is_policy_trained , \"No trained policy available. Call train() first.\" if play_context is None : play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes callbacks = self . _to_callback_list ( callbacks = callbacks ) callbacks = self . _add_plot_callbacks ( callbacks , default_plots , [ plot . Steps (), plot . Rewards ()]) self . _backend_agent . play ( play_context = play_context , callbacks = callbacks ) return play_context","title":"play"},{"location":"reference/easyagents/agents/#save_7","text":"def save ( self , directory : Union [ str , NoneType ] = None , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None ) -> str Saves the currently trained actor policy in directory. If save is called before a trained policy is created, eg by calling train, an exception is raised. Args: directory: the directory to save the policy weights to. if the directory does not exist yet, a new directory is created. if None the policy is saved in a temp directory. callbacks: list of callbacks called during save (eg log.Agent) Returns: the absolute path to the directory containing the saved policy. View Source def save ( self , directory : Optional [ str ] = None , callbacks : Union [ List[core.AgentCallback ] , core . AgentCallback , None ] = None ) -> str : \"\"\"Saves the currently trained actor policy in directory. If save is called before a trained policy is created, eg by calling train, an exception is raised. Args: directory: the directory to save the policy weights to. if the directory does not exist yet, a new directory is created. if None the policy is saved in a temp directory. callbacks: list of callbacks called during save (eg log.Agent) Returns: the absolute path to the directory containing the saved policy. \"\"\" if directory is None : directory = bcore . _get_temp_path () assert directory assert self . _backend_agent . _agent_context . _is_policy_trained , \"No trained policy available. Call train() first.\" directory = bcore . _mkdir ( directory ) agent_json_path = os . path . join ( directory , EasyAgent . _KEY_EASYAGENT_FILENAME ) with open ( agent_json_path , 'w' ) as jsonfile : agent_dict = self . _to_dict () json . dump ( agent_dict , jsonfile , sort_keys = True , indent = 2 ) callbacks = self . _to_callback_list ( callbacks = callbacks ) policy_directory = os . path . join ( directory , EasyAgent . _KEY_POLICY_DIRECTORY ) policy_directory = bcore . _mkdir ( policy_directory ) self . _backend_agent . save ( directory = policy_directory , callbacks = callbacks ) return directory","title":"save"},{"location":"reference/easyagents/agents/#train_7","text":"def train ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_iterations : int = 100 , num_episodes_per_iteration : int = 10 , max_steps_per_episode : int = 500 , num_epochs_per_iteration : int = 10 , num_iterations_between_eval : int = 5 , num_episodes_per_eval : int = 10 , learning_rate : float = 0.001 , train_context : easyagents . core . EpisodesTrainContext = None , default_plots : bool = None ) Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) num_episodes_per_iteration: number of episodes played per training iteration max_steps_per_episode: maximum number of steps per episode num_epochs_per_iteration: number of times the data collected for the current iteration is used to retrain the current policy num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training View Source def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 100 , num_episodes_per_iteration : int = 10 , max_steps_per_episode : int = 500 , num_epochs_per_iteration : int = 10 , num_iterations_between_eval : int = 5 , num_episodes_per_eval : int = 10 , learning_rate : float = 0 . 001 , train_context : core . EpisodesTrainContext = None , default_plots : bool = None ): \"\"\"Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) num_episodes_per_iteration: number of episodes played per training iteration max_steps_per_episode: maximum number of steps per episode num_epochs_per_iteration: number of times the data collected for the current iteration is used to retrain the current policy num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . EpisodesTrainContext () train_context . num_iterations = num_iterations train_context . num_episodes_per_iteration = num_episodes_per_iteration train_context . max_steps_per_episode = max_steps_per_episode train_context . num_epochs_per_iteration = num_epochs_per_iteration train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super (). train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context","title":"train"},{"location":"reference/easyagents/agents/#sacagent","text":"class SacAgent ( gym_env_name : str , fc_layers : Union [ Tuple [ int , ... ], int , NoneType ] = None , backend : str = None ) Agent based on the Soft-Actor-Critic algorithm (https://arxiv.org/abs/1812.05905). View Source class SacAgent ( DqnAgent ): \"\"\"Agent based on the Soft-Actor-Critic algorithm (https://arxiv.org/abs/1812.05905).\"\"\"","title":"SacAgent"},{"location":"reference/easyagents/agents/#ancestors-in-mro_8","text":"easyagents.agents.DqnAgent easyagents.agents.EasyAgent abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/agents/#methods_8","text":"","title":"Methods"},{"location":"reference/easyagents/agents/#evaluate_8","text":"def evaluate ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: extensible score metrics View Source def evaluate ( self , num_episodes : int = 50 , max_steps_per_episode : int = 50 ) : \"\"\"Plays num_episodes with the current policy and computes metrics on rewards. Args: num_episodes: number of episodes to play max_steps_per_episode: max steps per episode Returns: extensible score metrics \"\"\" play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes self . play ( play_context = play_context , default_plots = False ) Metrics = namedtuple ( 'Metrics' , 'steps rewards' ) Rewards = namedtuple ( 'Rewards' , 'mean std min max all' ) all_rewards = list ( play_context . sum_of_rewards . values ()) mean_reward , std_reward , min_reward , max_reward = statistics . mean ( all_rewards ), statistics . stdev ( all_rewards ), min ( all_rewards ), max ( all_rewards ) rewards = Rewards ( mean = mean_reward , std = std_reward , min = min_reward , max = max_reward , all = all_rewards ) Steps = namedtuple ( 'Steps' , 'mean std min max all' ) all_num_steps = [] for i in play_context . rewards . keys () : all_num_steps . append ( len ( play_context . rewards [ i ] )) mean_steps , std_steps , min_steps , max_steps = statistics . mean ( all_num_steps ), statistics . stdev ( all_num_steps ), min ( all_num_steps ), max ( all_num_steps ) steps = Steps ( mean = mean_steps , std = std_steps , min = min_steps , max = max_steps , all = all_num_steps ) metrics = Metrics ( rewards = rewards , steps = steps ) return metrics","title":"evaluate"},{"location":"reference/easyagents/agents/#play_8","text":"def play ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : easyagents . core . PlayContext = None , default_plots : bool = None ) Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training View Source def play ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_episodes : int = 1 , max_steps_per_episode : int = 1000 , play_context : core . PlayContext = None , default_plots : bool = None ): \"\"\"Plays num_episodes with the current policy. Args: callbacks: list of callbacks called during each episode play num_episodes: number of episodes to play max_steps_per_episode: max steps per episode play_context: play configuration to be used. If set override all other play context arguments default_plots: if set addes a set of default callbacks (plot.State, plot.Rewards, ...) Returns: play_context containg the actions taken and the rewards received during training \"\"\" assert self . _backend_agent . _agent_context . _is_policy_trained , \"No trained policy available. Call train() first.\" if play_context is None : play_context = core . PlayContext () play_context . max_steps_per_episode = max_steps_per_episode play_context . num_episodes = num_episodes callbacks = self . _to_callback_list ( callbacks = callbacks ) callbacks = self . _add_plot_callbacks ( callbacks , default_plots , [ plot . Steps (), plot . Rewards ()]) self . _backend_agent . play ( play_context = play_context , callbacks = callbacks ) return play_context","title":"play"},{"location":"reference/easyagents/agents/#save_8","text":"def save ( self , directory : Union [ str , NoneType ] = None , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None ) -> str Saves the currently trained actor policy in directory. If save is called before a trained policy is created, eg by calling train, an exception is raised. Args: directory: the directory to save the policy weights to. if the directory does not exist yet, a new directory is created. if None the policy is saved in a temp directory. callbacks: list of callbacks called during save (eg log.Agent) Returns: the absolute path to the directory containing the saved policy. View Source def save ( self , directory : Optional [ str ] = None , callbacks : Union [ List[core.AgentCallback ] , core . AgentCallback , None ] = None ) -> str : \"\"\"Saves the currently trained actor policy in directory. If save is called before a trained policy is created, eg by calling train, an exception is raised. Args: directory: the directory to save the policy weights to. if the directory does not exist yet, a new directory is created. if None the policy is saved in a temp directory. callbacks: list of callbacks called during save (eg log.Agent) Returns: the absolute path to the directory containing the saved policy. \"\"\" if directory is None : directory = bcore . _get_temp_path () assert directory assert self . _backend_agent . _agent_context . _is_policy_trained , \"No trained policy available. Call train() first.\" directory = bcore . _mkdir ( directory ) agent_json_path = os . path . join ( directory , EasyAgent . _KEY_EASYAGENT_FILENAME ) with open ( agent_json_path , 'w' ) as jsonfile : agent_dict = self . _to_dict () json . dump ( agent_dict , jsonfile , sort_keys = True , indent = 2 ) callbacks = self . _to_callback_list ( callbacks = callbacks ) policy_directory = os . path . join ( directory , EasyAgent . _KEY_POLICY_DIRECTORY ) policy_directory = bcore . _mkdir ( policy_directory ) self . _backend_agent . save ( directory = policy_directory , callbacks = callbacks ) return directory","title":"save"},{"location":"reference/easyagents/agents/#train_8","text":"def train ( self , callbacks : Union [ List [ easyagents . core . AgentCallback ], easyagents . core . AgentCallback , NoneType ] = None , num_iterations : int = 20000 , max_steps_per_episode : int = 500 , num_steps_per_iteration : int = 1 , num_steps_buffer_preload = 1000 , num_steps_sampled_from_buffer = 64 , num_iterations_between_eval : int = 1000 , num_episodes_per_eval : int = 10 , learning_rate : float = 0.001 , train_context : easyagents . core . StepsTrainContext = None , default_plots : bool = None ) Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) max_steps_per_episode: maximum number of steps per episode num_steps_per_iteration: number of steps played per training iteration num_steps_buffer_preload: number of initial collect steps to preload the buffer num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training View Source def train ( self , callbacks : Union [ List [ core . AgentCallback ], core . AgentCallback , None ] = None , num_iterations : int = 20000 , max_steps_per_episode : int = 500 , num_steps_per_iteration : int = 1 , num_steps_buffer_preload = 1000 , num_steps_sampled_from_buffer = 64 , num_iterations_between_eval : int = 1000 , num_episodes_per_eval : int = 10 , learning_rate : float = 0 . 001 , train_context : core . StepsTrainContext = None , default_plots : bool = None ): \"\"\"Trains a new model using the gym environment passed during instantiation. Args: callbacks: list of callbacks called during training and evaluation num_iterations: number of times the training is repeated (with additional data) max_steps_per_episode: maximum number of steps per episode num_steps_per_iteration: number of steps played per training iteration num_steps_buffer_preload: number of initial collect steps to preload the buffer num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training num_iterations_between_eval: number of training iterations before the current policy is evaluated. if 0 no evaluation is performed. num_episodes_per_eval: number of episodes played to estimate the average return and steps learning_rate: the learning rate used in the next iteration's policy training (0,1] train_context: training configuration to be used. if set overrides all other training context arguments. default_plots: if set adds a set of default callbacks (plot.State, plot.Rewards, plot.Loss,...). if None default callbacks are only added if the callbacks list is empty Returns: train_context: the training configuration containing the loss and sum of rewards encountered during training \"\"\" if train_context is None : train_context = core . StepsTrainContext () train_context . num_iterations = num_iterations train_context . max_steps_per_episode = max_steps_per_episode train_context . num_steps_per_iteration = num_steps_per_iteration train_context . num_steps_buffer_preload = num_steps_buffer_preload train_context . num_steps_sampled_from_buffer = num_steps_sampled_from_buffer train_context . num_iterations_between_eval = num_iterations_between_eval train_context . num_episodes_per_eval = num_episodes_per_eval train_context . learning_rate = learning_rate super (). train ( train_context = train_context , callbacks = callbacks , default_plots = default_plots ) return train_context","title":"train"},{"location":"reference/easyagents/core/","text":"Module easyagents.core This module contains the core datastructures shared between fronten and backend like the definition of all callbacks and agent configurations. View Source \"\"\"This module contains the core datastructures shared between fronten and backend like the definition of all callbacks and agent configurations. \"\"\" from abc import ABC from typing import Callable , Optional , Dict , Tuple , List , Union from enum import Flag , auto import math import easyagents.env import easyagents.backends.monitor import gym.core import matplotlib.pyplot as plt class GymContext ( object ): \"\"\"Contains the context for gym api calls (wrapping a gym env instance).\"\"\" def __init__ ( self ): self . _monitor_env : Optional [ easyagents . backends . monitor . _MonitorEnv ] = None self . _totals = None def __str__ ( self ): return f 'MonitorEnv={self._monitor_env} Totals={self._totals}' @property def gym_env ( self ) -> Optional [ gym . core . Env ]: result = None if self . _monitor_env : result = self . _monitor_env . env return result class PlotType ( Flag ): \"\"\"Defines the point in time when a plot is created / updated. NONE: No plot is updated. PLAY_EPISODE: Called after the last step of each played episode. The gym environment is still accessible through agent_context.play-gym_env. PLAY_STEP: Called after each play step. The gym environment is still accessible through agent_context.play-gym_env. TRAIN_EVAL: Called after the last step of the last evaluation episode during training. The gym environment is accessible through agent_context.play.gym_env. TRAIN_ITERATION: Called after each train iteration. No gym environment is available. \"\"\" NONE = 0 PLAY_EPISODE = auto () PLAY_STEP = auto () TRAIN_EVAL = auto () TRAIN_ITERATION = auto () class PyPlotContext ( object ): \"\"\"Contain the context for the maplotlib.pyplot figure plotting. Attributes figure: the figure to plot to figsize: figure (width,height) in inches for the figure to be created. is_jupyter_active: True if we plot to jupyter notebook cell, False otherwise. max_columns: the max number of subplot columns in the pyplot figure \"\"\" def __init__ ( self ): self . _created_subplots = PlotType . NONE self . figure : Optional [ plt . Figure ] = None self . figsize : ( float , float ) = ( 17 , 6 ) self . _call_jupyter_display = False self . is_jupyter_active = False self . max_columns = 3 def __str__ ( self ): figure_number = None figure_axes_len = 0 if self . figure : figure_number = self . figure . number if self . figure . axes : figure_axes_len = len ( self . figure . axes ) return f 'is_jupyter_active={self.is_jupyter_active} max_columns={self.max_columns} ' + \\ f '_created_subplots={self._created_subplots} figure={figure_number} axes={figure_axes_len} ' def _is_subplot_created ( self , plot_type : PlotType ): \"\"\"Yields true if a subplot of type plot_type was created by a plot callback.\"\"\" result = (( self . _created_subplots & plot_type ) != PlotType . NONE ) return result class ModelConfig ( object ): \"\"\"The model configurations, containing the name of the gym environment and the neural network architecture. Attributes: original_env_name: the name of the underlying gym environment, eg 'CartPole-v0' gym_env_name: the name of the actual gym environment used (a wrapper around the environment given by original_env_name) fc_layers: int tuple defining the number and size of each fully connected layer. seed: the seed to be used for example for the gym_env or None for no seed \"\"\" _KEY_SEED = 'seed' _KEY_GYM_ENV = 'gym_env' _KEY_FC_LAYERS = 'fc_layers' def __init__ ( self , gym_env_name : str , fc_layers : Union [ Tuple [ int , ... ], int , None ] = None , seed : Optional [ int ] = None ): \"\"\" Args: gym_env_name: the name of the registered gym environment to use, eg 'CartPole-v0' fc_layers: int tuple defining the number and size of each fully connected layer. \"\"\" if fc_layers is None : fc_layers = ( 100 , 100 ) if isinstance ( fc_layers , int ): fc_layers = ( fc_layers ,) assert isinstance ( gym_env_name , str ), \"passed gym_env_name not a string.\" assert gym_env_name != \"\" , \"gym environment name is empty.\" assert easyagents . env . _is_registered_with_gym ( gym_env_name ), \\ f '\"{gym_env_name}\" is not the name of an environment registered with OpenAI gym.' + \\ 'Consider using easyagents.env.register_with_gym to register your environment.' assert fc_layers is not None , \"fc_layers not set\" assert isinstance ( fc_layers , tuple ), \"fc_layers not a tuple\" assert fc_layers , \"fc_layers must contain at least 1 int\" for i in fc_layers : assert isinstance ( i , int ) and i >= 1 , f '{i} is not a valid size for a hidden layer' self . original_env_name = gym_env_name self . gym_env_name = None self . fc_layers = fc_layers self . seed = seed def __str__ ( self ): return f 'fc_layers={self.fc_layers} seed={self.seed} gym_env_name={self.gym_env_name}' @staticmethod def _from_dict ( from_dict : Dict [ str , object ]): \"\"\"Creates a new instance of ModelConfig based on the parameters contained in dict Returns: new instance of ModelConfig configured by dict \"\"\" assert from_dict # noinspection PyTypeChecker fc_layers = tuple ( from_dict [ ModelConfig . _KEY_FC_LAYERS ]) # noinspection PyTypeChecker result = ModelConfig ( gym_env_name = str ( from_dict [ ModelConfig . _KEY_GYM_ENV ]), fc_layers = fc_layers , seed = from_dict [ ModelConfig . _KEY_SEED ]) return result def _to_dict ( self ) -> Dict [ str , object ]: \"\"\"saves this model configuration to a dict. The model_config can be recreated by a call to _from_dict Retunns: dict containing all parameters of this model_config (this does not include any policy) \"\"\" result : Dict [ str , object ] = dict () result [ ModelConfig . _KEY_SEED ] = self . seed result [ ModelConfig . _KEY_GYM_ENV ] = self . original_env_name result [ ModelConfig . _KEY_FC_LAYERS ] = self . fc_layers return result class TrainContext ( object ): \"\"\"Contains the configuration of an agents train method like the number of iterations or the learning rate along with data gathered sofar during the training which is identical for all implementations. Hints: o TrainContext contains all the parameters needed to control the train loop. o Subclasses of TrainContext may contain additional Agent (but not backend) specific parameters. Attributes: num_iterations: number of times the training is repeated (with additional data), unlimited if None max_steps_per_episode: maximum number of steps per episode learning_rate: the learning rate used in the next iteration's policy training (0,1] reward_discount_gamma: the factor by which a reward is discounted for each step (0,1] max_steps_in_buffer: size of the agents buffer in steps training_done: if true the train loop is terminated at the end of the current iteration iterations_done_in_training: the number of iterations completed so far (during training) episodes_done_in_iteration: the number of episodes completed in the current iteration episodes_done_in_training: the number of episodes completed over all iterations so far. The episodes played for evaluation are not included in this count. steps_done_in_training: the number of steps taken over all iterations so far steps_done_in_iteration: the number of steps taken in the current iteration num_iterations_between_eval: number of training iterations before the current policy is evaluated. num_episodes_per_eval: number of episodes played to estimate the average return and steps eval_rewards: dict containg the rewards statistics for each policy evaluation. Each entry contains the tuple (min, average, max) over the sum of rewards over all episodes played for the current evaluation. The dict is indexed by the current_episode. eval_steps: dict containg the steps statistics for each policy evaluation. Each entry contains the tuple (min, average, max) over the number of step over all episodes played for the current evaluation. The dict is indexed by the current_episode. loss: dict containing the loss for each iteration training. The dict is indexed by the current_episode. \"\"\" def __init__ ( self ): self . num_iterations : Optional [ int ] = None self . max_steps_per_episode : Optional = 1000 self . num_iterations_between_eval : int = 10 self . num_episodes_per_eval : int = 10 self . learning_rate : float = 0.001 self . reward_discount_gamma : float = 1.0 self . max_steps_in_buffer : int = 100000 self . training_done : bool self . iterations_done_in_training : int self . episodes_done_in_iteration : int self . episodes_done_in_training : int self . steps_done_in_training : int self . steps_done_in_iteration = 0 self . loss : Dict [ int , float ] self . eval_rewards : Dict [ int , Tuple [ float , float , float ]] self . eval_steps : Dict [ int , Tuple [ float , float , float ]] self . _reset () def __str__ ( self ): return f 'training_done={self.training_done} ' + \\ f '#iterations_done_in_training={self.iterations_done_in_training} ' + \\ f '#episodes_done_in_iteration={self.episodes_done_in_iteration} ' + \\ f '#steps_done_in_iteration={self.steps_done_in_iteration} ' + \\ f '#iterations={self.num_iterations} ' + \\ f '#max_steps_per_episode={self.max_steps_per_episode} ' + \\ f '#iterations_between_eval={self.num_iterations_between_eval} ' + \\ f '#episodes_per_eval={self.num_episodes_per_eval} ' + \\ f '#learning_rate={self.learning_rate} ' + \\ f '#reward_discount_gamma={self.reward_discount_gamma} ' + \\ f '#max_steps_in_buffer={self.max_steps_in_buffer} ' def _reset ( self ): \"\"\"Clears all values modified during a train() call.\"\"\" self . training_done = False self . iterations_done_in_training = 0 self . episodes_done_in_iteration = 0 self . episodes_done_in_training = 0 self . steps_done_in_training = 0 self . steps_done_in_iteration = 0 self . loss = dict () self . eval_rewards = dict () self . eval_steps = dict () def _validate ( self ): \"\"\"Validates the consistency of all values, raising an exception if an inadmissible combination is detected.\"\"\" assert self . num_iterations is None or self . num_iterations > 0 , \"num_iterations not admissible\" assert self . max_steps_per_episode > 0 , \"max_steps_per_episode not admissible\" assert self . num_iterations_between_eval > 0 , \"num_iterations_between_eval not admissible\" assert self . num_episodes_per_eval > 0 , \"num_episodes_per_eval not admissible\" assert 0 < self . learning_rate <= 1 , \"learning_rate not in interval (0,1]\" assert 0 < self . reward_discount_gamma <= 1 , \"reward_discount_gamma not in interval (0,1]\" @property def num_iterations_between_plot ( self ): \"\"\"number of iterations between 2 plot updates during training. Returns: number of iterations or 0 if no plot updates should take place. \"\"\" result = 0 if self . num_iterations_between_eval : result = math . ceil ( self . num_iterations_between_eval / 3 ) return result class EpisodesTrainContext ( TrainContext ): \"\"\"Base class for all agent which evaluate a number of episodes during each iteration: The train loop proceeds roughly as follows: for i in num_iterations for e in num_episodes_per_iterations play episode and record steps train policy for num_epochs_per_iteration epochs if current_episode % num_iterations_between_eval == 0: evaluate policy if training_done break Attributes: num_episodes_per_iteration: number of episodes played per training iteration num_epochs_per_iteration: number of times the data collected for the current iteration is used to retrain the current policy \"\"\" def __init__ ( self ): self . num_episodes_per_iteration : int = 10 self . num_epochs_per_iteration : int = 10 super () . __init__ () def __str__ ( self ): return super () . __str__ () + \\ f '#episodes_per_iteration={self.num_episodes_per_iteration} ' + \\ f '#epochs_per_iteration={self.num_epochs_per_iteration} ' def _validate ( self ): \"\"\"Validates the consistency of all values, raising an exception if an inadmissible combination is detected.\"\"\" super () . _validate () assert self . num_episodes_per_iteration > 0 , \"num_episodes_per_iteration not admissible\" assert self . num_epochs_per_iteration > 0 , \"num_epochs_per_iteration not admissible\" class CemTrainContext ( EpisodesTrainContext ): \"\"\"Holds the configuration and current training state for Cross-Entropy-Methode agents. Attributes: elite_set_fraction: fraction of the elite policy set. num_steps_buffer_preload: number of steps performed to initially load the policy buffer \"\"\" def __init__ ( self ): super () . __init__ () self . num_iterations = 100 self . num_episodes_per_iteration : int = 50 self . elite_set_fraction : float = 0.1 self . num_steps_buffer_preload : int = 2000 def __str__ ( self ): return super () . __str__ () + f '#elite_set_fraction={self.elite_set_fraction} ' def _validate ( self ): \"\"\"Validates the consistency of all values, raising an exception if an inadmissible combination is detected.\"\"\" super () . _validate () assert 1 >= self . elite_set_fraction > 0 , \"elite_set_fraction must be in interval (0,1]\" class PpoTrainContext ( EpisodesTrainContext ): \"\"\"TrainContext for Actor-Critic type agents like Ppo or Sac. Attributes: actor_loss: loss observed during training of the actor network. dict is indexed by the current_episode. critic_loss: loss observed during training of the critic network. dict is indexed by the current_episode. \"\"\" def __init__ ( self ): super () . __init__ () self . actor_loss : Dict [ int , float ] self . critic_loss : Dict [ int , float ] def _reset ( self ): self . actor_loss = dict () self . critic_loss = dict () super () . _reset () class StepsTrainContext ( TrainContext ): \"\"\"Base class for all agent which evaluate a number of steps during each iteration: The train loop proceeds roughly as follows: for i in num_iterations for s in num_steps_per_iterations play episodes and record steps train policy for num_epochs_per_iteration epochs if current_episode % num_iterations_between_eval == 0: evaluate policy if training_done break Attributes: num_steps_per_iteration: number of steps played for each iteration num_steps_buffer_preload: number of initial collect steps to preload the buffer num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training \"\"\" def __init__ ( self ): super () . __init__ () self . num_iterations = 20000 self . num_iterations_between_eval = 1000 self . num_steps_per_iteration : int = 1 self . num_steps_buffer_preload : int = 1000 self . num_steps_sampled_from_buffer : int = 64 self . max_steps_in_buffer = 100000 def __str__ ( self ): return super () . __str__ () + \\ f '#steps_per_iteration={self.num_steps_per_iteration} ' + \\ f '#steps_buffer_preload={self.num_steps_buffer_preload} ' + \\ f '#steps_sampled_from_buffer={self.num_steps_sampled_from_buffer} ' def _validate ( self ): \"\"\"Validates the consistency of all values, raising an exception if an inadmissible combination is detected.\"\"\" super () . _validate () assert self . num_steps_per_iteration > 0 , \"num_steps_per_iteration not admissible\" class PlayContext ( object ): \"\"\"Contains the current configuration of an agents play method like the number of episodes to play and the max number of steps per episode. The EasyAgent.play() method proceeds (roughly) as follow: for e in num_episodes play (while steps_done_in_episode < max_steps_per_episode) if playing_done break Attributes: num_episodes: number of episodes to play, unlimited if None max_steps_per_episode: maximum number of steps per episode, unlimited if None play_done: if true the play loop is terminated at the end of the current episode episodes_done: the number of episodes played (including the current episode). steps_done_in_episode: the number of steps taken in the current episode. steps_done: the number of steps played (over all episodes so far) actions: dict containing for each episode the actions taken in each step rewards: dict containing for each episode the rewards received in each step sum_of_rewards: dict containing for each episode the sum of rewards over all steps gym_env: the gym environment used to play \"\"\" def __init__ ( self , train_context : Optional [ TrainContext ] = None ): \"\"\" Args: train_context: if set num_episodes, max_steps_per_episode and seed are set from train_context \"\"\" self . num_episodes : Optional [ int ] = None self . max_steps_per_episode : Optional [ int ] = None if train_context is not None : self . num_episodes = train_context . num_episodes_per_eval self . max_steps_per_episode = train_context . max_steps_per_episode self . play_done : bool self . episodes_done : int self . steps_done_in_episode : int self . steps_done : int self . actions : Dict [ int , List [ object ]] self . rewards : Dict [ int , List [ float ]] self . sum_of_rewards : Dict [ int , float ] self . gym_env : Optional [ gym . core . Env ] self . _reset () def __str__ ( self ): return f '#episodes={self.num_episodes} ' + \\ f 'max_steps_per_episode={self.max_steps_per_episode} ' + \\ f 'play_done={self.play_done} ' + \\ f 'episodes_done={self.episodes_done} ' + \\ f 'steps_done_in_episode={self.steps_done_in_episode} ' + \\ f 'steps_done={self.steps_done} ' def _reset ( self ): \"\"\"Clears all values modified during a train() call.\"\"\" self . play_done : bool = False self . episodes_done : int = 0 self . steps_done_in_episode : int = 0 self . steps_done : int = 0 self . actions : Dict [ int , List [ object ]] = dict () self . rewards : Dict [ int , List [ float ]] = dict () self . sum_of_rewards : Dict [ int , float ] = dict () self . gym_env : Optional [ gym . core . Env ] = None def _validate ( self ): \"\"\"Validates the consistency of all values, raising an exception if an inadmissible combination is detected.\"\"\" assert ( self . num_episodes is None ) or ( self . num_episodes > 0 ), \"num_episodes not admissible\" assert ( self . max_steps_per_episode is None ) or self . max_steps_per_episode > 0 , \\ \"max_steps_per_episode not admissible\" class AgentContext ( object ): \"\"\"Collection of state and configuration settings for a EasyAgent instance. Attributes: model: model configuration including the name of the underlying gym_environment and the policy's neural network archtitecture. train: training configuration and current train state. None if not inside a train call. play: play / eval configuration and current state. None if not inside a play call (directly or due to a evaluation inside a train loop) gym: context for gym environment related calls. pyplot: the context containing the matplotlib.pyplot figure to plot to during training or playing \"\"\" def __init__ ( self , model : ModelConfig ): \"\"\" Args: model: model configuration including the name of the underlying gym_environment and the policy's neural network archtitecture. \"\"\" assert isinstance ( model , ModelConfig ), \"model not set\" self . model : ModelConfig = model self . train : Optional [ TrainContext ] = None self . play : Optional [ PlayContext ] = None self . gym : GymContext = GymContext () self . pyplot : PyPlotContext = PyPlotContext () self . _is_policy_trained = False self . _agent_saver : Optional [ Callable [[ Optional [ str ], Union [ List [ AgentCallback ], AgentCallback , None ]], str ]] = None def __str__ ( self ): result = f 'agent_context:' result += f ' \\n api =[{self.gym}]' if self . train is not None : result += f ' \\n train =[{self.train}] ' if self . play is not None : result += f ' \\n play =[{self.play}] ' if self . pyplot is not None : result += f ' \\n pyplot=[{self.pyplot}] ' result += f ' \\n model =[{self.model}] ' return result @property def is_eval ( self ) -> bool : \"\"\"Yields true if a policy evaluation inside an agent.train(...) call is in progress.\"\"\" return ( self . play is not None ) and ( self . train is not None ) @property def is_play ( self ) -> bool : \"\"\"Yields true if an agent.play(...) call is in progress, but not a policy evaluation\"\"\" return ( self . play is not None ) and ( self . train is None ) def _is_plot_ready ( self , plot_type : PlotType ) -> bool : \"\"\"Yields true if any of the plots in plot_type is ready to be plotted. A plot_type is ready if a plot callback was registered for this type (like TRAIN_EVAL), the agent is in runtime state corresponding to the plot type (like in training and at the end of an evaluation period) and any frequency condition is met (like num_episodes_between_plot) \"\"\" result = False if ( plot_type & PlotType . PLAY_EPISODE ) != PlotType . NONE : result = result | ( self . is_play and self . pyplot . _is_subplot_created ( PlotType . PLAY_EPISODE )) if ( plot_type & PlotType . PLAY_STEP ) != PlotType . NONE : result = result | ( self . is_play and self . pyplot . _is_subplot_created ( PlotType . PLAY_STEP )) if ( plot_type & PlotType . TRAIN_EVAL ) != PlotType . NONE : train_result = self . is_eval train_result = train_result and self . pyplot . _is_subplot_created ( PlotType . TRAIN_EVAL ) train_result = train_result and ( self . play . episodes_done == self . train . num_episodes_per_eval ) result = result | train_result if ( plot_type & PlotType . TRAIN_ITERATION ) != PlotType . NONE : train_result = self . is_train train_result = train_result and self . pyplot . _is_subplot_created ( PlotType . TRAIN_ITERATION ) train_result = train_result and \\ self . train . num_iterations_between_plot > 0 and \\ (( self . train . iterations_done_in_training % self . train . num_iterations_between_plot ) == 0 ) result = result | train_result return result @property def is_train ( self ) -> bool : \"\"\"Yields true if an agent.train(...) call is in progress, but not a policy evaluation.\"\"\" return ( self . train is not None ) and ( self . play is None ) class AgentCallback ( ABC ): \"\"\"Base class for all callbacks monitoring the backend algorithms api calls or the api calls to the gym environment\"\"\" def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\" def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\" def on_play_episode_end ( self , agent_context : AgentContext ): \"\"\"Called once after an episode is done or stopped (during play or eval, but not during train).\"\"\" def on_play_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.play() call (during play or eval, but not during train). \"\"\" def on_play_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.play() call (during play or eval, but not during train)\"\"\" def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\" def on_play_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"Called once after a step is completed in the current episode (during play or eval, but not during train).\"\"\" def on_train_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.train() call. \"\"\" def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\" def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\" def on_train_iteration_end ( self , agent_context : AgentContext ): \"\"\"Called once after the current iteration is completed\"\"\" class _PostProcessCallback ( AgentCallback ): pass class _PreProcessCallback ( AgentCallback ): pass Classes AgentCallback class AgentCallback ( / , * args , ** kwargs ) Base class for all callbacks monitoring the backend algorithms api calls or the api calls to the gym environment View Source class AgentCallback ( ABC ): \"\"\"Base class for all callbacks monitoring the backend algorithms api calls or the api calls to the gym environment\"\"\" def on_api_log ( self , agent_context: AgentContext , api_target: str , log_msg: str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass def on_log ( self , agent_context: AgentContext , log_msg: str ): \"\"\"Logs a general message\"\"\" pass def on_gym_init_begin ( self , agent_context: AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" def on_gym_init_end ( self , agent_context: AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass def on_gym_reset_begin ( self , agent_context: AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\" def on_gym_reset_end ( self , agent_context: AgentContext , reset_result: Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass def on_gym_step_begin ( self , agent_context: AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass def on_gym_step_end ( self , agent_context: AgentContext , action , step_result: Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass def on_play_episode_begin ( self , agent_context: AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\" def on_play_episode_end ( self , agent_context: AgentContext ): \"\"\"Called once after an episode is done or stopped (during play or eval, but not during train).\"\"\" def on_play_begin ( self , agent_context: AgentContext ): \"\"\"Called once at the entry of an agent.play() call (during play or eval, but not during train). \"\"\" def on_play_end ( self , agent_context: AgentContext ): \"\"\"Called once before exiting an agent.play() call (during play or eval, but not during train)\"\"\" def on_play_step_begin ( self , agent_context: AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\" def on_play_step_end ( self , agent_context: AgentContext , action , step_result: Tuple ): \"\"\"Called once after a step is completed in the current episode (during play or eval, but not during train).\"\"\" def on_train_begin ( self , agent_context: AgentContext ): \"\"\"Called once at the entry of an agent.train() call. \"\"\" def on_train_end ( self , agent_context: AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\" def on_train_iteration_begin ( self , agent_context: AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\" def on_train_iteration_end ( self , agent_context: AgentContext ): \"\"\"Called once after the current iteration is completed\"\"\" Ancestors (in MRO) abc.ABC Descendants easyagents.core._PostProcessCallback easyagents.core._PreProcessCallback easyagents.callbacks.plot._PlotCallback easyagents.callbacks.plot.Clear easyagents.backends.core._BackendEvalCallback easyagents.callbacks.duration.Fast easyagents.callbacks.log._LogCallbackBase easyagents.callbacks.log._CallbackCounts easyagents.callbacks.save._SaveCallback Methods on_api_log def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass on_gym_init_begin def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" on_gym_init_end def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass on_gym_reset_begin def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\" on_gym_reset_end def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass on_gym_step_begin def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass on_gym_step_end def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass on_log def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass on_play_begin def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.play() call (during play or eval, but not during train). \"\"\" on_play_end def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.play() call (during play or eval, but not during train)\"\"\" on_play_episode_begin def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\" on_play_episode_end def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : AgentContext ): \"\"\"Called once after an episode is done or stopped (during play or eval, but not during train).\"\"\" on_play_step_begin def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\" on_play_step_end def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"Called once after a step is completed in the current episode (during play or eval, but not during train).\"\"\" on_train_begin def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.train() call. \"\"\" on_train_end def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\" on_train_iteration_begin def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\" on_train_iteration_end def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : AgentContext ): \"\"\"Called once after the current iteration is completed\"\"\" AgentContext class AgentContext ( model : easyagents . core . ModelConfig ) Collection of state and configuration settings for a EasyAgent instance. Attributes: model: model configuration including the name of the underlying gym_environment and the policy's neural network archtitecture. train: training configuration and current train state. None if not inside a train call. play: play / eval configuration and current state. None if not inside a play call (directly or due to a evaluation inside a train loop) gym: context for gym environment related calls. pyplot: the context containing the matplotlib.pyplot figure to plot to during training or playing View Source class AgentContext ( object ) : \"\"\"Collection of state and configuration settings for a EasyAgent instance. Attributes: model: model configuration including the name of the underlying gym_environment and the policy's neural network archtitecture. train: training configuration and current train state. None if not inside a train call. play: play / eval configuration and current state. None if not inside a play call (directly or due to a evaluation inside a train loop) gym: context for gym environment related calls. pyplot: the context containing the matplotlib.pyplot figure to plot to during training or playing \"\"\" def __init__ ( self , model : ModelConfig ) : \"\"\" Args: model: model configuration including the name of the underlying gym_environment and the policy's neural network archtitecture. \"\"\" assert isinstance ( model , ModelConfig ), \"model not set\" self . model : ModelConfig = model self . train : Optional [ TrainContext ] = None self . play : Optional [ PlayContext ] = None self . gym : GymContext = GymContext () self . pyplot : PyPlotContext = PyPlotContext () self . _is_policy_trained = False self . _agent_saver : Optional [ Callable[[Optional[str ] , Union [ List[AgentCallback ] , AgentCallback , None ]] , str ]] = None def __str__ ( self ) : result = f 'agent_context:' result += f '\\napi =[{self.gym}]' if self . train is not None : result += f '\\ntrain =[{self.train}] ' if self . play is not None : result += f '\\nplay =[{self.play}] ' if self . pyplot is not None : result += f '\\npyplot=[{self.pyplot}] ' result += f '\\nmodel =[{self.model}] ' return result @property def is_eval ( self ) -> bool : \"\"\"Yields true if a policy evaluation inside an agent.train(...) call is in progress.\"\"\" return ( self . play is not None ) and ( self . train is not None ) @property def is_play ( self ) -> bool : \"\"\"Yields true if an agent.play(...) call is in progress, but not a policy evaluation\"\"\" return ( self . play is not None ) and ( self . train is None ) def _is_plot_ready ( self , plot_type : PlotType ) -> bool : \"\"\"Yields true if any of the plots in plot_type is ready to be plotted. A plot_type is ready if a plot callback was registered for this type (like TRAIN_EVAL), the agent is in runtime state corresponding to the plot type (like in training and at the end of an evaluation period) and any frequency condition is met (like num_episodes_between_plot) \"\"\" result = False if ( plot_type & PlotType . PLAY_EPISODE ) != PlotType . NONE : result = result | ( self . is_play and self . pyplot . _is_subplot_created ( PlotType . PLAY_EPISODE )) if ( plot_type & PlotType . PLAY_STEP ) != PlotType . NONE : result = result | ( self . is_play and self . pyplot . _is_subplot_created ( PlotType . PLAY_STEP )) if ( plot_type & PlotType . TRAIN_EVAL ) != PlotType . NONE : train_result = self . is_eval train_result = train_result and self . pyplot . _is_subplot_created ( PlotType . TRAIN_EVAL ) train_result = train_result and ( self . play . episodes_done == self . train . num_episodes_per_eval ) result = result | train_result if ( plot_type & PlotType . TRAIN_ITERATION ) != PlotType . NONE : train_result = self . is_train train_result = train_result and self . pyplot . _is_subplot_created ( PlotType . TRAIN_ITERATION ) train_result = train_result and \\ self . train . num_iterations_between_plot > 0 and \\ (( self . train . iterations_done_in_training % self . train . num_iterations_between_plot ) == 0 ) result = result | train_result return result @property def is_train ( self ) -> bool : \"\"\"Yields true if an agent.train(...) call is in progress, but not a policy evaluation.\"\"\" return ( self . train is not None ) and ( self . play is None ) Instance variables is_eval Yields true if a policy evaluation inside an agent.train(...) call is in progress. is_play Yields true if an agent.play(...) call is in progress, but not a policy evaluation is_train Yields true if an agent.train(...) call is in progress, but not a policy evaluation. CemTrainContext class CemTrainContext ( ) Holds the configuration and current training state for Cross-Entropy-Methode agents. Attributes: elite_set_fraction: fraction of the elite policy set. num_steps_buffer_preload: number of steps performed to initially load the policy buffer View Source class CemTrainContext ( EpisodesTrainContext ): \"\"\"Holds the configuration and current training state for Cross-Entropy-Methode agents. Attributes: elite_set_fraction: fraction of the elite policy set. num_steps_buffer_preload: number of steps performed to initially load the policy buffer \"\"\" def __init__ ( self ): super (). __init__ () self . num_iterations = 100 self . num_episodes_per_iteration: int = 50 self . elite_set_fraction: float = 0.1 self . num_steps_buffer_preload: int = 2000 def __str__ ( self ): return super (). __str__ () + f' #elite_set_fraction={self.elite_set_fraction} ' def _validate ( self ): \"\"\"Validates the consistency of all values, raising an exception if an inadmissible combination is detected.\"\"\" super (). _validate () assert 1 >= self . elite_set_fraction > 0 , \"elite_set_fraction must be in interval (0,1]\" Ancestors (in MRO) easyagents.core.EpisodesTrainContext easyagents.core.TrainContext Instance variables num_iterations_between_plot number of iterations between 2 plot updates during training. Returns: number of iterations or 0 if no plot updates should take place. EpisodesTrainContext class EpisodesTrainContext ( ) Base class for all agent which evaluate a number of episodes during each iteration: The train loop proceeds roughly as follows: for i in num_iterations for e in num_episodes_per_iterations play episode and record steps train policy for num_epochs_per_iteration epochs if current_episode % num_iterations_between_eval == 0: evaluate policy if training_done break Attributes: num_episodes_per_iteration: number of episodes played per training iteration num_epochs_per_iteration: number of times the data collected for the current iteration is used to retrain the current policy View Source class EpisodesTrainContext ( TrainContext ): \"\"\"Base class for all agent which evaluate a number of episodes during each iteration: The train loop proceeds roughly as follows: for i in num_iterations for e in num_episodes_per_iterations play episode and record steps train policy for num_epochs_per_iteration epochs if current_episode % num_iterations_between_eval == 0: evaluate policy if training_done break Attributes: num_episodes_per_iteration: number of episodes played per training iteration num_epochs_per_iteration: number of times the data collected for the current iteration is used to retrain the current policy \"\"\" def __init__ ( self ): self . num_episodes_per_iteration: int = 10 self . num_epochs_per_iteration: int = 10 super (). __init__ () def __str__ ( self ): return super (). __str__ () + \\ f' #episodes_per_iteration={self.num_episodes_per_iteration} ' + \\ f' #epochs_per_iteration={self.num_epochs_per_iteration} ' def _validate ( self ): \"\"\"Validates the consistency of all values, raising an exception if an inadmissible combination is detected.\"\"\" super (). _validate () assert self . num_episodes_per_iteration > 0 , \"num_episodes_per_iteration not admissible\" assert self . num_epochs_per_iteration > 0 , \"num_epochs_per_iteration not admissible\" Ancestors (in MRO) easyagents.core.TrainContext Descendants easyagents.core.CemTrainContext easyagents.core.PpoTrainContext Instance variables num_iterations_between_plot number of iterations between 2 plot updates during training. Returns: number of iterations or 0 if no plot updates should take place. GymContext class GymContext ( ) Contains the context for gym api calls (wrapping a gym env instance). View Source class GymContext ( object ) : \"\"\"Contains the context for gym api calls (wrapping a gym env instance).\"\"\" def __init__ ( self ) : self . _monitor_env : Optional [ easyagents.backends.monitor._MonitorEnv ] = None self . _totals = None def __str__ ( self ) : return f 'MonitorEnv={self._monitor_env} Totals={self._totals}' @property def gym_env ( self ) -> Optional [ gym.core.Env ] : result = None if self . _monitor_env : result = self . _monitor_env . env return result Instance variables gym_env ModelConfig class ModelConfig ( gym_env_name : str , fc_layers : Union [ Tuple [ int , ... ], int , NoneType ] = None , seed : Union [ int , NoneType ] = None ) The model configurations, containing the name of the gym environment and the neural network architecture. Attributes: original_env_name: the name of the underlying gym environment, eg 'CartPole-v0' gym_env_name: the name of the actual gym environment used (a wrapper around the environment given by original_env_name) fc_layers: int tuple defining the number and size of each fully connected layer. seed: the seed to be used for example for the gym_env or None for no seed View Source class ModelConfig ( object ) : \"\"\"The model configurations, containing the name of the gym environment and the neural network architecture. Attributes: original_env_name: the name of the underlying gym environment, eg 'CartPole-v0' gym_env_name: the name of the actual gym environment used (a wrapper around the environment given by original_env_name) fc_layers: int tuple defining the number and size of each fully connected layer. seed: the seed to be used for example for the gym_env or None for no seed \"\"\" _KEY_SEED = 'seed' _KEY_GYM_ENV = 'gym_env' _KEY_FC_LAYERS = 'fc_layers' def __init__ ( self , gym_env_name : str , fc_layers : Union [ Tuple[int, ... ] , int , None ] = None , seed : Optional [ int ] = None ) : \"\"\" Args: gym_env_name: the name of the registered gym environment to use, eg 'CartPole-v0' fc_layers: int tuple defining the number and size of each fully connected layer. \"\"\" if fc_layers is None : fc_layers = ( 100 , 100 ) if isinstance ( fc_layers , int ) : fc_layers = ( fc_layers ,) assert isinstance ( gym_env_name , str ), \"passed gym_env_name not a string.\" assert gym_env_name != \"\" , \"gym environment name is empty.\" assert easyagents . env . _is_registered_with_gym ( gym_env_name ), \\ f '\"{gym_env_name}\" is not the name of an environment registered with OpenAI gym.' + \\ 'Consider using easyagents.env.register_with_gym to register your environment.' assert fc_layers is not None , \"fc_layers not set\" assert isinstance ( fc_layers , tuple ), \"fc_layers not a tuple\" assert fc_layers , \"fc_layers must contain at least 1 int\" for i in fc_layers : assert isinstance ( i , int ) and i >= 1 , f '{i} is not a valid size for a hidden layer' self . original_env_name = gym_env_name self . gym_env_name = None self . fc_layers = fc_layers self . seed = seed def __str__ ( self ) : return f 'fc_layers={self.fc_layers} seed={self.seed} gym_env_name={self.gym_env_name}' @staticmethod def _from_dict ( from_dict : Dict [ str, object ] ) : \"\"\"Creates a new instance of ModelConfig based on the parameters contained in dict Returns: new instance of ModelConfig configured by dict \"\"\" assert from_dict # noinspection PyTypeChecker fc_layers = tuple ( from_dict [ ModelConfig._KEY_FC_LAYERS ] ) # noinspection PyTypeChecker result = ModelConfig ( gym_env_name = str ( from_dict [ ModelConfig._KEY_GYM_ENV ] ), fc_layers = fc_layers , seed = from_dict [ ModelConfig._KEY_SEED ] ) return result def _to_dict ( self ) -> Dict [ str, object ] : \"\"\"saves this model configuration to a dict. The model_config can be recreated by a call to _from_dict Retunns: dict containing all parameters of this model_config (this does not include any policy) \"\"\" result : Dict [ str, object ] = dict () result [ ModelConfig._KEY_SEED ] = self . seed result [ ModelConfig._KEY_GYM_ENV ] = self . original_env_name result [ ModelConfig._KEY_FC_LAYERS ] = self . fc_layers return result PlayContext class PlayContext ( train_context : Union [ easyagents . core . TrainContext , NoneType ] = None ) Contains the current configuration of an agents play method like the number of episodes to play and the max number of steps per episode. The EasyAgent.play() method proceeds (roughly) as follow: for e in num_episodes play (while steps_done_in_episode < max_steps_per_episode) if playing_done break Attributes: num_episodes: number of episodes to play, unlimited if None max_steps_per_episode: maximum number of steps per episode, unlimited if None play_done: if true the play loop is terminated at the end of the current episode episodes_done: the number of episodes played (including the current episode). steps_done_in_episode: the number of steps taken in the current episode. steps_done: the number of steps played (over all episodes so far) actions : dict containing for each episode the actions taken in each step rewards : dict containing for each episode the rewards received in each step sum_of_rewards : dict containing for each episode the sum of rewards over all steps gym_env : the gym environment used to play View Source class PlayContext ( object ) : \"\"\"Contains the current configuration of an agents play method like the number of episodes to play and the max number of steps per episode. The EasyAgent.play() method proceeds (roughly) as follow: for e in num_episodes play (while steps_done_in_episode < max_steps_per_episode) if playing_done break Attributes: num_episodes: number of episodes to play, unlimited if None max_steps_per_episode: maximum number of steps per episode, unlimited if None play_done: if true the play loop is terminated at the end of the current episode episodes_done: the number of episodes played (including the current episode). steps_done_in_episode: the number of steps taken in the current episode. steps_done: the number of steps played (over all episodes so far) actions: dict containing for each episode the actions taken in each step rewards: dict containing for each episode the rewards received in each step sum_of_rewards: dict containing for each episode the sum of rewards over all steps gym_env: the gym environment used to play \"\"\" def __init__ ( self , train_context : Optional [ TrainContext ] = None ) : \"\"\" Args: train_context: if set num_episodes, max_steps_per_episode and seed are set from train_context \"\"\" self . num_episodes : Optional [ int ] = None self . max_steps_per_episode : Optional [ int ] = None if train_context is not None : self . num_episodes = train_context . num_episodes_per_eval self . max_steps_per_episode = train_context . max_steps_per_episode self . play_done : bool self . episodes_done : int self . steps_done_in_episode : int self . steps_done : int self . actions : Dict [ int, List[object ] ] self . rewards : Dict [ int, List[float ] ] self . sum_of_rewards : Dict [ int, float ] self . gym_env : Optional [ gym.core.Env ] self . _reset () def __str__ ( self ) : return f '#episodes={self.num_episodes} ' + \\ f 'max_steps_per_episode={self.max_steps_per_episode} ' + \\ f 'play_done={self.play_done} ' + \\ f 'episodes_done={self.episodes_done} ' + \\ f 'steps_done_in_episode={self.steps_done_in_episode} ' + \\ f 'steps_done={self.steps_done} ' def _reset ( self ) : \"\"\"Clears all values modified during a train() call.\"\"\" self . play_done : bool = False self . episodes_done : int = 0 self . steps_done_in_episode : int = 0 self . steps_done : int = 0 self . actions : Dict [ int, List[object ] ] = dict () self . rewards : Dict [ int, List[float ] ] = dict () self . sum_of_rewards : Dict [ int, float ] = dict () self . gym_env : Optional [ gym.core.Env ] = None def _validate ( self ) : \"\"\"Validates the consistency of all values, raising an exception if an inadmissible combination is detected.\"\"\" assert ( self . num_episodes is None ) or ( self . num_episodes > 0 ), \"num_episodes not admissible\" assert ( self . max_steps_per_episode is None ) or self . max_steps_per_episode > 0 , \\ \"max_steps_per_episode not admissible\" PlotType class PlotType ( / , * args , ** kwargs ) Defines the point in time when a plot is created / updated. NONE: No plot is updated. PLAY_EPISODE: Called after the last step of each played episode. The gym environment is still accessible through agent_context.play-gym_env. PLAY_STEP: Called after each play step. The gym environment is still accessible through agent_context.play-gym_env. TRAIN_EVAL: Called after the last step of the last evaluation episode during training. The gym environment is accessible through agent_context.play.gym_env. TRAIN_ITERATION: Called after each train iteration. No gym environment is available. View Source class PlotType ( Flag ): \"\"\"Defines the point in time when a plot is created / updated. NONE: No plot is updated. PLAY_EPISODE: Called after the last step of each played episode. The gym environment is still accessible through agent_context.play-gym_env. PLAY_STEP: Called after each play step. The gym environment is still accessible through agent_context.play-gym_env. TRAIN_EVAL: Called after the last step of the last evaluation episode during training. The gym environment is accessible through agent_context.play.gym_env. TRAIN_ITERATION: Called after each train iteration. No gym environment is available. \"\"\" NONE = 0 PLAY_EPISODE = auto () PLAY_STEP = auto () TRAIN_EVAL = auto () TRAIN_ITERATION = auto () Ancestors (in MRO) enum.Flag enum.Enum Class variables NONE PLAY_EPISODE PLAY_STEP TRAIN_EVAL TRAIN_ITERATION PpoTrainContext class PpoTrainContext ( ) TrainContext for Actor-Critic type agents like Ppo or Sac. Attributes: actor_loss: loss observed during training of the actor network. dict is indexed by the current_episode. critic_loss: loss observed during training of the critic network. dict is indexed by the current_episode. View Source class PpoTrainContext ( EpisodesTrainContext ): \"\"\"TrainContext for Actor-Critic type agents like Ppo or Sac. Attributes: actor_loss: loss observed during training of the actor network. dict is indexed by the current_episode. critic_loss: loss observed during training of the critic network. dict is indexed by the current_episode. \"\"\" def __init__ ( self ): super (). __init__ () self . actor_loss: Dict [ int , float ] self . critic_loss: Dict [ int , float ] def _reset ( self ): self . actor_loss = dict () self . critic_loss = dict () super (). _reset () Ancestors (in MRO) easyagents.core.EpisodesTrainContext easyagents.core.TrainContext Instance variables num_iterations_between_plot number of iterations between 2 plot updates during training. Returns: number of iterations or 0 if no plot updates should take place. PyPlotContext class PyPlotContext ( ) Contain the context for the maplotlib.pyplot figure plotting. Attributes figure: the figure to plot to figsize: figure (width,height) in inches for the figure to be created. is_jupyter_active: True if we plot to jupyter notebook cell, False otherwise. max_columns: the max number of subplot columns in the pyplot figure View Source class PyPlotContext ( object ): \"\"\"Contain the context for the maplotlib.pyplot figure plotting. Attributes figure: the figure to plot to figsize: figure (width,height) in inches for the figure to be created. is_jupyter_active: True if we plot to jupyter notebook cell, False otherwise. max_columns: the max number of subplot columns in the pyplot figure \"\"\" def __init__ ( self ): self . _created_subplots = PlotType . NONE self . figure: Optional [ plt . Figure ] = None self . figsize: ( float , float ) = ( 17 , 6 ) self . _call_jupyter_display = False self . is_jupyter_active = False self . max_columns = 3 def __str__ ( self ): figure_number = None figure_axes_len = 0 if self . figure: figure_number = self . figure . number if self . figure . axes: figure_axes_len = len ( self . figure . axes ) return f'is_jupyter_active ={ self . is_jupyter_active } max_columns ={ self . max_columns } ' + \\ f' _created_subplots ={ self . _created_subplots } figure ={ figure_number } axes ={ figure_axes_len } ' def _is_subplot_created ( self , plot_type: PlotType ): \"\"\"Yields true if a subplot of type plot_type was created by a plot callback.\"\"\" result = (( self . _created_subplots & plot_type ) != PlotType . NONE ) return result StepsTrainContext class StepsTrainContext ( ) Base class for all agent which evaluate a number of steps during each iteration: The train loop proceeds roughly as follows: for i in num_iterations for s in num_steps_per_iterations play episodes and record steps train policy for num_epochs_per_iteration epochs if current_episode % num_iterations_between_eval == 0: evaluate policy if training_done break Attributes: num_steps_per_iteration: number of steps played for each iteration num_steps_buffer_preload: number of initial collect steps to preload the buffer num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training View Source class StepsTrainContext ( TrainContext ): \"\"\"Base class for all agent which evaluate a number of steps during each iteration: The train loop proceeds roughly as follows: for i in num_iterations for s in num_steps_per_iterations play episodes and record steps train policy for num_epochs_per_iteration epochs if current_episode % num_iterations_between_eval == 0: evaluate policy if training_done break Attributes: num_steps_per_iteration: number of steps played for each iteration num_steps_buffer_preload: number of initial collect steps to preload the buffer num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training \"\"\" def __init__ ( self ): super (). __init__ () self . num_iterations = 20000 self . num_iterations_between_eval = 1000 self . num_steps_per_iteration: int = 1 self . num_steps_buffer_preload: int = 1000 self . num_steps_sampled_from_buffer: int = 64 self . max_steps_in_buffer = 100000 def __str__ ( self ): return super (). __str__ () + \\ f' #steps_per_iteration={self.num_steps_per_iteration} ' + \\ f' #steps_buffer_preload={self.num_steps_buffer_preload} ' + \\ f' #steps_sampled_from_buffer={self.num_steps_sampled_from_buffer} ' def _validate ( self ): \"\"\"Validates the consistency of all values, raising an exception if an inadmissible combination is detected.\"\"\" super (). _validate () assert self . num_steps_per_iteration > 0 , \"num_steps_per_iteration not admissible\" Ancestors (in MRO) easyagents.core.TrainContext Instance variables num_iterations_between_plot number of iterations between 2 plot updates during training. Returns: number of iterations or 0 if no plot updates should take place. TrainContext class TrainContext ( ) Contains the configuration of an agents train method like the number of iterations or the learning rate along with data gathered sofar during the training which is identical for all implementations. Hints: o TrainContext contains all the parameters needed to control the train loop. o Subclasses of TrainContext may contain additional Agent (but not backend) specific parameters. Attributes: num_iterations: number of times the training is repeated (with additional data), unlimited if None max_steps_per_episode: maximum number of steps per episode learning_rate: the learning rate used in the next iteration's policy training (0,1] reward_discount_gamma: the factor by which a reward is discounted for each step (0,1] max_steps_in_buffer: size of the agents buffer in steps training_done : if true the train loop is terminated at the end of the current iteration iterations_done_in_training : the number of iterations completed so far ( during training ) episodes_done_in_iteration : the number of episodes completed in the current iteration episodes_done_in_training : the number of episodes completed over all iterations so far . The episodes played for evaluation are not included in this count . steps_done_in_training : the number of steps taken over all iterations so far steps_done_in_iteration : the number of steps taken in the current iteration num_iterations_between_eval : number of training iterations before the current policy is evaluated . num_episodes_per_eval : number of episodes played to estimate the average return and steps eval_rewards : dict containg the rewards statistics for each policy evaluation . Each entry contains the tuple ( min , average , max ) over the sum of rewards over all episodes played for the current evaluation . The dict is indexed by the current_episode . eval_steps : dict containg the steps statistics for each policy evaluation . Each entry contains the tuple ( min , average , max ) over the number of step over all episodes played for the current evaluation . The dict is indexed by the current_episode . loss : dict containing the loss for each iteration training . The dict is indexed by the current_episode . View Source class TrainContext ( object ) : \"\"\"Contains the configuration of an agents train method like the number of iterations or the learning rate along with data gathered sofar during the training which is identical for all implementations. Hints: o TrainContext contains all the parameters needed to control the train loop. o Subclasses of TrainContext may contain additional Agent (but not backend) specific parameters. Attributes: num_iterations: number of times the training is repeated (with additional data), unlimited if None max_steps_per_episode: maximum number of steps per episode learning_rate: the learning rate used in the next iteration's policy training (0,1] reward_discount_gamma: the factor by which a reward is discounted for each step (0,1] max_steps_in_buffer: size of the agents buffer in steps training_done: if true the train loop is terminated at the end of the current iteration iterations_done_in_training: the number of iterations completed so far (during training) episodes_done_in_iteration: the number of episodes completed in the current iteration episodes_done_in_training: the number of episodes completed over all iterations so far. The episodes played for evaluation are not included in this count. steps_done_in_training: the number of steps taken over all iterations so far steps_done_in_iteration: the number of steps taken in the current iteration num_iterations_between_eval: number of training iterations before the current policy is evaluated. num_episodes_per_eval: number of episodes played to estimate the average return and steps eval_rewards: dict containg the rewards statistics for each policy evaluation. Each entry contains the tuple (min, average, max) over the sum of rewards over all episodes played for the current evaluation. The dict is indexed by the current_episode. eval_steps: dict containg the steps statistics for each policy evaluation. Each entry contains the tuple (min, average, max) over the number of step over all episodes played for the current evaluation. The dict is indexed by the current_episode. loss: dict containing the loss for each iteration training. The dict is indexed by the current_episode. \"\"\" def __init__ ( self ) : self . num_iterations : Optional [ int ] = None self . max_steps_per_episode : Optional = 1000 self . num_iterations_between_eval : int = 10 self . num_episodes_per_eval : int = 10 self . learning_rate : float = 0.001 self . reward_discount_gamma : float = 1.0 self . max_steps_in_buffer : int = 100000 self . training_done : bool self . iterations_done_in_training : int self . episodes_done_in_iteration : int self . episodes_done_in_training : int self . steps_done_in_training : int self . steps_done_in_iteration = 0 self . loss : Dict [ int, float ] self . eval_rewards : Dict [ int, Tuple[float, float, float ] ] self . eval_steps : Dict [ int, Tuple[float, float, float ] ] self . _reset () def __str__ ( self ) : return f 'training_done={self.training_done} ' + \\ f '#iterations_done_in_training={self.iterations_done_in_training} ' + \\ f '#episodes_done_in_iteration={self.episodes_done_in_iteration} ' + \\ f '#steps_done_in_iteration={self.steps_done_in_iteration} ' + \\ f '#iterations={self.num_iterations} ' + \\ f '#max_steps_per_episode={self.max_steps_per_episode} ' + \\ f '#iterations_between_eval={self.num_iterations_between_eval} ' + \\ f '#episodes_per_eval={self.num_episodes_per_eval} ' + \\ f '#learning_rate={self.learning_rate} ' + \\ f '#reward_discount_gamma={self.reward_discount_gamma} ' + \\ f '#max_steps_in_buffer={self.max_steps_in_buffer} ' def _reset ( self ) : \"\"\"Clears all values modified during a train() call.\"\"\" self . training_done = False self . iterations_done_in_training = 0 self . episodes_done_in_iteration = 0 self . episodes_done_in_training = 0 self . steps_done_in_training = 0 self . steps_done_in_iteration = 0 self . loss = dict () self . eval_rewards = dict () self . eval_steps = dict () def _validate ( self ) : \"\"\"Validates the consistency of all values, raising an exception if an inadmissible combination is detected.\"\"\" assert self . num_iterations is None or self . num_iterations > 0 , \"num_iterations not admissible\" assert self . max_steps_per_episode > 0 , \"max_steps_per_episode not admissible\" assert self . num_iterations_between_eval > 0 , \"num_iterations_between_eval not admissible\" assert self . num_episodes_per_eval > 0 , \"num_episodes_per_eval not admissible\" assert 0 < self . learning_rate <= 1 , \"learning_rate not in interval (0,1]\" assert 0 < self . reward_discount_gamma <= 1 , \"reward_discount_gamma not in interval (0,1]\" @property def num_iterations_between_plot ( self ) : \"\"\"number of iterations between 2 plot updates during training. Returns: number of iterations or 0 if no plot updates should take place. \"\"\" result = 0 if self . num_iterations_between_eval : result = math . ceil ( self . num_iterations_between_eval / 3 ) return result Descendants easyagents.core.EpisodesTrainContext easyagents.core.StepsTrainContext Instance variables num_iterations_between_plot number of iterations between 2 plot updates during training. Returns: number of iterations or 0 if no plot updates should take place.","title":"Core"},{"location":"reference/easyagents/core/#module-easyagentscore","text":"This module contains the core datastructures shared between fronten and backend like the definition of all callbacks and agent configurations. View Source \"\"\"This module contains the core datastructures shared between fronten and backend like the definition of all callbacks and agent configurations. \"\"\" from abc import ABC from typing import Callable , Optional , Dict , Tuple , List , Union from enum import Flag , auto import math import easyagents.env import easyagents.backends.monitor import gym.core import matplotlib.pyplot as plt class GymContext ( object ): \"\"\"Contains the context for gym api calls (wrapping a gym env instance).\"\"\" def __init__ ( self ): self . _monitor_env : Optional [ easyagents . backends . monitor . _MonitorEnv ] = None self . _totals = None def __str__ ( self ): return f 'MonitorEnv={self._monitor_env} Totals={self._totals}' @property def gym_env ( self ) -> Optional [ gym . core . Env ]: result = None if self . _monitor_env : result = self . _monitor_env . env return result class PlotType ( Flag ): \"\"\"Defines the point in time when a plot is created / updated. NONE: No plot is updated. PLAY_EPISODE: Called after the last step of each played episode. The gym environment is still accessible through agent_context.play-gym_env. PLAY_STEP: Called after each play step. The gym environment is still accessible through agent_context.play-gym_env. TRAIN_EVAL: Called after the last step of the last evaluation episode during training. The gym environment is accessible through agent_context.play.gym_env. TRAIN_ITERATION: Called after each train iteration. No gym environment is available. \"\"\" NONE = 0 PLAY_EPISODE = auto () PLAY_STEP = auto () TRAIN_EVAL = auto () TRAIN_ITERATION = auto () class PyPlotContext ( object ): \"\"\"Contain the context for the maplotlib.pyplot figure plotting. Attributes figure: the figure to plot to figsize: figure (width,height) in inches for the figure to be created. is_jupyter_active: True if we plot to jupyter notebook cell, False otherwise. max_columns: the max number of subplot columns in the pyplot figure \"\"\" def __init__ ( self ): self . _created_subplots = PlotType . NONE self . figure : Optional [ plt . Figure ] = None self . figsize : ( float , float ) = ( 17 , 6 ) self . _call_jupyter_display = False self . is_jupyter_active = False self . max_columns = 3 def __str__ ( self ): figure_number = None figure_axes_len = 0 if self . figure : figure_number = self . figure . number if self . figure . axes : figure_axes_len = len ( self . figure . axes ) return f 'is_jupyter_active={self.is_jupyter_active} max_columns={self.max_columns} ' + \\ f '_created_subplots={self._created_subplots} figure={figure_number} axes={figure_axes_len} ' def _is_subplot_created ( self , plot_type : PlotType ): \"\"\"Yields true if a subplot of type plot_type was created by a plot callback.\"\"\" result = (( self . _created_subplots & plot_type ) != PlotType . NONE ) return result class ModelConfig ( object ): \"\"\"The model configurations, containing the name of the gym environment and the neural network architecture. Attributes: original_env_name: the name of the underlying gym environment, eg 'CartPole-v0' gym_env_name: the name of the actual gym environment used (a wrapper around the environment given by original_env_name) fc_layers: int tuple defining the number and size of each fully connected layer. seed: the seed to be used for example for the gym_env or None for no seed \"\"\" _KEY_SEED = 'seed' _KEY_GYM_ENV = 'gym_env' _KEY_FC_LAYERS = 'fc_layers' def __init__ ( self , gym_env_name : str , fc_layers : Union [ Tuple [ int , ... ], int , None ] = None , seed : Optional [ int ] = None ): \"\"\" Args: gym_env_name: the name of the registered gym environment to use, eg 'CartPole-v0' fc_layers: int tuple defining the number and size of each fully connected layer. \"\"\" if fc_layers is None : fc_layers = ( 100 , 100 ) if isinstance ( fc_layers , int ): fc_layers = ( fc_layers ,) assert isinstance ( gym_env_name , str ), \"passed gym_env_name not a string.\" assert gym_env_name != \"\" , \"gym environment name is empty.\" assert easyagents . env . _is_registered_with_gym ( gym_env_name ), \\ f '\"{gym_env_name}\" is not the name of an environment registered with OpenAI gym.' + \\ 'Consider using easyagents.env.register_with_gym to register your environment.' assert fc_layers is not None , \"fc_layers not set\" assert isinstance ( fc_layers , tuple ), \"fc_layers not a tuple\" assert fc_layers , \"fc_layers must contain at least 1 int\" for i in fc_layers : assert isinstance ( i , int ) and i >= 1 , f '{i} is not a valid size for a hidden layer' self . original_env_name = gym_env_name self . gym_env_name = None self . fc_layers = fc_layers self . seed = seed def __str__ ( self ): return f 'fc_layers={self.fc_layers} seed={self.seed} gym_env_name={self.gym_env_name}' @staticmethod def _from_dict ( from_dict : Dict [ str , object ]): \"\"\"Creates a new instance of ModelConfig based on the parameters contained in dict Returns: new instance of ModelConfig configured by dict \"\"\" assert from_dict # noinspection PyTypeChecker fc_layers = tuple ( from_dict [ ModelConfig . _KEY_FC_LAYERS ]) # noinspection PyTypeChecker result = ModelConfig ( gym_env_name = str ( from_dict [ ModelConfig . _KEY_GYM_ENV ]), fc_layers = fc_layers , seed = from_dict [ ModelConfig . _KEY_SEED ]) return result def _to_dict ( self ) -> Dict [ str , object ]: \"\"\"saves this model configuration to a dict. The model_config can be recreated by a call to _from_dict Retunns: dict containing all parameters of this model_config (this does not include any policy) \"\"\" result : Dict [ str , object ] = dict () result [ ModelConfig . _KEY_SEED ] = self . seed result [ ModelConfig . _KEY_GYM_ENV ] = self . original_env_name result [ ModelConfig . _KEY_FC_LAYERS ] = self . fc_layers return result class TrainContext ( object ): \"\"\"Contains the configuration of an agents train method like the number of iterations or the learning rate along with data gathered sofar during the training which is identical for all implementations. Hints: o TrainContext contains all the parameters needed to control the train loop. o Subclasses of TrainContext may contain additional Agent (but not backend) specific parameters. Attributes: num_iterations: number of times the training is repeated (with additional data), unlimited if None max_steps_per_episode: maximum number of steps per episode learning_rate: the learning rate used in the next iteration's policy training (0,1] reward_discount_gamma: the factor by which a reward is discounted for each step (0,1] max_steps_in_buffer: size of the agents buffer in steps training_done: if true the train loop is terminated at the end of the current iteration iterations_done_in_training: the number of iterations completed so far (during training) episodes_done_in_iteration: the number of episodes completed in the current iteration episodes_done_in_training: the number of episodes completed over all iterations so far. The episodes played for evaluation are not included in this count. steps_done_in_training: the number of steps taken over all iterations so far steps_done_in_iteration: the number of steps taken in the current iteration num_iterations_between_eval: number of training iterations before the current policy is evaluated. num_episodes_per_eval: number of episodes played to estimate the average return and steps eval_rewards: dict containg the rewards statistics for each policy evaluation. Each entry contains the tuple (min, average, max) over the sum of rewards over all episodes played for the current evaluation. The dict is indexed by the current_episode. eval_steps: dict containg the steps statistics for each policy evaluation. Each entry contains the tuple (min, average, max) over the number of step over all episodes played for the current evaluation. The dict is indexed by the current_episode. loss: dict containing the loss for each iteration training. The dict is indexed by the current_episode. \"\"\" def __init__ ( self ): self . num_iterations : Optional [ int ] = None self . max_steps_per_episode : Optional = 1000 self . num_iterations_between_eval : int = 10 self . num_episodes_per_eval : int = 10 self . learning_rate : float = 0.001 self . reward_discount_gamma : float = 1.0 self . max_steps_in_buffer : int = 100000 self . training_done : bool self . iterations_done_in_training : int self . episodes_done_in_iteration : int self . episodes_done_in_training : int self . steps_done_in_training : int self . steps_done_in_iteration = 0 self . loss : Dict [ int , float ] self . eval_rewards : Dict [ int , Tuple [ float , float , float ]] self . eval_steps : Dict [ int , Tuple [ float , float , float ]] self . _reset () def __str__ ( self ): return f 'training_done={self.training_done} ' + \\ f '#iterations_done_in_training={self.iterations_done_in_training} ' + \\ f '#episodes_done_in_iteration={self.episodes_done_in_iteration} ' + \\ f '#steps_done_in_iteration={self.steps_done_in_iteration} ' + \\ f '#iterations={self.num_iterations} ' + \\ f '#max_steps_per_episode={self.max_steps_per_episode} ' + \\ f '#iterations_between_eval={self.num_iterations_between_eval} ' + \\ f '#episodes_per_eval={self.num_episodes_per_eval} ' + \\ f '#learning_rate={self.learning_rate} ' + \\ f '#reward_discount_gamma={self.reward_discount_gamma} ' + \\ f '#max_steps_in_buffer={self.max_steps_in_buffer} ' def _reset ( self ): \"\"\"Clears all values modified during a train() call.\"\"\" self . training_done = False self . iterations_done_in_training = 0 self . episodes_done_in_iteration = 0 self . episodes_done_in_training = 0 self . steps_done_in_training = 0 self . steps_done_in_iteration = 0 self . loss = dict () self . eval_rewards = dict () self . eval_steps = dict () def _validate ( self ): \"\"\"Validates the consistency of all values, raising an exception if an inadmissible combination is detected.\"\"\" assert self . num_iterations is None or self . num_iterations > 0 , \"num_iterations not admissible\" assert self . max_steps_per_episode > 0 , \"max_steps_per_episode not admissible\" assert self . num_iterations_between_eval > 0 , \"num_iterations_between_eval not admissible\" assert self . num_episodes_per_eval > 0 , \"num_episodes_per_eval not admissible\" assert 0 < self . learning_rate <= 1 , \"learning_rate not in interval (0,1]\" assert 0 < self . reward_discount_gamma <= 1 , \"reward_discount_gamma not in interval (0,1]\" @property def num_iterations_between_plot ( self ): \"\"\"number of iterations between 2 plot updates during training. Returns: number of iterations or 0 if no plot updates should take place. \"\"\" result = 0 if self . num_iterations_between_eval : result = math . ceil ( self . num_iterations_between_eval / 3 ) return result class EpisodesTrainContext ( TrainContext ): \"\"\"Base class for all agent which evaluate a number of episodes during each iteration: The train loop proceeds roughly as follows: for i in num_iterations for e in num_episodes_per_iterations play episode and record steps train policy for num_epochs_per_iteration epochs if current_episode % num_iterations_between_eval == 0: evaluate policy if training_done break Attributes: num_episodes_per_iteration: number of episodes played per training iteration num_epochs_per_iteration: number of times the data collected for the current iteration is used to retrain the current policy \"\"\" def __init__ ( self ): self . num_episodes_per_iteration : int = 10 self . num_epochs_per_iteration : int = 10 super () . __init__ () def __str__ ( self ): return super () . __str__ () + \\ f '#episodes_per_iteration={self.num_episodes_per_iteration} ' + \\ f '#epochs_per_iteration={self.num_epochs_per_iteration} ' def _validate ( self ): \"\"\"Validates the consistency of all values, raising an exception if an inadmissible combination is detected.\"\"\" super () . _validate () assert self . num_episodes_per_iteration > 0 , \"num_episodes_per_iteration not admissible\" assert self . num_epochs_per_iteration > 0 , \"num_epochs_per_iteration not admissible\" class CemTrainContext ( EpisodesTrainContext ): \"\"\"Holds the configuration and current training state for Cross-Entropy-Methode agents. Attributes: elite_set_fraction: fraction of the elite policy set. num_steps_buffer_preload: number of steps performed to initially load the policy buffer \"\"\" def __init__ ( self ): super () . __init__ () self . num_iterations = 100 self . num_episodes_per_iteration : int = 50 self . elite_set_fraction : float = 0.1 self . num_steps_buffer_preload : int = 2000 def __str__ ( self ): return super () . __str__ () + f '#elite_set_fraction={self.elite_set_fraction} ' def _validate ( self ): \"\"\"Validates the consistency of all values, raising an exception if an inadmissible combination is detected.\"\"\" super () . _validate () assert 1 >= self . elite_set_fraction > 0 , \"elite_set_fraction must be in interval (0,1]\" class PpoTrainContext ( EpisodesTrainContext ): \"\"\"TrainContext for Actor-Critic type agents like Ppo or Sac. Attributes: actor_loss: loss observed during training of the actor network. dict is indexed by the current_episode. critic_loss: loss observed during training of the critic network. dict is indexed by the current_episode. \"\"\" def __init__ ( self ): super () . __init__ () self . actor_loss : Dict [ int , float ] self . critic_loss : Dict [ int , float ] def _reset ( self ): self . actor_loss = dict () self . critic_loss = dict () super () . _reset () class StepsTrainContext ( TrainContext ): \"\"\"Base class for all agent which evaluate a number of steps during each iteration: The train loop proceeds roughly as follows: for i in num_iterations for s in num_steps_per_iterations play episodes and record steps train policy for num_epochs_per_iteration epochs if current_episode % num_iterations_between_eval == 0: evaluate policy if training_done break Attributes: num_steps_per_iteration: number of steps played for each iteration num_steps_buffer_preload: number of initial collect steps to preload the buffer num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training \"\"\" def __init__ ( self ): super () . __init__ () self . num_iterations = 20000 self . num_iterations_between_eval = 1000 self . num_steps_per_iteration : int = 1 self . num_steps_buffer_preload : int = 1000 self . num_steps_sampled_from_buffer : int = 64 self . max_steps_in_buffer = 100000 def __str__ ( self ): return super () . __str__ () + \\ f '#steps_per_iteration={self.num_steps_per_iteration} ' + \\ f '#steps_buffer_preload={self.num_steps_buffer_preload} ' + \\ f '#steps_sampled_from_buffer={self.num_steps_sampled_from_buffer} ' def _validate ( self ): \"\"\"Validates the consistency of all values, raising an exception if an inadmissible combination is detected.\"\"\" super () . _validate () assert self . num_steps_per_iteration > 0 , \"num_steps_per_iteration not admissible\" class PlayContext ( object ): \"\"\"Contains the current configuration of an agents play method like the number of episodes to play and the max number of steps per episode. The EasyAgent.play() method proceeds (roughly) as follow: for e in num_episodes play (while steps_done_in_episode < max_steps_per_episode) if playing_done break Attributes: num_episodes: number of episodes to play, unlimited if None max_steps_per_episode: maximum number of steps per episode, unlimited if None play_done: if true the play loop is terminated at the end of the current episode episodes_done: the number of episodes played (including the current episode). steps_done_in_episode: the number of steps taken in the current episode. steps_done: the number of steps played (over all episodes so far) actions: dict containing for each episode the actions taken in each step rewards: dict containing for each episode the rewards received in each step sum_of_rewards: dict containing for each episode the sum of rewards over all steps gym_env: the gym environment used to play \"\"\" def __init__ ( self , train_context : Optional [ TrainContext ] = None ): \"\"\" Args: train_context: if set num_episodes, max_steps_per_episode and seed are set from train_context \"\"\" self . num_episodes : Optional [ int ] = None self . max_steps_per_episode : Optional [ int ] = None if train_context is not None : self . num_episodes = train_context . num_episodes_per_eval self . max_steps_per_episode = train_context . max_steps_per_episode self . play_done : bool self . episodes_done : int self . steps_done_in_episode : int self . steps_done : int self . actions : Dict [ int , List [ object ]] self . rewards : Dict [ int , List [ float ]] self . sum_of_rewards : Dict [ int , float ] self . gym_env : Optional [ gym . core . Env ] self . _reset () def __str__ ( self ): return f '#episodes={self.num_episodes} ' + \\ f 'max_steps_per_episode={self.max_steps_per_episode} ' + \\ f 'play_done={self.play_done} ' + \\ f 'episodes_done={self.episodes_done} ' + \\ f 'steps_done_in_episode={self.steps_done_in_episode} ' + \\ f 'steps_done={self.steps_done} ' def _reset ( self ): \"\"\"Clears all values modified during a train() call.\"\"\" self . play_done : bool = False self . episodes_done : int = 0 self . steps_done_in_episode : int = 0 self . steps_done : int = 0 self . actions : Dict [ int , List [ object ]] = dict () self . rewards : Dict [ int , List [ float ]] = dict () self . sum_of_rewards : Dict [ int , float ] = dict () self . gym_env : Optional [ gym . core . Env ] = None def _validate ( self ): \"\"\"Validates the consistency of all values, raising an exception if an inadmissible combination is detected.\"\"\" assert ( self . num_episodes is None ) or ( self . num_episodes > 0 ), \"num_episodes not admissible\" assert ( self . max_steps_per_episode is None ) or self . max_steps_per_episode > 0 , \\ \"max_steps_per_episode not admissible\" class AgentContext ( object ): \"\"\"Collection of state and configuration settings for a EasyAgent instance. Attributes: model: model configuration including the name of the underlying gym_environment and the policy's neural network archtitecture. train: training configuration and current train state. None if not inside a train call. play: play / eval configuration and current state. None if not inside a play call (directly or due to a evaluation inside a train loop) gym: context for gym environment related calls. pyplot: the context containing the matplotlib.pyplot figure to plot to during training or playing \"\"\" def __init__ ( self , model : ModelConfig ): \"\"\" Args: model: model configuration including the name of the underlying gym_environment and the policy's neural network archtitecture. \"\"\" assert isinstance ( model , ModelConfig ), \"model not set\" self . model : ModelConfig = model self . train : Optional [ TrainContext ] = None self . play : Optional [ PlayContext ] = None self . gym : GymContext = GymContext () self . pyplot : PyPlotContext = PyPlotContext () self . _is_policy_trained = False self . _agent_saver : Optional [ Callable [[ Optional [ str ], Union [ List [ AgentCallback ], AgentCallback , None ]], str ]] = None def __str__ ( self ): result = f 'agent_context:' result += f ' \\n api =[{self.gym}]' if self . train is not None : result += f ' \\n train =[{self.train}] ' if self . play is not None : result += f ' \\n play =[{self.play}] ' if self . pyplot is not None : result += f ' \\n pyplot=[{self.pyplot}] ' result += f ' \\n model =[{self.model}] ' return result @property def is_eval ( self ) -> bool : \"\"\"Yields true if a policy evaluation inside an agent.train(...) call is in progress.\"\"\" return ( self . play is not None ) and ( self . train is not None ) @property def is_play ( self ) -> bool : \"\"\"Yields true if an agent.play(...) call is in progress, but not a policy evaluation\"\"\" return ( self . play is not None ) and ( self . train is None ) def _is_plot_ready ( self , plot_type : PlotType ) -> bool : \"\"\"Yields true if any of the plots in plot_type is ready to be plotted. A plot_type is ready if a plot callback was registered for this type (like TRAIN_EVAL), the agent is in runtime state corresponding to the plot type (like in training and at the end of an evaluation period) and any frequency condition is met (like num_episodes_between_plot) \"\"\" result = False if ( plot_type & PlotType . PLAY_EPISODE ) != PlotType . NONE : result = result | ( self . is_play and self . pyplot . _is_subplot_created ( PlotType . PLAY_EPISODE )) if ( plot_type & PlotType . PLAY_STEP ) != PlotType . NONE : result = result | ( self . is_play and self . pyplot . _is_subplot_created ( PlotType . PLAY_STEP )) if ( plot_type & PlotType . TRAIN_EVAL ) != PlotType . NONE : train_result = self . is_eval train_result = train_result and self . pyplot . _is_subplot_created ( PlotType . TRAIN_EVAL ) train_result = train_result and ( self . play . episodes_done == self . train . num_episodes_per_eval ) result = result | train_result if ( plot_type & PlotType . TRAIN_ITERATION ) != PlotType . NONE : train_result = self . is_train train_result = train_result and self . pyplot . _is_subplot_created ( PlotType . TRAIN_ITERATION ) train_result = train_result and \\ self . train . num_iterations_between_plot > 0 and \\ (( self . train . iterations_done_in_training % self . train . num_iterations_between_plot ) == 0 ) result = result | train_result return result @property def is_train ( self ) -> bool : \"\"\"Yields true if an agent.train(...) call is in progress, but not a policy evaluation.\"\"\" return ( self . train is not None ) and ( self . play is None ) class AgentCallback ( ABC ): \"\"\"Base class for all callbacks monitoring the backend algorithms api calls or the api calls to the gym environment\"\"\" def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\" def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\" def on_play_episode_end ( self , agent_context : AgentContext ): \"\"\"Called once after an episode is done or stopped (during play or eval, but not during train).\"\"\" def on_play_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.play() call (during play or eval, but not during train). \"\"\" def on_play_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.play() call (during play or eval, but not during train)\"\"\" def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\" def on_play_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"Called once after a step is completed in the current episode (during play or eval, but not during train).\"\"\" def on_train_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.train() call. \"\"\" def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\" def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\" def on_train_iteration_end ( self , agent_context : AgentContext ): \"\"\"Called once after the current iteration is completed\"\"\" class _PostProcessCallback ( AgentCallback ): pass class _PreProcessCallback ( AgentCallback ): pass","title":"Module easyagents.core"},{"location":"reference/easyagents/core/#classes","text":"","title":"Classes"},{"location":"reference/easyagents/core/#agentcallback","text":"class AgentCallback ( / , * args , ** kwargs ) Base class for all callbacks monitoring the backend algorithms api calls or the api calls to the gym environment View Source class AgentCallback ( ABC ): \"\"\"Base class for all callbacks monitoring the backend algorithms api calls or the api calls to the gym environment\"\"\" def on_api_log ( self , agent_context: AgentContext , api_target: str , log_msg: str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass def on_log ( self , agent_context: AgentContext , log_msg: str ): \"\"\"Logs a general message\"\"\" pass def on_gym_init_begin ( self , agent_context: AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" def on_gym_init_end ( self , agent_context: AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass def on_gym_reset_begin ( self , agent_context: AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\" def on_gym_reset_end ( self , agent_context: AgentContext , reset_result: Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass def on_gym_step_begin ( self , agent_context: AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass def on_gym_step_end ( self , agent_context: AgentContext , action , step_result: Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass def on_play_episode_begin ( self , agent_context: AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\" def on_play_episode_end ( self , agent_context: AgentContext ): \"\"\"Called once after an episode is done or stopped (during play or eval, but not during train).\"\"\" def on_play_begin ( self , agent_context: AgentContext ): \"\"\"Called once at the entry of an agent.play() call (during play or eval, but not during train). \"\"\" def on_play_end ( self , agent_context: AgentContext ): \"\"\"Called once before exiting an agent.play() call (during play or eval, but not during train)\"\"\" def on_play_step_begin ( self , agent_context: AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\" def on_play_step_end ( self , agent_context: AgentContext , action , step_result: Tuple ): \"\"\"Called once after a step is completed in the current episode (during play or eval, but not during train).\"\"\" def on_train_begin ( self , agent_context: AgentContext ): \"\"\"Called once at the entry of an agent.train() call. \"\"\" def on_train_end ( self , agent_context: AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\" def on_train_iteration_begin ( self , agent_context: AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\" def on_train_iteration_end ( self , agent_context: AgentContext ): \"\"\"Called once after the current iteration is completed\"\"\"","title":"AgentCallback"},{"location":"reference/easyagents/core/#ancestors-in-mro","text":"abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/core/#descendants","text":"easyagents.core._PostProcessCallback easyagents.core._PreProcessCallback easyagents.callbacks.plot._PlotCallback easyagents.callbacks.plot.Clear easyagents.backends.core._BackendEvalCallback easyagents.callbacks.duration.Fast easyagents.callbacks.log._LogCallbackBase easyagents.callbacks.log._CallbackCounts easyagents.callbacks.save._SaveCallback","title":"Descendants"},{"location":"reference/easyagents/core/#methods","text":"","title":"Methods"},{"location":"reference/easyagents/core/#on_api_log","text":"def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass","title":"on_api_log"},{"location":"reference/easyagents/core/#on_gym_init_begin","text":"def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\"","title":"on_gym_init_begin"},{"location":"reference/easyagents/core/#on_gym_init_end","text":"def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass","title":"on_gym_init_end"},{"location":"reference/easyagents/core/#on_gym_reset_begin","text":"def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\"","title":"on_gym_reset_begin"},{"location":"reference/easyagents/core/#on_gym_reset_end","text":"def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass","title":"on_gym_reset_end"},{"location":"reference/easyagents/core/#on_gym_step_begin","text":"def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass","title":"on_gym_step_begin"},{"location":"reference/easyagents/core/#on_gym_step_end","text":"def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass","title":"on_gym_step_end"},{"location":"reference/easyagents/core/#on_log","text":"def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass","title":"on_log"},{"location":"reference/easyagents/core/#on_play_begin","text":"def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.play() call (during play or eval, but not during train). \"\"\"","title":"on_play_begin"},{"location":"reference/easyagents/core/#on_play_end","text":"def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.play() call (during play or eval, but not during train)\"\"\"","title":"on_play_end"},{"location":"reference/easyagents/core/#on_play_episode_begin","text":"def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\"","title":"on_play_episode_begin"},{"location":"reference/easyagents/core/#on_play_episode_end","text":"def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : AgentContext ): \"\"\"Called once after an episode is done or stopped (during play or eval, but not during train).\"\"\"","title":"on_play_episode_end"},{"location":"reference/easyagents/core/#on_play_step_begin","text":"def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\"","title":"on_play_step_begin"},{"location":"reference/easyagents/core/#on_play_step_end","text":"def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"Called once after a step is completed in the current episode (during play or eval, but not during train).\"\"\"","title":"on_play_step_end"},{"location":"reference/easyagents/core/#on_train_begin","text":"def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.train() call. \"\"\"","title":"on_train_begin"},{"location":"reference/easyagents/core/#on_train_end","text":"def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\"","title":"on_train_end"},{"location":"reference/easyagents/core/#on_train_iteration_begin","text":"def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\"","title":"on_train_iteration_begin"},{"location":"reference/easyagents/core/#on_train_iteration_end","text":"def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : AgentContext ): \"\"\"Called once after the current iteration is completed\"\"\"","title":"on_train_iteration_end"},{"location":"reference/easyagents/core/#agentcontext","text":"class AgentContext ( model : easyagents . core . ModelConfig ) Collection of state and configuration settings for a EasyAgent instance. Attributes: model: model configuration including the name of the underlying gym_environment and the policy's neural network archtitecture. train: training configuration and current train state. None if not inside a train call. play: play / eval configuration and current state. None if not inside a play call (directly or due to a evaluation inside a train loop) gym: context for gym environment related calls. pyplot: the context containing the matplotlib.pyplot figure to plot to during training or playing View Source class AgentContext ( object ) : \"\"\"Collection of state and configuration settings for a EasyAgent instance. Attributes: model: model configuration including the name of the underlying gym_environment and the policy's neural network archtitecture. train: training configuration and current train state. None if not inside a train call. play: play / eval configuration and current state. None if not inside a play call (directly or due to a evaluation inside a train loop) gym: context for gym environment related calls. pyplot: the context containing the matplotlib.pyplot figure to plot to during training or playing \"\"\" def __init__ ( self , model : ModelConfig ) : \"\"\" Args: model: model configuration including the name of the underlying gym_environment and the policy's neural network archtitecture. \"\"\" assert isinstance ( model , ModelConfig ), \"model not set\" self . model : ModelConfig = model self . train : Optional [ TrainContext ] = None self . play : Optional [ PlayContext ] = None self . gym : GymContext = GymContext () self . pyplot : PyPlotContext = PyPlotContext () self . _is_policy_trained = False self . _agent_saver : Optional [ Callable[[Optional[str ] , Union [ List[AgentCallback ] , AgentCallback , None ]] , str ]] = None def __str__ ( self ) : result = f 'agent_context:' result += f '\\napi =[{self.gym}]' if self . train is not None : result += f '\\ntrain =[{self.train}] ' if self . play is not None : result += f '\\nplay =[{self.play}] ' if self . pyplot is not None : result += f '\\npyplot=[{self.pyplot}] ' result += f '\\nmodel =[{self.model}] ' return result @property def is_eval ( self ) -> bool : \"\"\"Yields true if a policy evaluation inside an agent.train(...) call is in progress.\"\"\" return ( self . play is not None ) and ( self . train is not None ) @property def is_play ( self ) -> bool : \"\"\"Yields true if an agent.play(...) call is in progress, but not a policy evaluation\"\"\" return ( self . play is not None ) and ( self . train is None ) def _is_plot_ready ( self , plot_type : PlotType ) -> bool : \"\"\"Yields true if any of the plots in plot_type is ready to be plotted. A plot_type is ready if a plot callback was registered for this type (like TRAIN_EVAL), the agent is in runtime state corresponding to the plot type (like in training and at the end of an evaluation period) and any frequency condition is met (like num_episodes_between_plot) \"\"\" result = False if ( plot_type & PlotType . PLAY_EPISODE ) != PlotType . NONE : result = result | ( self . is_play and self . pyplot . _is_subplot_created ( PlotType . PLAY_EPISODE )) if ( plot_type & PlotType . PLAY_STEP ) != PlotType . NONE : result = result | ( self . is_play and self . pyplot . _is_subplot_created ( PlotType . PLAY_STEP )) if ( plot_type & PlotType . TRAIN_EVAL ) != PlotType . NONE : train_result = self . is_eval train_result = train_result and self . pyplot . _is_subplot_created ( PlotType . TRAIN_EVAL ) train_result = train_result and ( self . play . episodes_done == self . train . num_episodes_per_eval ) result = result | train_result if ( plot_type & PlotType . TRAIN_ITERATION ) != PlotType . NONE : train_result = self . is_train train_result = train_result and self . pyplot . _is_subplot_created ( PlotType . TRAIN_ITERATION ) train_result = train_result and \\ self . train . num_iterations_between_plot > 0 and \\ (( self . train . iterations_done_in_training % self . train . num_iterations_between_plot ) == 0 ) result = result | train_result return result @property def is_train ( self ) -> bool : \"\"\"Yields true if an agent.train(...) call is in progress, but not a policy evaluation.\"\"\" return ( self . train is not None ) and ( self . play is None )","title":"AgentContext"},{"location":"reference/easyagents/core/#instance-variables","text":"is_eval Yields true if a policy evaluation inside an agent.train(...) call is in progress. is_play Yields true if an agent.play(...) call is in progress, but not a policy evaluation is_train Yields true if an agent.train(...) call is in progress, but not a policy evaluation.","title":"Instance variables"},{"location":"reference/easyagents/core/#cemtraincontext","text":"class CemTrainContext ( ) Holds the configuration and current training state for Cross-Entropy-Methode agents. Attributes: elite_set_fraction: fraction of the elite policy set. num_steps_buffer_preload: number of steps performed to initially load the policy buffer View Source class CemTrainContext ( EpisodesTrainContext ): \"\"\"Holds the configuration and current training state for Cross-Entropy-Methode agents. Attributes: elite_set_fraction: fraction of the elite policy set. num_steps_buffer_preload: number of steps performed to initially load the policy buffer \"\"\" def __init__ ( self ): super (). __init__ () self . num_iterations = 100 self . num_episodes_per_iteration: int = 50 self . elite_set_fraction: float = 0.1 self . num_steps_buffer_preload: int = 2000 def __str__ ( self ): return super (). __str__ () + f' #elite_set_fraction={self.elite_set_fraction} ' def _validate ( self ): \"\"\"Validates the consistency of all values, raising an exception if an inadmissible combination is detected.\"\"\" super (). _validate () assert 1 >= self . elite_set_fraction > 0 , \"elite_set_fraction must be in interval (0,1]\"","title":"CemTrainContext"},{"location":"reference/easyagents/core/#ancestors-in-mro_1","text":"easyagents.core.EpisodesTrainContext easyagents.core.TrainContext","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/core/#instance-variables_1","text":"num_iterations_between_plot number of iterations between 2 plot updates during training. Returns: number of iterations or 0 if no plot updates should take place.","title":"Instance variables"},{"location":"reference/easyagents/core/#episodestraincontext","text":"class EpisodesTrainContext ( ) Base class for all agent which evaluate a number of episodes during each iteration: The train loop proceeds roughly as follows: for i in num_iterations for e in num_episodes_per_iterations play episode and record steps train policy for num_epochs_per_iteration epochs if current_episode % num_iterations_between_eval == 0: evaluate policy if training_done break Attributes: num_episodes_per_iteration: number of episodes played per training iteration num_epochs_per_iteration: number of times the data collected for the current iteration is used to retrain the current policy View Source class EpisodesTrainContext ( TrainContext ): \"\"\"Base class for all agent which evaluate a number of episodes during each iteration: The train loop proceeds roughly as follows: for i in num_iterations for e in num_episodes_per_iterations play episode and record steps train policy for num_epochs_per_iteration epochs if current_episode % num_iterations_between_eval == 0: evaluate policy if training_done break Attributes: num_episodes_per_iteration: number of episodes played per training iteration num_epochs_per_iteration: number of times the data collected for the current iteration is used to retrain the current policy \"\"\" def __init__ ( self ): self . num_episodes_per_iteration: int = 10 self . num_epochs_per_iteration: int = 10 super (). __init__ () def __str__ ( self ): return super (). __str__ () + \\ f' #episodes_per_iteration={self.num_episodes_per_iteration} ' + \\ f' #epochs_per_iteration={self.num_epochs_per_iteration} ' def _validate ( self ): \"\"\"Validates the consistency of all values, raising an exception if an inadmissible combination is detected.\"\"\" super (). _validate () assert self . num_episodes_per_iteration > 0 , \"num_episodes_per_iteration not admissible\" assert self . num_epochs_per_iteration > 0 , \"num_epochs_per_iteration not admissible\"","title":"EpisodesTrainContext"},{"location":"reference/easyagents/core/#ancestors-in-mro_2","text":"easyagents.core.TrainContext","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/core/#descendants_1","text":"easyagents.core.CemTrainContext easyagents.core.PpoTrainContext","title":"Descendants"},{"location":"reference/easyagents/core/#instance-variables_2","text":"num_iterations_between_plot number of iterations between 2 plot updates during training. Returns: number of iterations or 0 if no plot updates should take place.","title":"Instance variables"},{"location":"reference/easyagents/core/#gymcontext","text":"class GymContext ( ) Contains the context for gym api calls (wrapping a gym env instance). View Source class GymContext ( object ) : \"\"\"Contains the context for gym api calls (wrapping a gym env instance).\"\"\" def __init__ ( self ) : self . _monitor_env : Optional [ easyagents.backends.monitor._MonitorEnv ] = None self . _totals = None def __str__ ( self ) : return f 'MonitorEnv={self._monitor_env} Totals={self._totals}' @property def gym_env ( self ) -> Optional [ gym.core.Env ] : result = None if self . _monitor_env : result = self . _monitor_env . env return result","title":"GymContext"},{"location":"reference/easyagents/core/#instance-variables_3","text":"gym_env","title":"Instance variables"},{"location":"reference/easyagents/core/#modelconfig","text":"class ModelConfig ( gym_env_name : str , fc_layers : Union [ Tuple [ int , ... ], int , NoneType ] = None , seed : Union [ int , NoneType ] = None ) The model configurations, containing the name of the gym environment and the neural network architecture. Attributes: original_env_name: the name of the underlying gym environment, eg 'CartPole-v0' gym_env_name: the name of the actual gym environment used (a wrapper around the environment given by original_env_name) fc_layers: int tuple defining the number and size of each fully connected layer. seed: the seed to be used for example for the gym_env or None for no seed View Source class ModelConfig ( object ) : \"\"\"The model configurations, containing the name of the gym environment and the neural network architecture. Attributes: original_env_name: the name of the underlying gym environment, eg 'CartPole-v0' gym_env_name: the name of the actual gym environment used (a wrapper around the environment given by original_env_name) fc_layers: int tuple defining the number and size of each fully connected layer. seed: the seed to be used for example for the gym_env or None for no seed \"\"\" _KEY_SEED = 'seed' _KEY_GYM_ENV = 'gym_env' _KEY_FC_LAYERS = 'fc_layers' def __init__ ( self , gym_env_name : str , fc_layers : Union [ Tuple[int, ... ] , int , None ] = None , seed : Optional [ int ] = None ) : \"\"\" Args: gym_env_name: the name of the registered gym environment to use, eg 'CartPole-v0' fc_layers: int tuple defining the number and size of each fully connected layer. \"\"\" if fc_layers is None : fc_layers = ( 100 , 100 ) if isinstance ( fc_layers , int ) : fc_layers = ( fc_layers ,) assert isinstance ( gym_env_name , str ), \"passed gym_env_name not a string.\" assert gym_env_name != \"\" , \"gym environment name is empty.\" assert easyagents . env . _is_registered_with_gym ( gym_env_name ), \\ f '\"{gym_env_name}\" is not the name of an environment registered with OpenAI gym.' + \\ 'Consider using easyagents.env.register_with_gym to register your environment.' assert fc_layers is not None , \"fc_layers not set\" assert isinstance ( fc_layers , tuple ), \"fc_layers not a tuple\" assert fc_layers , \"fc_layers must contain at least 1 int\" for i in fc_layers : assert isinstance ( i , int ) and i >= 1 , f '{i} is not a valid size for a hidden layer' self . original_env_name = gym_env_name self . gym_env_name = None self . fc_layers = fc_layers self . seed = seed def __str__ ( self ) : return f 'fc_layers={self.fc_layers} seed={self.seed} gym_env_name={self.gym_env_name}' @staticmethod def _from_dict ( from_dict : Dict [ str, object ] ) : \"\"\"Creates a new instance of ModelConfig based on the parameters contained in dict Returns: new instance of ModelConfig configured by dict \"\"\" assert from_dict # noinspection PyTypeChecker fc_layers = tuple ( from_dict [ ModelConfig._KEY_FC_LAYERS ] ) # noinspection PyTypeChecker result = ModelConfig ( gym_env_name = str ( from_dict [ ModelConfig._KEY_GYM_ENV ] ), fc_layers = fc_layers , seed = from_dict [ ModelConfig._KEY_SEED ] ) return result def _to_dict ( self ) -> Dict [ str, object ] : \"\"\"saves this model configuration to a dict. The model_config can be recreated by a call to _from_dict Retunns: dict containing all parameters of this model_config (this does not include any policy) \"\"\" result : Dict [ str, object ] = dict () result [ ModelConfig._KEY_SEED ] = self . seed result [ ModelConfig._KEY_GYM_ENV ] = self . original_env_name result [ ModelConfig._KEY_FC_LAYERS ] = self . fc_layers return result","title":"ModelConfig"},{"location":"reference/easyagents/core/#playcontext","text":"class PlayContext ( train_context : Union [ easyagents . core . TrainContext , NoneType ] = None ) Contains the current configuration of an agents play method like the number of episodes to play and the max number of steps per episode. The EasyAgent.play() method proceeds (roughly) as follow: for e in num_episodes play (while steps_done_in_episode < max_steps_per_episode) if playing_done break Attributes: num_episodes: number of episodes to play, unlimited if None max_steps_per_episode: maximum number of steps per episode, unlimited if None play_done: if true the play loop is terminated at the end of the current episode episodes_done: the number of episodes played (including the current episode). steps_done_in_episode: the number of steps taken in the current episode. steps_done: the number of steps played (over all episodes so far) actions : dict containing for each episode the actions taken in each step rewards : dict containing for each episode the rewards received in each step sum_of_rewards : dict containing for each episode the sum of rewards over all steps gym_env : the gym environment used to play View Source class PlayContext ( object ) : \"\"\"Contains the current configuration of an agents play method like the number of episodes to play and the max number of steps per episode. The EasyAgent.play() method proceeds (roughly) as follow: for e in num_episodes play (while steps_done_in_episode < max_steps_per_episode) if playing_done break Attributes: num_episodes: number of episodes to play, unlimited if None max_steps_per_episode: maximum number of steps per episode, unlimited if None play_done: if true the play loop is terminated at the end of the current episode episodes_done: the number of episodes played (including the current episode). steps_done_in_episode: the number of steps taken in the current episode. steps_done: the number of steps played (over all episodes so far) actions: dict containing for each episode the actions taken in each step rewards: dict containing for each episode the rewards received in each step sum_of_rewards: dict containing for each episode the sum of rewards over all steps gym_env: the gym environment used to play \"\"\" def __init__ ( self , train_context : Optional [ TrainContext ] = None ) : \"\"\" Args: train_context: if set num_episodes, max_steps_per_episode and seed are set from train_context \"\"\" self . num_episodes : Optional [ int ] = None self . max_steps_per_episode : Optional [ int ] = None if train_context is not None : self . num_episodes = train_context . num_episodes_per_eval self . max_steps_per_episode = train_context . max_steps_per_episode self . play_done : bool self . episodes_done : int self . steps_done_in_episode : int self . steps_done : int self . actions : Dict [ int, List[object ] ] self . rewards : Dict [ int, List[float ] ] self . sum_of_rewards : Dict [ int, float ] self . gym_env : Optional [ gym.core.Env ] self . _reset () def __str__ ( self ) : return f '#episodes={self.num_episodes} ' + \\ f 'max_steps_per_episode={self.max_steps_per_episode} ' + \\ f 'play_done={self.play_done} ' + \\ f 'episodes_done={self.episodes_done} ' + \\ f 'steps_done_in_episode={self.steps_done_in_episode} ' + \\ f 'steps_done={self.steps_done} ' def _reset ( self ) : \"\"\"Clears all values modified during a train() call.\"\"\" self . play_done : bool = False self . episodes_done : int = 0 self . steps_done_in_episode : int = 0 self . steps_done : int = 0 self . actions : Dict [ int, List[object ] ] = dict () self . rewards : Dict [ int, List[float ] ] = dict () self . sum_of_rewards : Dict [ int, float ] = dict () self . gym_env : Optional [ gym.core.Env ] = None def _validate ( self ) : \"\"\"Validates the consistency of all values, raising an exception if an inadmissible combination is detected.\"\"\" assert ( self . num_episodes is None ) or ( self . num_episodes > 0 ), \"num_episodes not admissible\" assert ( self . max_steps_per_episode is None ) or self . max_steps_per_episode > 0 , \\ \"max_steps_per_episode not admissible\"","title":"PlayContext"},{"location":"reference/easyagents/core/#plottype","text":"class PlotType ( / , * args , ** kwargs ) Defines the point in time when a plot is created / updated. NONE: No plot is updated. PLAY_EPISODE: Called after the last step of each played episode. The gym environment is still accessible through agent_context.play-gym_env. PLAY_STEP: Called after each play step. The gym environment is still accessible through agent_context.play-gym_env. TRAIN_EVAL: Called after the last step of the last evaluation episode during training. The gym environment is accessible through agent_context.play.gym_env. TRAIN_ITERATION: Called after each train iteration. No gym environment is available. View Source class PlotType ( Flag ): \"\"\"Defines the point in time when a plot is created / updated. NONE: No plot is updated. PLAY_EPISODE: Called after the last step of each played episode. The gym environment is still accessible through agent_context.play-gym_env. PLAY_STEP: Called after each play step. The gym environment is still accessible through agent_context.play-gym_env. TRAIN_EVAL: Called after the last step of the last evaluation episode during training. The gym environment is accessible through agent_context.play.gym_env. TRAIN_ITERATION: Called after each train iteration. No gym environment is available. \"\"\" NONE = 0 PLAY_EPISODE = auto () PLAY_STEP = auto () TRAIN_EVAL = auto () TRAIN_ITERATION = auto ()","title":"PlotType"},{"location":"reference/easyagents/core/#ancestors-in-mro_3","text":"enum.Flag enum.Enum","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/core/#class-variables","text":"NONE PLAY_EPISODE PLAY_STEP TRAIN_EVAL TRAIN_ITERATION","title":"Class variables"},{"location":"reference/easyagents/core/#ppotraincontext","text":"class PpoTrainContext ( ) TrainContext for Actor-Critic type agents like Ppo or Sac. Attributes: actor_loss: loss observed during training of the actor network. dict is indexed by the current_episode. critic_loss: loss observed during training of the critic network. dict is indexed by the current_episode. View Source class PpoTrainContext ( EpisodesTrainContext ): \"\"\"TrainContext for Actor-Critic type agents like Ppo or Sac. Attributes: actor_loss: loss observed during training of the actor network. dict is indexed by the current_episode. critic_loss: loss observed during training of the critic network. dict is indexed by the current_episode. \"\"\" def __init__ ( self ): super (). __init__ () self . actor_loss: Dict [ int , float ] self . critic_loss: Dict [ int , float ] def _reset ( self ): self . actor_loss = dict () self . critic_loss = dict () super (). _reset ()","title":"PpoTrainContext"},{"location":"reference/easyagents/core/#ancestors-in-mro_4","text":"easyagents.core.EpisodesTrainContext easyagents.core.TrainContext","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/core/#instance-variables_4","text":"num_iterations_between_plot number of iterations between 2 plot updates during training. Returns: number of iterations or 0 if no plot updates should take place.","title":"Instance variables"},{"location":"reference/easyagents/core/#pyplotcontext","text":"class PyPlotContext ( ) Contain the context for the maplotlib.pyplot figure plotting. Attributes figure: the figure to plot to figsize: figure (width,height) in inches for the figure to be created. is_jupyter_active: True if we plot to jupyter notebook cell, False otherwise. max_columns: the max number of subplot columns in the pyplot figure View Source class PyPlotContext ( object ): \"\"\"Contain the context for the maplotlib.pyplot figure plotting. Attributes figure: the figure to plot to figsize: figure (width,height) in inches for the figure to be created. is_jupyter_active: True if we plot to jupyter notebook cell, False otherwise. max_columns: the max number of subplot columns in the pyplot figure \"\"\" def __init__ ( self ): self . _created_subplots = PlotType . NONE self . figure: Optional [ plt . Figure ] = None self . figsize: ( float , float ) = ( 17 , 6 ) self . _call_jupyter_display = False self . is_jupyter_active = False self . max_columns = 3 def __str__ ( self ): figure_number = None figure_axes_len = 0 if self . figure: figure_number = self . figure . number if self . figure . axes: figure_axes_len = len ( self . figure . axes ) return f'is_jupyter_active ={ self . is_jupyter_active } max_columns ={ self . max_columns } ' + \\ f' _created_subplots ={ self . _created_subplots } figure ={ figure_number } axes ={ figure_axes_len } ' def _is_subplot_created ( self , plot_type: PlotType ): \"\"\"Yields true if a subplot of type plot_type was created by a plot callback.\"\"\" result = (( self . _created_subplots & plot_type ) != PlotType . NONE ) return result","title":"PyPlotContext"},{"location":"reference/easyagents/core/#stepstraincontext","text":"class StepsTrainContext ( ) Base class for all agent which evaluate a number of steps during each iteration: The train loop proceeds roughly as follows: for i in num_iterations for s in num_steps_per_iterations play episodes and record steps train policy for num_epochs_per_iteration epochs if current_episode % num_iterations_between_eval == 0: evaluate policy if training_done break Attributes: num_steps_per_iteration: number of steps played for each iteration num_steps_buffer_preload: number of initial collect steps to preload the buffer num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training View Source class StepsTrainContext ( TrainContext ): \"\"\"Base class for all agent which evaluate a number of steps during each iteration: The train loop proceeds roughly as follows: for i in num_iterations for s in num_steps_per_iterations play episodes and record steps train policy for num_epochs_per_iteration epochs if current_episode % num_iterations_between_eval == 0: evaluate policy if training_done break Attributes: num_steps_per_iteration: number of steps played for each iteration num_steps_buffer_preload: number of initial collect steps to preload the buffer num_steps_sampled_from_buffer: the number of steps sampled from buffer for each iteration training \"\"\" def __init__ ( self ): super (). __init__ () self . num_iterations = 20000 self . num_iterations_between_eval = 1000 self . num_steps_per_iteration: int = 1 self . num_steps_buffer_preload: int = 1000 self . num_steps_sampled_from_buffer: int = 64 self . max_steps_in_buffer = 100000 def __str__ ( self ): return super (). __str__ () + \\ f' #steps_per_iteration={self.num_steps_per_iteration} ' + \\ f' #steps_buffer_preload={self.num_steps_buffer_preload} ' + \\ f' #steps_sampled_from_buffer={self.num_steps_sampled_from_buffer} ' def _validate ( self ): \"\"\"Validates the consistency of all values, raising an exception if an inadmissible combination is detected.\"\"\" super (). _validate () assert self . num_steps_per_iteration > 0 , \"num_steps_per_iteration not admissible\"","title":"StepsTrainContext"},{"location":"reference/easyagents/core/#ancestors-in-mro_5","text":"easyagents.core.TrainContext","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/core/#instance-variables_5","text":"num_iterations_between_plot number of iterations between 2 plot updates during training. Returns: number of iterations or 0 if no plot updates should take place.","title":"Instance variables"},{"location":"reference/easyagents/core/#traincontext","text":"class TrainContext ( ) Contains the configuration of an agents train method like the number of iterations or the learning rate along with data gathered sofar during the training which is identical for all implementations. Hints: o TrainContext contains all the parameters needed to control the train loop. o Subclasses of TrainContext may contain additional Agent (but not backend) specific parameters. Attributes: num_iterations: number of times the training is repeated (with additional data), unlimited if None max_steps_per_episode: maximum number of steps per episode learning_rate: the learning rate used in the next iteration's policy training (0,1] reward_discount_gamma: the factor by which a reward is discounted for each step (0,1] max_steps_in_buffer: size of the agents buffer in steps training_done : if true the train loop is terminated at the end of the current iteration iterations_done_in_training : the number of iterations completed so far ( during training ) episodes_done_in_iteration : the number of episodes completed in the current iteration episodes_done_in_training : the number of episodes completed over all iterations so far . The episodes played for evaluation are not included in this count . steps_done_in_training : the number of steps taken over all iterations so far steps_done_in_iteration : the number of steps taken in the current iteration num_iterations_between_eval : number of training iterations before the current policy is evaluated . num_episodes_per_eval : number of episodes played to estimate the average return and steps eval_rewards : dict containg the rewards statistics for each policy evaluation . Each entry contains the tuple ( min , average , max ) over the sum of rewards over all episodes played for the current evaluation . The dict is indexed by the current_episode . eval_steps : dict containg the steps statistics for each policy evaluation . Each entry contains the tuple ( min , average , max ) over the number of step over all episodes played for the current evaluation . The dict is indexed by the current_episode . loss : dict containing the loss for each iteration training . The dict is indexed by the current_episode . View Source class TrainContext ( object ) : \"\"\"Contains the configuration of an agents train method like the number of iterations or the learning rate along with data gathered sofar during the training which is identical for all implementations. Hints: o TrainContext contains all the parameters needed to control the train loop. o Subclasses of TrainContext may contain additional Agent (but not backend) specific parameters. Attributes: num_iterations: number of times the training is repeated (with additional data), unlimited if None max_steps_per_episode: maximum number of steps per episode learning_rate: the learning rate used in the next iteration's policy training (0,1] reward_discount_gamma: the factor by which a reward is discounted for each step (0,1] max_steps_in_buffer: size of the agents buffer in steps training_done: if true the train loop is terminated at the end of the current iteration iterations_done_in_training: the number of iterations completed so far (during training) episodes_done_in_iteration: the number of episodes completed in the current iteration episodes_done_in_training: the number of episodes completed over all iterations so far. The episodes played for evaluation are not included in this count. steps_done_in_training: the number of steps taken over all iterations so far steps_done_in_iteration: the number of steps taken in the current iteration num_iterations_between_eval: number of training iterations before the current policy is evaluated. num_episodes_per_eval: number of episodes played to estimate the average return and steps eval_rewards: dict containg the rewards statistics for each policy evaluation. Each entry contains the tuple (min, average, max) over the sum of rewards over all episodes played for the current evaluation. The dict is indexed by the current_episode. eval_steps: dict containg the steps statistics for each policy evaluation. Each entry contains the tuple (min, average, max) over the number of step over all episodes played for the current evaluation. The dict is indexed by the current_episode. loss: dict containing the loss for each iteration training. The dict is indexed by the current_episode. \"\"\" def __init__ ( self ) : self . num_iterations : Optional [ int ] = None self . max_steps_per_episode : Optional = 1000 self . num_iterations_between_eval : int = 10 self . num_episodes_per_eval : int = 10 self . learning_rate : float = 0.001 self . reward_discount_gamma : float = 1.0 self . max_steps_in_buffer : int = 100000 self . training_done : bool self . iterations_done_in_training : int self . episodes_done_in_iteration : int self . episodes_done_in_training : int self . steps_done_in_training : int self . steps_done_in_iteration = 0 self . loss : Dict [ int, float ] self . eval_rewards : Dict [ int, Tuple[float, float, float ] ] self . eval_steps : Dict [ int, Tuple[float, float, float ] ] self . _reset () def __str__ ( self ) : return f 'training_done={self.training_done} ' + \\ f '#iterations_done_in_training={self.iterations_done_in_training} ' + \\ f '#episodes_done_in_iteration={self.episodes_done_in_iteration} ' + \\ f '#steps_done_in_iteration={self.steps_done_in_iteration} ' + \\ f '#iterations={self.num_iterations} ' + \\ f '#max_steps_per_episode={self.max_steps_per_episode} ' + \\ f '#iterations_between_eval={self.num_iterations_between_eval} ' + \\ f '#episodes_per_eval={self.num_episodes_per_eval} ' + \\ f '#learning_rate={self.learning_rate} ' + \\ f '#reward_discount_gamma={self.reward_discount_gamma} ' + \\ f '#max_steps_in_buffer={self.max_steps_in_buffer} ' def _reset ( self ) : \"\"\"Clears all values modified during a train() call.\"\"\" self . training_done = False self . iterations_done_in_training = 0 self . episodes_done_in_iteration = 0 self . episodes_done_in_training = 0 self . steps_done_in_training = 0 self . steps_done_in_iteration = 0 self . loss = dict () self . eval_rewards = dict () self . eval_steps = dict () def _validate ( self ) : \"\"\"Validates the consistency of all values, raising an exception if an inadmissible combination is detected.\"\"\" assert self . num_iterations is None or self . num_iterations > 0 , \"num_iterations not admissible\" assert self . max_steps_per_episode > 0 , \"max_steps_per_episode not admissible\" assert self . num_iterations_between_eval > 0 , \"num_iterations_between_eval not admissible\" assert self . num_episodes_per_eval > 0 , \"num_episodes_per_eval not admissible\" assert 0 < self . learning_rate <= 1 , \"learning_rate not in interval (0,1]\" assert 0 < self . reward_discount_gamma <= 1 , \"reward_discount_gamma not in interval (0,1]\" @property def num_iterations_between_plot ( self ) : \"\"\"number of iterations between 2 plot updates during training. Returns: number of iterations or 0 if no plot updates should take place. \"\"\" result = 0 if self . num_iterations_between_eval : result = math . ceil ( self . num_iterations_between_eval / 3 ) return result","title":"TrainContext"},{"location":"reference/easyagents/core/#descendants_2","text":"easyagents.core.EpisodesTrainContext easyagents.core.StepsTrainContext","title":"Descendants"},{"location":"reference/easyagents/core/#instance-variables_6","text":"num_iterations_between_plot number of iterations between 2 plot updates during training. Returns: number of iterations or 0 if no plot updates should take place.","title":"Instance variables"},{"location":"reference/easyagents/env/","text":"Module easyagents.env This module contains support classes and methods to interact with OpenAI gym environments as well as a gym env implementation for unit tests (linewordl) see https://github.com/openai/gym View Source \"\"\"This module contains support classes and methods to interact with OpenAI gym environments as well as a gym env implementation for unit tests (linewordl) see https://github.com/openai/gym \"\"\" import gym , gym.envs , gym.error , gym.spaces import inspect import math import matplotlib as plt import numpy as np from typing import List , Optional , Tuple , Dict , Callable , Any def _is_registered_with_gym ( gym_env_name : str ) -> bool : \"\"\"Determines if a gym environment with the name id exists. Args: gym_env_name: gym id to test. Returns: True if it exists, false otherwise \"\"\" result = False try : spec = gym . envs . registration . spec ( gym_env_name ) assert spec is not None result = True except gym . error . UnregisteredEnv : pass return result # noinspection DuplicatedCode def register_with_gym ( gym_env_name : str , entry_point : type , max_episode_steps : int = 100000 , ** kwargs ): \"\"\"Registers the class entry_point in gym by the name gym_env_name allowing overriding registrations. Thus different implementations of the same class (and the same name) maybe registered consecutively. The latest registrated version is used for instantiation. This facilitates developing an environment in a jupyter notebook without haveing to reregister a modified class under a new name. limitation: the max_episode_steps value of the first registration holds for all registrations with the same gym_env_name Args: gym_env_name: the gym environment name to be used as argument with gym.make max_episode_steps: all episodes end latest after this number of steps entry_point: the class to be registed with gym id gym_env_name kwargs: the args passed to the entry_point constructor call \"\"\" assert gym_env_name is not None , \"None is not an admissible environment name\" assert type ( gym_env_name ) is str , \"gym_env_name is not a str\" assert len ( gym_env_name ) > 0 , \"empty string is not an admissible environment name\" assert inspect . isclass ( entry_point ), \"entry_point not a class\" assert issubclass ( entry_point , gym . Env ), \"entry_point not a subclass of gym.Env\" assert callable ( entry_point ), \"entry_point not callable\" if gym_env_name not in _ShimEnv . _entry_points : gym . envs . registration . register ( id = gym_env_name , entry_point = _ShimEnv , max_episode_steps = max_episode_steps , kwargs = { _ShimEnv . _KWARG_GYM_NAME : gym_env_name }) _ShimEnv . _entry_points [ gym_env_name ] = ( entry_point , kwargs ) class _LineWorldEnv ( gym . Env ): \"\"\"Simple environment for fast unittest, registered as 'LineWorld-v0' * an agent lives in a finite linear world of uneven elements * at each moment it is in a certain position * initial position is the middle * some positions gain rewards, some don't * rewards are between 0 and 15 * agent can either move left or right * objective: maximize total reward = sum(rewards) + sum(steps) * Cost per step: -1 * Done Condition: agent is at pos 0 or total reward <= -20 \"\"\" @staticmethod def register_with_gym (): \"\"\"Register this environment with gym and yields the gym environment name.\"\"\" result = \"LineWorld-v0\" register_with_gym ( result , _LineWorldEnv ) return result def __init__ ( self , world : Optional [ List [ int ]] = None ): \"\"\"Creates the lineword, size and rewards are given by the world arg. Args: world: list of rewards to collect in each position of the lineworld. \"\"\" if world is None : world = [ 10 , 0 , 0 , 5 , 0 , 2 , 15 ] assert world , \"world must not be None or empty.\" self . world : np . array = np . array ( world ) number_of_actions : int = 2 self . action_space : gym . spaces . Discrete = gym . spaces . Discrete ( number_of_actions ) self . size_of_world : int = len ( world ) self . max_reward : int = max ( world ) self . min_reward : int = min ( world ) # the environment's current state is described by the position of the agent and the remaining rewards self . observation_size : int = 1 + self . size_of_world low : np . array = np . full ( self . observation_size , self . min_reward ) high : np . array = np . full ( self . observation_size , self . max_reward ) self . observation_space : gym . spaces . Box = gym . spaces . Box ( low = low , high = high , dtype = np . float32 ) self . reward_range : Tuple [ int , int ] = ( self . min_reward , self . max_reward ) self . steps : int = 0 self . done : bool = False self . pos : int = 0 self . _figure = None self . reset () def get_observation ( self ): return np . append ([ self . pos ], self . remaining_rewards ) def reset ( self ): self . total_reward = 0 self . done = False self . pos = math . floor ( len ( self . world ) / 2 ) self . steps = 0 self . remaining_rewards = np . array ( self . world , copy = True ) return self . get_observation () def step ( self , action ): \"\"\"perform action on this lineword. Args: action: 0 ==> move left, 1 ==> move right \"\"\" if isinstance ( action , np . ndarray ): assert action . size == 1 , \"action of type numpy.array as invalid size\" action = ( int )( action ) if isinstance ( action , np . int32 ): action = ( int )( action ) assert isinstance ( action , int ) if action <= 0 and self . pos > 0 : self . pos -= 1 if action > 0 and self . pos < self . size_of_world - 1 : self . pos += 1 reward = self . remaining_rewards [ self . pos ] - 1 self . total_reward += reward self . remaining_rewards [ self . pos ] = 0 self . done = ( self . pos == 0 ) or ( self . total_reward <= - 20 ) self . steps += 1 observation = self . get_observation () info = None return observation , reward , self . done , info def _render_to_ansi ( self ): return f 'position: {self.pos} , remaining rewards: {self.remaining_rewards} ,' + \\ f 'total reward so far: {self.total_reward} , steps so far: {self.steps} , game done: {self.done} ' def _render_to_figure ( self ): \"\"\" Renders the current state as a graph with matplotlib \"\"\" if self . _figure is not None : plt . close ( self . _figure ) self . _figure , ax = plt . subplots ( 1 , figsize = ( 8 , 4 )) ax . set_ylim ( bottom =- 1 , top = self . max_reward + 1 ) x = np . arange ( 0 , self . size_of_world , 1 , dtype = np . uint8 ) y = self . remaining_rewards plt . plot ([ self . pos , self . pos ], [ 0 , 2 ], 'r^-' ) ax . scatter ( x , y , s = 75 ) self . _figure . canvas . draw () return self . _figure def _render_to_rgb ( self ): \"\"\" convert the output of render_to_figure to a rgb_array \"\"\" self . _render_to_figure () self . _figure . canvas . draw () buf = self . _figure . canvas . tostring_rgb () num_cols , num_rows = self . _figure . canvas . get_width_height () plt . close ( self . _figure ) self . _figure = None result = np . fromstring ( buf , dtype = np . uint8 ) . reshape ( num_rows , num_cols , 3 ) return result def render ( self , mode = 'ansi' ): if mode == 'ansi' : return self . _render_to_ansi () elif mode == 'human' : return self . _render_to_figure () elif mode == 'rgb_array' : return self . _render_to_rgb () else : super () . render ( mode = mode ) class _ShimEnv ( gym . Wrapper ): \"\"\"Wrapper to redirect the instantiation of a gym environment to its current implementation. \"\"\" _KWARG_GYM_NAME = \"shimenv_gym_name\" _entry_points : Dict [ str , Tuple [ Callable , Dict ]] = {} def __init__ ( self , ** kwargs ): assert _ShimEnv . _KWARG_GYM_NAME in kwargs , f ' {_ShimEnv._KWARG_GYM_NAME} missing from kwargs' self . _gym_env_name = kwargs [ _ShimEnv . _KWARG_GYM_NAME ] entry_point , gym_kwargs = _ShimEnv . _entry_points [ self . _gym_env_name ] self . _gym_env = entry_point ( ** gym_kwargs ) super () . __init__ ( self . _gym_env ) def step ( self , action ): return self . env . step ( action ) def reset ( self , ** kwargs ): return self . env . reset ( ** kwargs ) class _StepCountEnv ( gym . core . Env ): \"\"\"Debug Env that runs forever, counting the calls to reset and step.\"\"\" metadata = { 'render.modes' : [ 'ansi' ]} reward_range = ( 0 , 1 ) max = 10 ** 7 action_space = gym . spaces . discrete . Discrete ( 2 ) observation_space = gym . spaces . Box ( low = 0 , high = max , shape = ( 1 ,)) step_count : int = 0 reset_count : int = 0 @staticmethod def register_with_gym (): \"\"\"Register this environment with gym and yields the gym environment name.\"\"\" result = \"_StepCountEnv-v0\" register_with_gym ( result , _StepCountEnv ) return result @staticmethod def clear (): _StepCountEnv . step_count = 0 _StepCountEnv . reset_count = 0 def __str__ ( self ): return f 'reset_count= {_StepCountEnv.reset_count} step_count= {_StepCountEnv.step_count} ' def step ( self , action ): _StepCountEnv . step_count += 1 # noinspection PyRedundantParentheses return [ _StepCountEnv . step_count ], 1 , False , None def reset ( self ): _StepCountEnv . reset_count += 1 return [ _StepCountEnv . step_count ] def render ( self , mode = 'ansi' ): return str ( self ) Functions register_with_gym def register_with_gym ( gym_env_name : str , entry_point : type , max_episode_steps : int = 100000 , ** kwargs ) Registers the class entry_point in gym by the name gym_env_name allowing overriding registrations. Thus different implementations of the same class (and the same name) maybe registered consecutively. The latest registrated version is used for instantiation. This facilitates developing an environment in a jupyter notebook without haveing to reregister a modified class under a new name. limitation: the max_episode_steps value of the first registration holds for all registrations with the same gym_env_name Args: gym_env_name: the gym environment name to be used as argument with gym.make max_episode_steps: all episodes end latest after this number of steps entry_point: the class to be registed with gym id gym_env_name kwargs: the args passed to the entry_point constructor call View Source def register_with_gym ( gym_env_name : str , entry_point : type , max_episode_steps : int = 100000 , ** kwargs ) : \"\"\"Registers the class entry_point in gym by the name gym_env_name allowing overriding registrations. Thus different implementations of the same class (and the same name) maybe registered consecutively. The latest registrated version is used for instantiation. This facilitates developing an environment in a jupyter notebook without haveing to reregister a modified class under a new name. limitation: the max_episode_steps value of the first registration holds for all registrations with the same gym_env_name Args: gym_env_name: the gym environment name to be used as argument with gym.make max_episode_steps: all episodes end latest after this number of steps entry_point: the class to be registed with gym id gym_env_name kwargs: the args passed to the entry_point constructor call \"\"\" assert gym_env_name is not None , \"None is not an admissible environment name\" assert type ( gym_env_name ) is str , \"gym_env_name is not a str\" assert len ( gym_env_name ) > 0 , \"empty string is not an admissible environment name\" assert inspect . isclass ( entry_point ), \"entry_point not a class\" assert issubclass ( entry_point , gym . Env ), \"entry_point not a subclass of gym.Env\" assert callable ( entry_point ), \"entry_point not callable\" if gym_env_name not in _ShimEnv . _entry_points : gym . envs . registration . register ( id = gym_env_name , entry_point = _ShimEnv , max_episode_steps = max_episode_steps , kwargs = { _ShimEnv . _KWARG_GYM_NAME : gym_env_name } ) _ShimEnv . _entry_points [ gym_env_name ] = ( entry_point , kwargs )","title":"Env"},{"location":"reference/easyagents/env/#module-easyagentsenv","text":"This module contains support classes and methods to interact with OpenAI gym environments as well as a gym env implementation for unit tests (linewordl) see https://github.com/openai/gym View Source \"\"\"This module contains support classes and methods to interact with OpenAI gym environments as well as a gym env implementation for unit tests (linewordl) see https://github.com/openai/gym \"\"\" import gym , gym.envs , gym.error , gym.spaces import inspect import math import matplotlib as plt import numpy as np from typing import List , Optional , Tuple , Dict , Callable , Any def _is_registered_with_gym ( gym_env_name : str ) -> bool : \"\"\"Determines if a gym environment with the name id exists. Args: gym_env_name: gym id to test. Returns: True if it exists, false otherwise \"\"\" result = False try : spec = gym . envs . registration . spec ( gym_env_name ) assert spec is not None result = True except gym . error . UnregisteredEnv : pass return result # noinspection DuplicatedCode def register_with_gym ( gym_env_name : str , entry_point : type , max_episode_steps : int = 100000 , ** kwargs ): \"\"\"Registers the class entry_point in gym by the name gym_env_name allowing overriding registrations. Thus different implementations of the same class (and the same name) maybe registered consecutively. The latest registrated version is used for instantiation. This facilitates developing an environment in a jupyter notebook without haveing to reregister a modified class under a new name. limitation: the max_episode_steps value of the first registration holds for all registrations with the same gym_env_name Args: gym_env_name: the gym environment name to be used as argument with gym.make max_episode_steps: all episodes end latest after this number of steps entry_point: the class to be registed with gym id gym_env_name kwargs: the args passed to the entry_point constructor call \"\"\" assert gym_env_name is not None , \"None is not an admissible environment name\" assert type ( gym_env_name ) is str , \"gym_env_name is not a str\" assert len ( gym_env_name ) > 0 , \"empty string is not an admissible environment name\" assert inspect . isclass ( entry_point ), \"entry_point not a class\" assert issubclass ( entry_point , gym . Env ), \"entry_point not a subclass of gym.Env\" assert callable ( entry_point ), \"entry_point not callable\" if gym_env_name not in _ShimEnv . _entry_points : gym . envs . registration . register ( id = gym_env_name , entry_point = _ShimEnv , max_episode_steps = max_episode_steps , kwargs = { _ShimEnv . _KWARG_GYM_NAME : gym_env_name }) _ShimEnv . _entry_points [ gym_env_name ] = ( entry_point , kwargs ) class _LineWorldEnv ( gym . Env ): \"\"\"Simple environment for fast unittest, registered as 'LineWorld-v0' * an agent lives in a finite linear world of uneven elements * at each moment it is in a certain position * initial position is the middle * some positions gain rewards, some don't * rewards are between 0 and 15 * agent can either move left or right * objective: maximize total reward = sum(rewards) + sum(steps) * Cost per step: -1 * Done Condition: agent is at pos 0 or total reward <= -20 \"\"\" @staticmethod def register_with_gym (): \"\"\"Register this environment with gym and yields the gym environment name.\"\"\" result = \"LineWorld-v0\" register_with_gym ( result , _LineWorldEnv ) return result def __init__ ( self , world : Optional [ List [ int ]] = None ): \"\"\"Creates the lineword, size and rewards are given by the world arg. Args: world: list of rewards to collect in each position of the lineworld. \"\"\" if world is None : world = [ 10 , 0 , 0 , 5 , 0 , 2 , 15 ] assert world , \"world must not be None or empty.\" self . world : np . array = np . array ( world ) number_of_actions : int = 2 self . action_space : gym . spaces . Discrete = gym . spaces . Discrete ( number_of_actions ) self . size_of_world : int = len ( world ) self . max_reward : int = max ( world ) self . min_reward : int = min ( world ) # the environment's current state is described by the position of the agent and the remaining rewards self . observation_size : int = 1 + self . size_of_world low : np . array = np . full ( self . observation_size , self . min_reward ) high : np . array = np . full ( self . observation_size , self . max_reward ) self . observation_space : gym . spaces . Box = gym . spaces . Box ( low = low , high = high , dtype = np . float32 ) self . reward_range : Tuple [ int , int ] = ( self . min_reward , self . max_reward ) self . steps : int = 0 self . done : bool = False self . pos : int = 0 self . _figure = None self . reset () def get_observation ( self ): return np . append ([ self . pos ], self . remaining_rewards ) def reset ( self ): self . total_reward = 0 self . done = False self . pos = math . floor ( len ( self . world ) / 2 ) self . steps = 0 self . remaining_rewards = np . array ( self . world , copy = True ) return self . get_observation () def step ( self , action ): \"\"\"perform action on this lineword. Args: action: 0 ==> move left, 1 ==> move right \"\"\" if isinstance ( action , np . ndarray ): assert action . size == 1 , \"action of type numpy.array as invalid size\" action = ( int )( action ) if isinstance ( action , np . int32 ): action = ( int )( action ) assert isinstance ( action , int ) if action <= 0 and self . pos > 0 : self . pos -= 1 if action > 0 and self . pos < self . size_of_world - 1 : self . pos += 1 reward = self . remaining_rewards [ self . pos ] - 1 self . total_reward += reward self . remaining_rewards [ self . pos ] = 0 self . done = ( self . pos == 0 ) or ( self . total_reward <= - 20 ) self . steps += 1 observation = self . get_observation () info = None return observation , reward , self . done , info def _render_to_ansi ( self ): return f 'position: {self.pos} , remaining rewards: {self.remaining_rewards} ,' + \\ f 'total reward so far: {self.total_reward} , steps so far: {self.steps} , game done: {self.done} ' def _render_to_figure ( self ): \"\"\" Renders the current state as a graph with matplotlib \"\"\" if self . _figure is not None : plt . close ( self . _figure ) self . _figure , ax = plt . subplots ( 1 , figsize = ( 8 , 4 )) ax . set_ylim ( bottom =- 1 , top = self . max_reward + 1 ) x = np . arange ( 0 , self . size_of_world , 1 , dtype = np . uint8 ) y = self . remaining_rewards plt . plot ([ self . pos , self . pos ], [ 0 , 2 ], 'r^-' ) ax . scatter ( x , y , s = 75 ) self . _figure . canvas . draw () return self . _figure def _render_to_rgb ( self ): \"\"\" convert the output of render_to_figure to a rgb_array \"\"\" self . _render_to_figure () self . _figure . canvas . draw () buf = self . _figure . canvas . tostring_rgb () num_cols , num_rows = self . _figure . canvas . get_width_height () plt . close ( self . _figure ) self . _figure = None result = np . fromstring ( buf , dtype = np . uint8 ) . reshape ( num_rows , num_cols , 3 ) return result def render ( self , mode = 'ansi' ): if mode == 'ansi' : return self . _render_to_ansi () elif mode == 'human' : return self . _render_to_figure () elif mode == 'rgb_array' : return self . _render_to_rgb () else : super () . render ( mode = mode ) class _ShimEnv ( gym . Wrapper ): \"\"\"Wrapper to redirect the instantiation of a gym environment to its current implementation. \"\"\" _KWARG_GYM_NAME = \"shimenv_gym_name\" _entry_points : Dict [ str , Tuple [ Callable , Dict ]] = {} def __init__ ( self , ** kwargs ): assert _ShimEnv . _KWARG_GYM_NAME in kwargs , f ' {_ShimEnv._KWARG_GYM_NAME} missing from kwargs' self . _gym_env_name = kwargs [ _ShimEnv . _KWARG_GYM_NAME ] entry_point , gym_kwargs = _ShimEnv . _entry_points [ self . _gym_env_name ] self . _gym_env = entry_point ( ** gym_kwargs ) super () . __init__ ( self . _gym_env ) def step ( self , action ): return self . env . step ( action ) def reset ( self , ** kwargs ): return self . env . reset ( ** kwargs ) class _StepCountEnv ( gym . core . Env ): \"\"\"Debug Env that runs forever, counting the calls to reset and step.\"\"\" metadata = { 'render.modes' : [ 'ansi' ]} reward_range = ( 0 , 1 ) max = 10 ** 7 action_space = gym . spaces . discrete . Discrete ( 2 ) observation_space = gym . spaces . Box ( low = 0 , high = max , shape = ( 1 ,)) step_count : int = 0 reset_count : int = 0 @staticmethod def register_with_gym (): \"\"\"Register this environment with gym and yields the gym environment name.\"\"\" result = \"_StepCountEnv-v0\" register_with_gym ( result , _StepCountEnv ) return result @staticmethod def clear (): _StepCountEnv . step_count = 0 _StepCountEnv . reset_count = 0 def __str__ ( self ): return f 'reset_count= {_StepCountEnv.reset_count} step_count= {_StepCountEnv.step_count} ' def step ( self , action ): _StepCountEnv . step_count += 1 # noinspection PyRedundantParentheses return [ _StepCountEnv . step_count ], 1 , False , None def reset ( self ): _StepCountEnv . reset_count += 1 return [ _StepCountEnv . step_count ] def render ( self , mode = 'ansi' ): return str ( self )","title":"Module easyagents.env"},{"location":"reference/easyagents/env/#functions","text":"","title":"Functions"},{"location":"reference/easyagents/env/#register_with_gym","text":"def register_with_gym ( gym_env_name : str , entry_point : type , max_episode_steps : int = 100000 , ** kwargs ) Registers the class entry_point in gym by the name gym_env_name allowing overriding registrations. Thus different implementations of the same class (and the same name) maybe registered consecutively. The latest registrated version is used for instantiation. This facilitates developing an environment in a jupyter notebook without haveing to reregister a modified class under a new name. limitation: the max_episode_steps value of the first registration holds for all registrations with the same gym_env_name Args: gym_env_name: the gym environment name to be used as argument with gym.make max_episode_steps: all episodes end latest after this number of steps entry_point: the class to be registed with gym id gym_env_name kwargs: the args passed to the entry_point constructor call View Source def register_with_gym ( gym_env_name : str , entry_point : type , max_episode_steps : int = 100000 , ** kwargs ) : \"\"\"Registers the class entry_point in gym by the name gym_env_name allowing overriding registrations. Thus different implementations of the same class (and the same name) maybe registered consecutively. The latest registrated version is used for instantiation. This facilitates developing an environment in a jupyter notebook without haveing to reregister a modified class under a new name. limitation: the max_episode_steps value of the first registration holds for all registrations with the same gym_env_name Args: gym_env_name: the gym environment name to be used as argument with gym.make max_episode_steps: all episodes end latest after this number of steps entry_point: the class to be registed with gym id gym_env_name kwargs: the args passed to the entry_point constructor call \"\"\" assert gym_env_name is not None , \"None is not an admissible environment name\" assert type ( gym_env_name ) is str , \"gym_env_name is not a str\" assert len ( gym_env_name ) > 0 , \"empty string is not an admissible environment name\" assert inspect . isclass ( entry_point ), \"entry_point not a class\" assert issubclass ( entry_point , gym . Env ), \"entry_point not a subclass of gym.Env\" assert callable ( entry_point ), \"entry_point not callable\" if gym_env_name not in _ShimEnv . _entry_points : gym . envs . registration . register ( id = gym_env_name , entry_point = _ShimEnv , max_episode_steps = max_episode_steps , kwargs = { _ShimEnv . _KWARG_GYM_NAME : gym_env_name } ) _ShimEnv . _entry_points [ gym_env_name ] = ( entry_point , kwargs )","title":"register_with_gym"},{"location":"reference/easyagents/backends/core/","text":"Module easyagents.backends.core This module contains backend core classes like Backend and BackendAgent. The concrete backends like tfagent or baselines are implemented in seprate modules. View Source \"\"\"This module contains backend core classes like Backend and BackendAgent. The concrete backends like tfagent or baselines are implemented in seprate modules. \"\"\" from abc import ABC , ABCMeta , abstractmethod from typing import List , Optional , Tuple , Type , Dict import datetime import gym import os import shutil import tempfile import tensorflow import numpy import random from easyagents import core from easyagents.backends import monitor from easyagents.callbacks import plot _tf_eager_execution_active : Optional [ bool ] = None def _get_temp_path (): \"\"\"Yields a path to a non-existent temporary directory inside the systems temp path.\"\"\" result = os . path . join ( tempfile . gettempdir (), tempfile . gettempprefix ()) n = datetime . datetime . now () result = result + f '-{n.year % 100:2} {n.month:02}{n.day:02} - {n.hour:02}{n.minute:02}{n.second:02} -' + \\ f ' {n.microsecond:06} ' return result def _mkdir ( directory : str ): \"\"\"Creates a directory with the given path. NoOps if the directory already exists. If a file exists at path, the file is removed. Returns: the absolute path to directory \"\"\" directory = os . path . abspath ( directory ) if os . path . isfile ( directory ): _rmpath ( directory ) os . makedirs ( directory , exist_ok = True ) return directory def _rmpath ( path : str ): \"\"\"Removes the file or directory and its content. NoOps if the directory does not exist. Errors are ignored. \"\"\" if path : if os . path . isdir ( path ): shutil . rmtree ( path , ignore_errors = True ) if os . path . isfile ( path ): os . remove ( path ) class _BackendEvalCallback ( core . AgentCallback ): \"\"\"Evaluates an agents current policy and updates its train_context accordingly.\"\"\" def __init__ ( self , train_context : core . TrainContext ): assert train_context , \"train_context not set\" assert train_context . num_episodes_per_eval > 0 , \"num_episodes_per_eval is 0.\" self . _train_contex = train_context def on_play_episode_end ( self , agent_context : core . AgentContext ): pc = agent_context . play tc = self . _train_contex sum_of_r = pc . sum_of_rewards . values () tc . eval_rewards [ tc . episodes_done_in_training ] = ( min ( sum_of_r ), sum ( sum_of_r ) / len ( sum_of_r ), max ( sum_of_r )) steps = [ len ( episode_rewards ) for episode_rewards in pc . rewards . values ()] tc . eval_steps [ tc . episodes_done_in_training ] = ( min ( steps ), sum ( steps ) / len ( steps ), max ( steps )) class _BackendAgent ( ABC ): \"\"\"Base class for all backend agent implementations. Implements the train loop and calls the Callbacks. \"\"\" def __init__ ( self , model_config : core . ModelConfig , backend_name : str , tf_eager_execution : bool ): \"\"\" Args: model_config: defines the model and environment to be used backend_name: id of the backend to which this agent belongs to. \"\"\" global _tf_eager_execution_active assert model_config is not None , \"model_config not set.\" assert backend_name if _tf_eager_execution_active is None : _tf_eager_execution_active = tf_eager_execution assert _tf_eager_execution_active == tf_eager_execution , \\ \"Due to an incompatibility between tensorforce and tfagents their agents can not be instantiated in the\" + \\ \"same python runtime instance (conflicting excpectations on tensorflows eager execution mode).\" self . _backend_name : str = backend_name self . model_config = model_config self . _agent_context : core . AgentContext = core . AgentContext ( self . model_config ) self . _agent_context . gym . _totals = monitor . _register_gym_monitor ( self . model_config . original_env_name ) self . model_config . gym_env_name = self . _agent_context . gym . _totals . gym_env_name self . _preprocess_callbacks : List [ core . _PreProcessCallback ] = [ plot . _PreProcess ()] self . _callbacks : List [ core . AgentCallback ] = [] self . _postprocess_callbacks : List [ core . _PostProcessCallback ] = [ plot . _PostProcess ()] self . _train_total_episodes_on_iteration_begin : int = 0 def _set_seed ( self ): \"\"\" sets the random seeds for all dependent packages \"\"\" if not self . model_config . seed is None : seed = self . model_config . seed self . log_api ( f 'tf.random.set_seed' , f '(seed= {seed} )' ) tensorflow . random . set_seed ( seed = seed ) self . log_api ( f 'numpy.random.seed' , f '( {seed} )' ) numpy . random . seed ( seed ) self . log_api ( f 'random.seed' , f '( {seed} )' ) random . seed ( seed ) return def _eval_current_policy ( self ): \"\"\"Evaluates the current policy using play and updates the train_context If num_episodes_per_eval or num_iterations_per_eval is 0 no evaluation is performed. \"\"\" tc = self . _agent_context . train assert tc , \"train_context not set\" if tc . num_episodes_per_eval and tc . num_iterations_between_eval : callbacks = [ _BackendEvalCallback ( self . _agent_context . train )] + self . _callbacks self . play ( play_context = core . PlayContext ( self . _agent_context . train ), callbacks = callbacks ) def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ): \"\"\"Logs a call to api_target with additional log_msg.\"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg ) def log ( self , log_msg : str ): \"\"\"Logs msg.\"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg ) def _on_gym_init_begin ( self ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Hint: the total instances count is not incremented yet.\"\"\" self . _agent_context . gym . _monitor_env = None for c in self . _callbacks : c . on_gym_init_begin ( self . _agent_context ) self . _agent_context . gym . _monitor_env = None def _on_gym_init_end ( self , env : monitor . _MonitorEnv ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Hint: o the total instances count is incremented by now o the new env (and its action space) is seeded with the api_context's seed \"\"\" self . _agent_context . gym . _monitor_env = env if self . _agent_context . model . seed is not None : env = self . _agent_context . gym . gym_env seed = self . _agent_context . model . seed env . seed ( seed ) for c in self . _callbacks : c . on_gym_init_end ( self . _agent_context ) self . _agent_context . gym . _monitor_env = None def _on_gym_reset_begin ( self , env : monitor . _MonitorEnv , ** kwargs ): \"\"\"called when the monitored environment begins a reset. Hint: the total reset count is not incremented yet.\"\"\" self . _agent_context . gym . _monitor_env = env for c in self . _callbacks : c . on_gym_reset_begin ( self . _agent_context , ** kwargs ) self . _agent_context . gym . _monitor_env = None def _on_gym_reset_end ( self , env : monitor . _MonitorEnv , reset_result : Tuple , ** kwargs ): \"\"\"called when the monitored environment completed a reset. Hint: the total episode count is incremented by now (if a step was performed before the last reset).\"\"\" self . _agent_context . gym . _monitor_env = env for c in self . _callbacks : c . on_gym_reset_end ( self . _agent_context , reset_result , ** kwargs ) self . _agent_context . gym . _monitor_env = None def _on_gym_step_begin ( self , env : monitor . _MonitorEnv , action ): \"\"\"called when the monitored environment begins a step. Hint: o sets env.max_steps_per_episode if we are in train / play. Thus the episode is ended by the MonitorEnv if the step limit is exceeded \"\"\" ac = self . _agent_context ac . gym . _monitor_env = env env . max_steps_per_episode = None if ac . is_play or ac . is_eval : env . max_steps_per_episode = ac . play . max_steps_per_episode self . _on_play_step_begin ( action ) if ac . is_train : env . max_steps_per_episode = ac . train . max_steps_per_episode self . _on_train_step_begin ( action ) for c in self . _callbacks : c . on_gym_step_begin ( self . _agent_context , action ) self . _agent_context . gym . _monitor_env = None def _on_gym_step_end ( self , env : monitor . _MonitorEnv , action , step_result : Tuple ): \"\"\"called when the monitored environment completed a step. Args: env: the gym_env the last step was done on step_result: the result (state, reward, done, info) of the last step call \"\"\" ac = self . _agent_context ac . gym . _monitor_env = env if ac . is_play or ac . is_eval : self . _on_play_step_end ( action , step_result ) if ac . is_train : self . _on_train_step_end ( action , step_result ) for c in self . _callbacks : c . on_gym_step_end ( self . _agent_context , action , step_result ) self . _agent_context . gym . _monitor_env = None env . max_steps_per_episode = None def _on_play_begin ( self ): \"\"\"Must NOT be called by play_implementation\"\"\" for c in self . _callbacks : c . on_play_begin ( self . _agent_context ) def _on_play_end ( self ): \"\"\"Must NOT be called by play_implementation\"\"\" for c in self . _callbacks : c . on_play_end ( self . _agent_context ) self . _agent_context . play . gym_env = None def on_play_episode_begin ( self , env : gym . core . Env ): \"\"\"Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. \"\"\" assert env , \"env not set.\" assert isinstance ( env , gym . core . Env ), \"env not an an instance of gym.Env.\" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context ) def on_play_episode_end ( self ): \"\"\"Must be called by play_implementation at the end of an episode\"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context ) def _on_play_step_begin ( self , action ): \"\"\"Called before each call to gym.step on the current play env (agent_context.play.gym_env) Args: action: the action to be passed to the upcoming gym_env.step call \"\"\" for c in self . _callbacks : c . on_play_step_begin ( self . _agent_context , action ) def _on_play_step_end ( self , action , step_result : Tuple ): \"\"\"Called after each call to gym.step on the current play env (agent_context.play.gym_env) Args: step_result: the result (state, reward, done, info) of the last step call \"\"\" ( state , reward , done , info ) = step_result pc = self . _agent_context . play pc . steps_done_in_episode += 1 pc . steps_done += 1 pc . actions [ pc . episodes_done + 1 ] . append ( action ) pc . rewards [ pc . episodes_done + 1 ] . append ( reward ) pc . sum_of_rewards [ pc . episodes_done + 1 ] += reward for c in self . _callbacks : c . on_play_step_end ( self . _agent_context , action , step_result ) def _on_train_begin ( self ): \"\"\"Must NOT be called by train_implementation\"\"\" for c in self . _callbacks : c . on_train_begin ( self . _agent_context ) def _on_train_end ( self ): \"\"\"Must NOT be called by train_implementation\"\"\" tc = self . _agent_context . train if tc . episodes_done_in_training not in tc . eval_rewards : self . _eval_current_policy () for c in self . _callbacks : c . on_train_end ( self . _agent_context ) def on_train_iteration_begin ( self ): \"\"\"Must be called by train_implementation at the begining of a new iteration\"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context ) def on_train_iteration_end ( self , loss : float , ** kwargs ): \"\"\"Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc . episodes_done_in_training ] = loss # set traincontext dict from kwargs: for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ): prop_instance [ tc . episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ): self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context ) def _on_train_step_begin ( self , action ): \"\"\"Called before each call to gym.step on the current train env (agent_context.train.gym_env) Args: action: the action to be passed to the upcoming gym_env.step call \"\"\" pass def load ( self , directory : str , callbacks : List [ core . AgentCallback ]): \"\"\"Loads a previously trained and saved actor policy from directory. The loaded policy may afterwards be used by calling play(). Args: directory: the directory containing the trained policy callbacks: list of callbacks called during the load. \"\"\" assert callbacks is not None , \"callbacks not set\" assert directory assert os . path . isdir ( directory ) self . _callbacks = callbacks self . load_implementation ( directory ) self . _agent_context . _is_policy_trained = True self . _callbacks = None @abstractmethod def load_implementation ( self , directory : str ): \"\"\"Loads a previously trained and saved actor policy from directory. The loaded policy may afterwards be used by calling play(). Args: directory: the directory containing the trained policy. \"\"\" # noinspection PyUnusedLocal def _on_train_step_end ( self , action : object , step_result : Tuple ): \"\"\"Called after each call to gym.step on the current train env (agent_context.train.gym_env) Args: step_result: the result (state, reward, done, info) of the last step call \"\"\" tc = self . _agent_context . train tc . steps_done_in_iteration += 1 tc . steps_done_in_training += 1 def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None @abstractmethod def play_implementation ( self , play_context : core . PlayContext ): \"\"\"Agent specific implementation of playing a single episode with the current policy. For implementation details see BackendBaseAgent. \"\"\" def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f ' {self._backend_name} ' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . _agent_context . _is_policy_trained = True self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None @abstractmethod def train_implementation ( self , train_context : core . TrainContext ): \"\"\"Agent specific implementation of the train loop. For implementational details see BackendBaseAgent. \"\"\" def save ( self , directory : str , callbacks : List [ core . AgentCallback ]): \"\"\"Saves the currently trained actor policy in directory. Only the actor policy is guaranteed to be saved. Thus after a call to load resuming training is not supported. Args: directory: the directory to save the policy weights to. the directory must exist. callbacks: list of callbacks called during policy load. \"\"\" assert callbacks is not None , \"callbacks not set\" assert directory assert os . path . isdir ( directory ) assert self . _agent_context . _is_policy_trained , \"No trained policy available.\" self . _callbacks = callbacks self . save_implementation ( directory ) self . _callbacks = None @abstractmethod def save_implementation ( self , directory : str ): \"\"\"Agent speecific implementation of saving the weights for the actor policy. Save must only guarantee to persist the weights of the actor policy. The implementation may write multiple files with fixed filenames. Args: directory: the (existing) directory to save the policy weights to. \"\"\" class BackendAgent ( _BackendAgent , metaclass = ABCMeta ): \"\"\"Base class for all BackendAgent implementation. Explicitely exhibits all methods that should be overriden by an implementing agent. \"\"\" @abstractmethod def load_implementation ( self , directory : str ): \"\"\"Loads a previously trained and saved actor policy from directory. The loaded policy may afterwards be used by calling play(). Args: directory: the directory containing the trained policy. \"\"\" @abstractmethod def play_implementation ( self , play_context : core . PlayContext ): \"\"\"Agent specific implementation of playing a number of episodes with the current policy. The implementation should have the form: while True: on_play_episode_begin(env) state = env.reset() while True: action = _trained_policy.action(state) (state, reward, done, info) = env.step(action) if done: break on_play_episode_end() if play_context.play_done: break Args: play_context: play configuration to be used \"\"\" @abstractmethod def save_implementation ( self , directory : str ): \"\"\"Agent speecific implementation of saving the weights for the actor policy. Save must only guarantee to persist the weights of the actor policy. The implementation may write multiple files with fixed filenames. Args: directory: the directory to save the policy weights to. \"\"\" @abstractmethod def train_implementation ( self , train_context : core . TrainContext ): \"\"\"Agent specific implementation of the train loop. The implementation should have the form: while True: on_iteration_begin for e in num_episodes_per_iterations play episode and record steps (while steps_in_episode < max_steps_per_episode and) train policy for num_epochs_per_iteration epochs on_iteration_end( loss ) if training_done break Args: train_context: context configuring the train loop Hints: o the subclasses training loss is passed through to BackendAgent by on_iteration_end. Thus the subclass must not add the experienced loss to the TrainContext. \"\"\" class BackendAgentFactory ( ABC ): \"\"\"Backend agent factory defining the currently available agents (algorithms). \"\"\" backend_name : str = 'abstract_BackendAgentFactory' def create_agent ( self , easyagent_type : Type , model_config : core . ModelConfig ) \\ -> Optional [ _BackendAgent ]: \"\"\"Creates a backend agent instance implementing the algorithm given by agent_type. Args: easyagent_type: the EasyAgent derived type for which an implementing backend instance will be created model_config: the model_config passed to the constructor of the backend instance. Returns: instance of the agent or None if not implemented by this backend. \"\"\" result : Optional [ _BackendAgent ] = None algorithms = self . get_algorithms () if easyagent_type in algorithms : result = algorithms [ easyagent_type ]( model_config = model_config ) return result def get_algorithms ( self ) -> Dict [ Type , Type [ _BackendAgent ]]: \"\"\"Yields a mapping of EasyAgent types to the implementations provided by this backend.\"\"\" return {} Classes BackendAgent class BackendAgent ( model_config : easyagents . core . ModelConfig , backend_name : str , tf_eager_execution : bool ) Base class for all BackendAgent implementation. Explicitely exhibits all methods that should be overriden by an implementing agent. View Source class BackendAgent ( _BackendAgent , metaclass = ABCMeta ) : \"\"\"Base class for all BackendAgent implementation. Explicitely exhibits all methods that should be overriden by an implementing agent. \"\"\" @abstractmethod def load_implementation ( self , directory : str ) : \"\"\"Loads a previously trained and saved actor policy from directory. The loaded policy may afterwards be used by calling play(). Args: directory: the directory containing the trained policy. \"\"\" @abstractmethod def play_implementation ( self , play_context : core . PlayContext ) : \"\"\"Agent specific implementation of playing a number of episodes with the current policy. The implementation should have the form: while True: on_play_episode_begin(env) state = env.reset() while True: action = _trained_policy.action(state) (state, reward, done, info) = env.step(action) if done: break on_play_episode_end() if play_context.play_done: break Args: play_context: play configuration to be used \"\"\" @abstractmethod def save_implementation ( self , directory : str ) : \"\"\"Agent speecific implementation of saving the weights for the actor policy. Save must only guarantee to persist the weights of the actor policy. The implementation may write multiple files with fixed filenames. Args: directory: the directory to save the policy weights to. \"\"\" @abstractmethod def train_implementation ( self , train_context : core . TrainContext ) : \"\"\"Agent specific implementation of the train loop. The implementation should have the form: while True: on_iteration_begin for e in num_episodes_per_iterations play episode and record steps (while steps_in_episode < max_steps_per_episode and) train policy for num_epochs_per_iteration epochs on_iteration_end( loss ) if training_done break Args: train_context: context configuring the train loop Hints: o the subclasses training loss is passed through to BackendAgent by on_iteration_end. Thus the subclass must not add the experienced loss to the TrainContext. \"\"\" Ancestors (in MRO) easyagents.backends.core._BackendAgent abc.ABC Descendants easyagents.backends.default.TensorforceNotActiveAgent easyagents.backends.default.TfAgentsNotActiveAgent easyagents.backends.default.SetTensorforceBackendAgent easyagents.backends.default.NotImplementedYetAgent easyagents.backends.tfagents.TfAgent Methods load def load ( self , directory : str , callbacks : List [ easyagents . core . AgentCallback ] ) Loads a previously trained and saved actor policy from directory. The loaded policy may afterwards be used by calling play(). Args: directory: the directory containing the trained policy callbacks: list of callbacks called during the load. View Source def load ( self , directory : str , callbacks : List [ core . AgentCallback ]): \"\"\"Loads a previously trained and saved actor policy from directory. The loaded policy may afterwards be used by calling play(). Args: directory: the directory containing the trained policy callbacks: list of callbacks called during the load. \"\"\" assert callbacks is not None , \"callbacks not set\" assert directory assert os . path . isdir ( directory ) self . _callbacks = callbacks self . load_implementation ( directory ) self . _agent_context . _is_policy_trained = True self . _callbacks = None load_implementation def load_implementation ( self , directory : str ) Loads a previously trained and saved actor policy from directory. The loaded policy may afterwards be used by calling play(). Args: directory: the directory containing the trained policy. View Source @abstractmethod def load_implementation ( self , directory : str ) : \"\"\"Loads a previously trained and saved actor policy from directory. The loaded policy may afterwards be used by calling play(). Args: directory: the directory containing the trained policy. \"\"\" log def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ): \"\"\"Logs msg.\"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg ) log_api def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\"Logs a call to api_target with additional log_msg.\"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg ) on_play_episode_begin def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ): \"\"\"Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. \"\"\" assert env , \"env not set.\" assert isinstance ( env , gym . core . Env ), \"env not an an instance of gym.Env.\" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context ) on_play_episode_end def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ): \"\"\"Must be called by play_implementation at the end of an episode\"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context ) on_train_iteration_begin def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ): \"\"\"Must be called by train_implementation at the begining of a new iteration\"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context ) on_train_iteration_end def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\"Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc.episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc.episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context ) play def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None play_implementation def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a number of episodes with the current policy. The implementation should have the form: while True: on_play_episode_begin(env) state = env.reset() while True: action = _trained_policy.action(state) (state, reward, done, info) = env.step(action) if done: break on_play_episode_end() if play_context.play_done: break Args: play_context: play configuration to be used View Source @abstractmethod def play_implementation ( self , play_context : core . PlayContext ) : \"\"\"Agent specific implementation of playing a number of episodes with the current policy. The implementation should have the form: while True: on_play_episode_begin(env) state = env.reset() while True: action = _trained_policy.action(state) (state, reward, done, info) = env.step(action) if done: break on_play_episode_end() if play_context.play_done: break Args: play_context: play configuration to be used \"\"\" save def save ( self , directory : str , callbacks : List [ easyagents . core . AgentCallback ] ) Saves the currently trained actor policy in directory. Only the actor policy is guaranteed to be saved. Thus after a call to load resuming training is not supported. Args: directory: the directory to save the policy weights to. the directory must exist. callbacks: list of callbacks called during policy load. View Source def save ( self , directory : str , callbacks : List [ core . AgentCallback ]): \"\"\"Saves the currently trained actor policy in directory. Only the actor policy is guaranteed to be saved. Thus after a call to load resuming training is not supported. Args: directory: the directory to save the policy weights to. the directory must exist. callbacks: list of callbacks called during policy load. \"\"\" assert callbacks is not None , \"callbacks not set\" assert directory assert os . path . isdir ( directory ) assert self . _agent_context . _is_policy_trained , \"No trained policy available.\" self . _callbacks = callbacks self . save_implementation ( directory ) self . _callbacks = None save_implementation def save_implementation ( self , directory : str ) Agent speecific implementation of saving the weights for the actor policy. Save must only guarantee to persist the weights of the actor policy. The implementation may write multiple files with fixed filenames. Args: directory: the directory to save the policy weights to. View Source @abstractmethod def save_implementation ( self , directory : str ) : \"\"\"Agent speecific implementation of saving the weights for the actor policy. Save must only guarantee to persist the weights of the actor policy. The implementation may write multiple files with fixed filenames. Args: directory: the directory to save the policy weights to. \"\"\" train def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . _agent_context . _is_policy_trained = True self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None train_implementation def train_implementation ( self , train_context : easyagents . core . TrainContext ) Agent specific implementation of the train loop. The implementation should have the form: while True: on_iteration_begin for e in num_episodes_per_iterations play episode and record steps (while steps_in_episode < max_steps_per_episode and) train policy for num_epochs_per_iteration epochs on_iteration_end( loss ) if training_done break Args: train_context: context configuring the train loop Hints: o the subclasses training loss is passed through to BackendAgent by on_iteration_end. Thus the subclass must not add the experienced loss to the TrainContext. View Source @abstractmethod def train_implementation ( self , train_context : core . TrainContext ) : \"\"\"Agent specific implementation of the train loop. The implementation should have the form: while True: on_iteration_begin for e in num_episodes_per_iterations play episode and record steps (while steps_in_episode < max_steps_per_episode and) train policy for num_epochs_per_iteration epochs on_iteration_end( loss ) if training_done break Args: train_context: context configuring the train loop Hints: o the subclasses training loss is passed through to BackendAgent by on_iteration_end. Thus the subclass must not add the experienced loss to the TrainContext. \"\"\" BackendAgentFactory class BackendAgentFactory ( / , * args , ** kwargs ) Backend agent factory defining the currently available agents (algorithms). View Source class BackendAgentFactory ( ABC ) : \"\"\"Backend agent factory defining the currently available agents (algorithms). \"\"\" backend_name : str = 'abstract_BackendAgentFactory' def create_agent ( self , easyagent_type : Type , model_config : core . ModelConfig ) \\ -> Optional [ _BackendAgent ] : \"\"\"Creates a backend agent instance implementing the algorithm given by agent_type. Args: easyagent_type: the EasyAgent derived type for which an implementing backend instance will be created model_config: the model_config passed to the constructor of the backend instance. Returns: instance of the agent or None if not implemented by this backend. \"\"\" result : Optional [ _BackendAgent ] = None algorithms = self . get_algorithms () if easyagent_type in algorithms : result = algorithms [ easyagent_type ] ( model_config = model_config ) return result def get_algorithms ( self ) -> Dict [ Type, Type[_BackendAgent ] ]: \"\"\"Yields a mapping of EasyAgent types to the implementations provided by this backend.\"\"\" return {} Ancestors (in MRO) abc.ABC Descendants easyagents.backends.default.DefaultAgentFactory easyagents.backends.tfagents.TfAgentAgentFactory Class variables backend_name Methods create_agent def create_agent ( self , easyagent_type : Type , model_config : easyagents . core . ModelConfig ) -> Union [ easyagents . backends . core . _BackendAgent , NoneType ] Creates a backend agent instance implementing the algorithm given by agent_type. Args: easyagent_type: the EasyAgent derived type for which an implementing backend instance will be created model_config: the model_config passed to the constructor of the backend instance. Returns: instance of the agent or None if not implemented by this backend. View Source def create_agent ( self , easyagent_type : Type , model_config : core . ModelConfig ) \\ -> Optional [ _BackendAgent ] : \"\"\"Creates a backend agent instance implementing the algorithm given by agent_type. Args: easyagent_type: the EasyAgent derived type for which an implementing backend instance will be created model_config: the model_config passed to the constructor of the backend instance. Returns: instance of the agent or None if not implemented by this backend. \"\"\" result : Optional [ _BackendAgent ] = None algorithms = self . get_algorithms () if easyagent_type in algorithms : result = algorithms [ easyagent_type ] ( model_config = model_config ) return result get_algorithms def get_algorithms ( self ) -> Dict [ Type , Type [ easyagents . backends . core . _BackendAgent ]] Yields a mapping of EasyAgent types to the implementations provided by this backend. View Source def get_algorithms ( self ) -> Dict [ Type, Type[_BackendAgent ] ]: \"\"\"Yields a mapping of EasyAgent types to the implementations provided by this backend.\"\"\" return {}","title":"Core"},{"location":"reference/easyagents/backends/core/#module-easyagentsbackendscore","text":"This module contains backend core classes like Backend and BackendAgent. The concrete backends like tfagent or baselines are implemented in seprate modules. View Source \"\"\"This module contains backend core classes like Backend and BackendAgent. The concrete backends like tfagent or baselines are implemented in seprate modules. \"\"\" from abc import ABC , ABCMeta , abstractmethod from typing import List , Optional , Tuple , Type , Dict import datetime import gym import os import shutil import tempfile import tensorflow import numpy import random from easyagents import core from easyagents.backends import monitor from easyagents.callbacks import plot _tf_eager_execution_active : Optional [ bool ] = None def _get_temp_path (): \"\"\"Yields a path to a non-existent temporary directory inside the systems temp path.\"\"\" result = os . path . join ( tempfile . gettempdir (), tempfile . gettempprefix ()) n = datetime . datetime . now () result = result + f '-{n.year % 100:2} {n.month:02}{n.day:02} - {n.hour:02}{n.minute:02}{n.second:02} -' + \\ f ' {n.microsecond:06} ' return result def _mkdir ( directory : str ): \"\"\"Creates a directory with the given path. NoOps if the directory already exists. If a file exists at path, the file is removed. Returns: the absolute path to directory \"\"\" directory = os . path . abspath ( directory ) if os . path . isfile ( directory ): _rmpath ( directory ) os . makedirs ( directory , exist_ok = True ) return directory def _rmpath ( path : str ): \"\"\"Removes the file or directory and its content. NoOps if the directory does not exist. Errors are ignored. \"\"\" if path : if os . path . isdir ( path ): shutil . rmtree ( path , ignore_errors = True ) if os . path . isfile ( path ): os . remove ( path ) class _BackendEvalCallback ( core . AgentCallback ): \"\"\"Evaluates an agents current policy and updates its train_context accordingly.\"\"\" def __init__ ( self , train_context : core . TrainContext ): assert train_context , \"train_context not set\" assert train_context . num_episodes_per_eval > 0 , \"num_episodes_per_eval is 0.\" self . _train_contex = train_context def on_play_episode_end ( self , agent_context : core . AgentContext ): pc = agent_context . play tc = self . _train_contex sum_of_r = pc . sum_of_rewards . values () tc . eval_rewards [ tc . episodes_done_in_training ] = ( min ( sum_of_r ), sum ( sum_of_r ) / len ( sum_of_r ), max ( sum_of_r )) steps = [ len ( episode_rewards ) for episode_rewards in pc . rewards . values ()] tc . eval_steps [ tc . episodes_done_in_training ] = ( min ( steps ), sum ( steps ) / len ( steps ), max ( steps )) class _BackendAgent ( ABC ): \"\"\"Base class for all backend agent implementations. Implements the train loop and calls the Callbacks. \"\"\" def __init__ ( self , model_config : core . ModelConfig , backend_name : str , tf_eager_execution : bool ): \"\"\" Args: model_config: defines the model and environment to be used backend_name: id of the backend to which this agent belongs to. \"\"\" global _tf_eager_execution_active assert model_config is not None , \"model_config not set.\" assert backend_name if _tf_eager_execution_active is None : _tf_eager_execution_active = tf_eager_execution assert _tf_eager_execution_active == tf_eager_execution , \\ \"Due to an incompatibility between tensorforce and tfagents their agents can not be instantiated in the\" + \\ \"same python runtime instance (conflicting excpectations on tensorflows eager execution mode).\" self . _backend_name : str = backend_name self . model_config = model_config self . _agent_context : core . AgentContext = core . AgentContext ( self . model_config ) self . _agent_context . gym . _totals = monitor . _register_gym_monitor ( self . model_config . original_env_name ) self . model_config . gym_env_name = self . _agent_context . gym . _totals . gym_env_name self . _preprocess_callbacks : List [ core . _PreProcessCallback ] = [ plot . _PreProcess ()] self . _callbacks : List [ core . AgentCallback ] = [] self . _postprocess_callbacks : List [ core . _PostProcessCallback ] = [ plot . _PostProcess ()] self . _train_total_episodes_on_iteration_begin : int = 0 def _set_seed ( self ): \"\"\" sets the random seeds for all dependent packages \"\"\" if not self . model_config . seed is None : seed = self . model_config . seed self . log_api ( f 'tf.random.set_seed' , f '(seed= {seed} )' ) tensorflow . random . set_seed ( seed = seed ) self . log_api ( f 'numpy.random.seed' , f '( {seed} )' ) numpy . random . seed ( seed ) self . log_api ( f 'random.seed' , f '( {seed} )' ) random . seed ( seed ) return def _eval_current_policy ( self ): \"\"\"Evaluates the current policy using play and updates the train_context If num_episodes_per_eval or num_iterations_per_eval is 0 no evaluation is performed. \"\"\" tc = self . _agent_context . train assert tc , \"train_context not set\" if tc . num_episodes_per_eval and tc . num_iterations_between_eval : callbacks = [ _BackendEvalCallback ( self . _agent_context . train )] + self . _callbacks self . play ( play_context = core . PlayContext ( self . _agent_context . train ), callbacks = callbacks ) def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ): \"\"\"Logs a call to api_target with additional log_msg.\"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg ) def log ( self , log_msg : str ): \"\"\"Logs msg.\"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg ) def _on_gym_init_begin ( self ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Hint: the total instances count is not incremented yet.\"\"\" self . _agent_context . gym . _monitor_env = None for c in self . _callbacks : c . on_gym_init_begin ( self . _agent_context ) self . _agent_context . gym . _monitor_env = None def _on_gym_init_end ( self , env : monitor . _MonitorEnv ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Hint: o the total instances count is incremented by now o the new env (and its action space) is seeded with the api_context's seed \"\"\" self . _agent_context . gym . _monitor_env = env if self . _agent_context . model . seed is not None : env = self . _agent_context . gym . gym_env seed = self . _agent_context . model . seed env . seed ( seed ) for c in self . _callbacks : c . on_gym_init_end ( self . _agent_context ) self . _agent_context . gym . _monitor_env = None def _on_gym_reset_begin ( self , env : monitor . _MonitorEnv , ** kwargs ): \"\"\"called when the monitored environment begins a reset. Hint: the total reset count is not incremented yet.\"\"\" self . _agent_context . gym . _monitor_env = env for c in self . _callbacks : c . on_gym_reset_begin ( self . _agent_context , ** kwargs ) self . _agent_context . gym . _monitor_env = None def _on_gym_reset_end ( self , env : monitor . _MonitorEnv , reset_result : Tuple , ** kwargs ): \"\"\"called when the monitored environment completed a reset. Hint: the total episode count is incremented by now (if a step was performed before the last reset).\"\"\" self . _agent_context . gym . _monitor_env = env for c in self . _callbacks : c . on_gym_reset_end ( self . _agent_context , reset_result , ** kwargs ) self . _agent_context . gym . _monitor_env = None def _on_gym_step_begin ( self , env : monitor . _MonitorEnv , action ): \"\"\"called when the monitored environment begins a step. Hint: o sets env.max_steps_per_episode if we are in train / play. Thus the episode is ended by the MonitorEnv if the step limit is exceeded \"\"\" ac = self . _agent_context ac . gym . _monitor_env = env env . max_steps_per_episode = None if ac . is_play or ac . is_eval : env . max_steps_per_episode = ac . play . max_steps_per_episode self . _on_play_step_begin ( action ) if ac . is_train : env . max_steps_per_episode = ac . train . max_steps_per_episode self . _on_train_step_begin ( action ) for c in self . _callbacks : c . on_gym_step_begin ( self . _agent_context , action ) self . _agent_context . gym . _monitor_env = None def _on_gym_step_end ( self , env : monitor . _MonitorEnv , action , step_result : Tuple ): \"\"\"called when the monitored environment completed a step. Args: env: the gym_env the last step was done on step_result: the result (state, reward, done, info) of the last step call \"\"\" ac = self . _agent_context ac . gym . _monitor_env = env if ac . is_play or ac . is_eval : self . _on_play_step_end ( action , step_result ) if ac . is_train : self . _on_train_step_end ( action , step_result ) for c in self . _callbacks : c . on_gym_step_end ( self . _agent_context , action , step_result ) self . _agent_context . gym . _monitor_env = None env . max_steps_per_episode = None def _on_play_begin ( self ): \"\"\"Must NOT be called by play_implementation\"\"\" for c in self . _callbacks : c . on_play_begin ( self . _agent_context ) def _on_play_end ( self ): \"\"\"Must NOT be called by play_implementation\"\"\" for c in self . _callbacks : c . on_play_end ( self . _agent_context ) self . _agent_context . play . gym_env = None def on_play_episode_begin ( self , env : gym . core . Env ): \"\"\"Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. \"\"\" assert env , \"env not set.\" assert isinstance ( env , gym . core . Env ), \"env not an an instance of gym.Env.\" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context ) def on_play_episode_end ( self ): \"\"\"Must be called by play_implementation at the end of an episode\"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context ) def _on_play_step_begin ( self , action ): \"\"\"Called before each call to gym.step on the current play env (agent_context.play.gym_env) Args: action: the action to be passed to the upcoming gym_env.step call \"\"\" for c in self . _callbacks : c . on_play_step_begin ( self . _agent_context , action ) def _on_play_step_end ( self , action , step_result : Tuple ): \"\"\"Called after each call to gym.step on the current play env (agent_context.play.gym_env) Args: step_result: the result (state, reward, done, info) of the last step call \"\"\" ( state , reward , done , info ) = step_result pc = self . _agent_context . play pc . steps_done_in_episode += 1 pc . steps_done += 1 pc . actions [ pc . episodes_done + 1 ] . append ( action ) pc . rewards [ pc . episodes_done + 1 ] . append ( reward ) pc . sum_of_rewards [ pc . episodes_done + 1 ] += reward for c in self . _callbacks : c . on_play_step_end ( self . _agent_context , action , step_result ) def _on_train_begin ( self ): \"\"\"Must NOT be called by train_implementation\"\"\" for c in self . _callbacks : c . on_train_begin ( self . _agent_context ) def _on_train_end ( self ): \"\"\"Must NOT be called by train_implementation\"\"\" tc = self . _agent_context . train if tc . episodes_done_in_training not in tc . eval_rewards : self . _eval_current_policy () for c in self . _callbacks : c . on_train_end ( self . _agent_context ) def on_train_iteration_begin ( self ): \"\"\"Must be called by train_implementation at the begining of a new iteration\"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context ) def on_train_iteration_end ( self , loss : float , ** kwargs ): \"\"\"Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc . episodes_done_in_training ] = loss # set traincontext dict from kwargs: for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ): prop_instance [ tc . episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ): self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context ) def _on_train_step_begin ( self , action ): \"\"\"Called before each call to gym.step on the current train env (agent_context.train.gym_env) Args: action: the action to be passed to the upcoming gym_env.step call \"\"\" pass def load ( self , directory : str , callbacks : List [ core . AgentCallback ]): \"\"\"Loads a previously trained and saved actor policy from directory. The loaded policy may afterwards be used by calling play(). Args: directory: the directory containing the trained policy callbacks: list of callbacks called during the load. \"\"\" assert callbacks is not None , \"callbacks not set\" assert directory assert os . path . isdir ( directory ) self . _callbacks = callbacks self . load_implementation ( directory ) self . _agent_context . _is_policy_trained = True self . _callbacks = None @abstractmethod def load_implementation ( self , directory : str ): \"\"\"Loads a previously trained and saved actor policy from directory. The loaded policy may afterwards be used by calling play(). Args: directory: the directory containing the trained policy. \"\"\" # noinspection PyUnusedLocal def _on_train_step_end ( self , action : object , step_result : Tuple ): \"\"\"Called after each call to gym.step on the current train env (agent_context.train.gym_env) Args: step_result: the result (state, reward, done, info) of the last step call \"\"\" tc = self . _agent_context . train tc . steps_done_in_iteration += 1 tc . steps_done_in_training += 1 def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None @abstractmethod def play_implementation ( self , play_context : core . PlayContext ): \"\"\"Agent specific implementation of playing a single episode with the current policy. For implementation details see BackendBaseAgent. \"\"\" def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f ' {self._backend_name} ' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . _agent_context . _is_policy_trained = True self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None @abstractmethod def train_implementation ( self , train_context : core . TrainContext ): \"\"\"Agent specific implementation of the train loop. For implementational details see BackendBaseAgent. \"\"\" def save ( self , directory : str , callbacks : List [ core . AgentCallback ]): \"\"\"Saves the currently trained actor policy in directory. Only the actor policy is guaranteed to be saved. Thus after a call to load resuming training is not supported. Args: directory: the directory to save the policy weights to. the directory must exist. callbacks: list of callbacks called during policy load. \"\"\" assert callbacks is not None , \"callbacks not set\" assert directory assert os . path . isdir ( directory ) assert self . _agent_context . _is_policy_trained , \"No trained policy available.\" self . _callbacks = callbacks self . save_implementation ( directory ) self . _callbacks = None @abstractmethod def save_implementation ( self , directory : str ): \"\"\"Agent speecific implementation of saving the weights for the actor policy. Save must only guarantee to persist the weights of the actor policy. The implementation may write multiple files with fixed filenames. Args: directory: the (existing) directory to save the policy weights to. \"\"\" class BackendAgent ( _BackendAgent , metaclass = ABCMeta ): \"\"\"Base class for all BackendAgent implementation. Explicitely exhibits all methods that should be overriden by an implementing agent. \"\"\" @abstractmethod def load_implementation ( self , directory : str ): \"\"\"Loads a previously trained and saved actor policy from directory. The loaded policy may afterwards be used by calling play(). Args: directory: the directory containing the trained policy. \"\"\" @abstractmethod def play_implementation ( self , play_context : core . PlayContext ): \"\"\"Agent specific implementation of playing a number of episodes with the current policy. The implementation should have the form: while True: on_play_episode_begin(env) state = env.reset() while True: action = _trained_policy.action(state) (state, reward, done, info) = env.step(action) if done: break on_play_episode_end() if play_context.play_done: break Args: play_context: play configuration to be used \"\"\" @abstractmethod def save_implementation ( self , directory : str ): \"\"\"Agent speecific implementation of saving the weights for the actor policy. Save must only guarantee to persist the weights of the actor policy. The implementation may write multiple files with fixed filenames. Args: directory: the directory to save the policy weights to. \"\"\" @abstractmethod def train_implementation ( self , train_context : core . TrainContext ): \"\"\"Agent specific implementation of the train loop. The implementation should have the form: while True: on_iteration_begin for e in num_episodes_per_iterations play episode and record steps (while steps_in_episode < max_steps_per_episode and) train policy for num_epochs_per_iteration epochs on_iteration_end( loss ) if training_done break Args: train_context: context configuring the train loop Hints: o the subclasses training loss is passed through to BackendAgent by on_iteration_end. Thus the subclass must not add the experienced loss to the TrainContext. \"\"\" class BackendAgentFactory ( ABC ): \"\"\"Backend agent factory defining the currently available agents (algorithms). \"\"\" backend_name : str = 'abstract_BackendAgentFactory' def create_agent ( self , easyagent_type : Type , model_config : core . ModelConfig ) \\ -> Optional [ _BackendAgent ]: \"\"\"Creates a backend agent instance implementing the algorithm given by agent_type. Args: easyagent_type: the EasyAgent derived type for which an implementing backend instance will be created model_config: the model_config passed to the constructor of the backend instance. Returns: instance of the agent or None if not implemented by this backend. \"\"\" result : Optional [ _BackendAgent ] = None algorithms = self . get_algorithms () if easyagent_type in algorithms : result = algorithms [ easyagent_type ]( model_config = model_config ) return result def get_algorithms ( self ) -> Dict [ Type , Type [ _BackendAgent ]]: \"\"\"Yields a mapping of EasyAgent types to the implementations provided by this backend.\"\"\" return {}","title":"Module easyagents.backends.core"},{"location":"reference/easyagents/backends/core/#classes","text":"","title":"Classes"},{"location":"reference/easyagents/backends/core/#backendagent","text":"class BackendAgent ( model_config : easyagents . core . ModelConfig , backend_name : str , tf_eager_execution : bool ) Base class for all BackendAgent implementation. Explicitely exhibits all methods that should be overriden by an implementing agent. View Source class BackendAgent ( _BackendAgent , metaclass = ABCMeta ) : \"\"\"Base class for all BackendAgent implementation. Explicitely exhibits all methods that should be overriden by an implementing agent. \"\"\" @abstractmethod def load_implementation ( self , directory : str ) : \"\"\"Loads a previously trained and saved actor policy from directory. The loaded policy may afterwards be used by calling play(). Args: directory: the directory containing the trained policy. \"\"\" @abstractmethod def play_implementation ( self , play_context : core . PlayContext ) : \"\"\"Agent specific implementation of playing a number of episodes with the current policy. The implementation should have the form: while True: on_play_episode_begin(env) state = env.reset() while True: action = _trained_policy.action(state) (state, reward, done, info) = env.step(action) if done: break on_play_episode_end() if play_context.play_done: break Args: play_context: play configuration to be used \"\"\" @abstractmethod def save_implementation ( self , directory : str ) : \"\"\"Agent speecific implementation of saving the weights for the actor policy. Save must only guarantee to persist the weights of the actor policy. The implementation may write multiple files with fixed filenames. Args: directory: the directory to save the policy weights to. \"\"\" @abstractmethod def train_implementation ( self , train_context : core . TrainContext ) : \"\"\"Agent specific implementation of the train loop. The implementation should have the form: while True: on_iteration_begin for e in num_episodes_per_iterations play episode and record steps (while steps_in_episode < max_steps_per_episode and) train policy for num_epochs_per_iteration epochs on_iteration_end( loss ) if training_done break Args: train_context: context configuring the train loop Hints: o the subclasses training loss is passed through to BackendAgent by on_iteration_end. Thus the subclass must not add the experienced loss to the TrainContext. \"\"\"","title":"BackendAgent"},{"location":"reference/easyagents/backends/core/#ancestors-in-mro","text":"easyagents.backends.core._BackendAgent abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/backends/core/#descendants","text":"easyagents.backends.default.TensorforceNotActiveAgent easyagents.backends.default.TfAgentsNotActiveAgent easyagents.backends.default.SetTensorforceBackendAgent easyagents.backends.default.NotImplementedYetAgent easyagents.backends.tfagents.TfAgent","title":"Descendants"},{"location":"reference/easyagents/backends/core/#methods","text":"","title":"Methods"},{"location":"reference/easyagents/backends/core/#load","text":"def load ( self , directory : str , callbacks : List [ easyagents . core . AgentCallback ] ) Loads a previously trained and saved actor policy from directory. The loaded policy may afterwards be used by calling play(). Args: directory: the directory containing the trained policy callbacks: list of callbacks called during the load. View Source def load ( self , directory : str , callbacks : List [ core . AgentCallback ]): \"\"\"Loads a previously trained and saved actor policy from directory. The loaded policy may afterwards be used by calling play(). Args: directory: the directory containing the trained policy callbacks: list of callbacks called during the load. \"\"\" assert callbacks is not None , \"callbacks not set\" assert directory assert os . path . isdir ( directory ) self . _callbacks = callbacks self . load_implementation ( directory ) self . _agent_context . _is_policy_trained = True self . _callbacks = None","title":"load"},{"location":"reference/easyagents/backends/core/#load_implementation","text":"def load_implementation ( self , directory : str ) Loads a previously trained and saved actor policy from directory. The loaded policy may afterwards be used by calling play(). Args: directory: the directory containing the trained policy. View Source @abstractmethod def load_implementation ( self , directory : str ) : \"\"\"Loads a previously trained and saved actor policy from directory. The loaded policy may afterwards be used by calling play(). Args: directory: the directory containing the trained policy. \"\"\"","title":"load_implementation"},{"location":"reference/easyagents/backends/core/#log","text":"def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ): \"\"\"Logs msg.\"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg )","title":"log"},{"location":"reference/easyagents/backends/core/#log_api","text":"def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\"Logs a call to api_target with additional log_msg.\"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg )","title":"log_api"},{"location":"reference/easyagents/backends/core/#on_play_episode_begin","text":"def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ): \"\"\"Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. \"\"\" assert env , \"env not set.\" assert isinstance ( env , gym . core . Env ), \"env not an an instance of gym.Env.\" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context )","title":"on_play_episode_begin"},{"location":"reference/easyagents/backends/core/#on_play_episode_end","text":"def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ): \"\"\"Must be called by play_implementation at the end of an episode\"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context )","title":"on_play_episode_end"},{"location":"reference/easyagents/backends/core/#on_train_iteration_begin","text":"def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ): \"\"\"Must be called by train_implementation at the begining of a new iteration\"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context )","title":"on_train_iteration_begin"},{"location":"reference/easyagents/backends/core/#on_train_iteration_end","text":"def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\"Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc.episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc.episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context )","title":"on_train_iteration_end"},{"location":"reference/easyagents/backends/core/#play","text":"def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None","title":"play"},{"location":"reference/easyagents/backends/core/#play_implementation","text":"def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a number of episodes with the current policy. The implementation should have the form: while True: on_play_episode_begin(env) state = env.reset() while True: action = _trained_policy.action(state) (state, reward, done, info) = env.step(action) if done: break on_play_episode_end() if play_context.play_done: break Args: play_context: play configuration to be used View Source @abstractmethod def play_implementation ( self , play_context : core . PlayContext ) : \"\"\"Agent specific implementation of playing a number of episodes with the current policy. The implementation should have the form: while True: on_play_episode_begin(env) state = env.reset() while True: action = _trained_policy.action(state) (state, reward, done, info) = env.step(action) if done: break on_play_episode_end() if play_context.play_done: break Args: play_context: play configuration to be used \"\"\"","title":"play_implementation"},{"location":"reference/easyagents/backends/core/#save","text":"def save ( self , directory : str , callbacks : List [ easyagents . core . AgentCallback ] ) Saves the currently trained actor policy in directory. Only the actor policy is guaranteed to be saved. Thus after a call to load resuming training is not supported. Args: directory: the directory to save the policy weights to. the directory must exist. callbacks: list of callbacks called during policy load. View Source def save ( self , directory : str , callbacks : List [ core . AgentCallback ]): \"\"\"Saves the currently trained actor policy in directory. Only the actor policy is guaranteed to be saved. Thus after a call to load resuming training is not supported. Args: directory: the directory to save the policy weights to. the directory must exist. callbacks: list of callbacks called during policy load. \"\"\" assert callbacks is not None , \"callbacks not set\" assert directory assert os . path . isdir ( directory ) assert self . _agent_context . _is_policy_trained , \"No trained policy available.\" self . _callbacks = callbacks self . save_implementation ( directory ) self . _callbacks = None","title":"save"},{"location":"reference/easyagents/backends/core/#save_implementation","text":"def save_implementation ( self , directory : str ) Agent speecific implementation of saving the weights for the actor policy. Save must only guarantee to persist the weights of the actor policy. The implementation may write multiple files with fixed filenames. Args: directory: the directory to save the policy weights to. View Source @abstractmethod def save_implementation ( self , directory : str ) : \"\"\"Agent speecific implementation of saving the weights for the actor policy. Save must only guarantee to persist the weights of the actor policy. The implementation may write multiple files with fixed filenames. Args: directory: the directory to save the policy weights to. \"\"\"","title":"save_implementation"},{"location":"reference/easyagents/backends/core/#train","text":"def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . _agent_context . _is_policy_trained = True self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None","title":"train"},{"location":"reference/easyagents/backends/core/#train_implementation","text":"def train_implementation ( self , train_context : easyagents . core . TrainContext ) Agent specific implementation of the train loop. The implementation should have the form: while True: on_iteration_begin for e in num_episodes_per_iterations play episode and record steps (while steps_in_episode < max_steps_per_episode and) train policy for num_epochs_per_iteration epochs on_iteration_end( loss ) if training_done break Args: train_context: context configuring the train loop Hints: o the subclasses training loss is passed through to BackendAgent by on_iteration_end. Thus the subclass must not add the experienced loss to the TrainContext. View Source @abstractmethod def train_implementation ( self , train_context : core . TrainContext ) : \"\"\"Agent specific implementation of the train loop. The implementation should have the form: while True: on_iteration_begin for e in num_episodes_per_iterations play episode and record steps (while steps_in_episode < max_steps_per_episode and) train policy for num_epochs_per_iteration epochs on_iteration_end( loss ) if training_done break Args: train_context: context configuring the train loop Hints: o the subclasses training loss is passed through to BackendAgent by on_iteration_end. Thus the subclass must not add the experienced loss to the TrainContext. \"\"\"","title":"train_implementation"},{"location":"reference/easyagents/backends/core/#backendagentfactory","text":"class BackendAgentFactory ( / , * args , ** kwargs ) Backend agent factory defining the currently available agents (algorithms). View Source class BackendAgentFactory ( ABC ) : \"\"\"Backend agent factory defining the currently available agents (algorithms). \"\"\" backend_name : str = 'abstract_BackendAgentFactory' def create_agent ( self , easyagent_type : Type , model_config : core . ModelConfig ) \\ -> Optional [ _BackendAgent ] : \"\"\"Creates a backend agent instance implementing the algorithm given by agent_type. Args: easyagent_type: the EasyAgent derived type for which an implementing backend instance will be created model_config: the model_config passed to the constructor of the backend instance. Returns: instance of the agent or None if not implemented by this backend. \"\"\" result : Optional [ _BackendAgent ] = None algorithms = self . get_algorithms () if easyagent_type in algorithms : result = algorithms [ easyagent_type ] ( model_config = model_config ) return result def get_algorithms ( self ) -> Dict [ Type, Type[_BackendAgent ] ]: \"\"\"Yields a mapping of EasyAgent types to the implementations provided by this backend.\"\"\" return {}","title":"BackendAgentFactory"},{"location":"reference/easyagents/backends/core/#ancestors-in-mro_1","text":"abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/backends/core/#descendants_1","text":"easyagents.backends.default.DefaultAgentFactory easyagents.backends.tfagents.TfAgentAgentFactory","title":"Descendants"},{"location":"reference/easyagents/backends/core/#class-variables","text":"backend_name","title":"Class variables"},{"location":"reference/easyagents/backends/core/#methods_1","text":"","title":"Methods"},{"location":"reference/easyagents/backends/core/#create_agent","text":"def create_agent ( self , easyagent_type : Type , model_config : easyagents . core . ModelConfig ) -> Union [ easyagents . backends . core . _BackendAgent , NoneType ] Creates a backend agent instance implementing the algorithm given by agent_type. Args: easyagent_type: the EasyAgent derived type for which an implementing backend instance will be created model_config: the model_config passed to the constructor of the backend instance. Returns: instance of the agent or None if not implemented by this backend. View Source def create_agent ( self , easyagent_type : Type , model_config : core . ModelConfig ) \\ -> Optional [ _BackendAgent ] : \"\"\"Creates a backend agent instance implementing the algorithm given by agent_type. Args: easyagent_type: the EasyAgent derived type for which an implementing backend instance will be created model_config: the model_config passed to the constructor of the backend instance. Returns: instance of the agent or None if not implemented by this backend. \"\"\" result : Optional [ _BackendAgent ] = None algorithms = self . get_algorithms () if easyagent_type in algorithms : result = algorithms [ easyagent_type ] ( model_config = model_config ) return result","title":"create_agent"},{"location":"reference/easyagents/backends/core/#get_algorithms","text":"def get_algorithms ( self ) -> Dict [ Type , Type [ easyagents . backends . core . _BackendAgent ]] Yields a mapping of EasyAgent types to the implementations provided by this backend. View Source def get_algorithms ( self ) -> Dict [ Type, Type[_BackendAgent ] ]: \"\"\"Yields a mapping of EasyAgent types to the implementations provided by this backend.\"\"\" return {}","title":"get_algorithms"},{"location":"reference/easyagents/backends/tfagents/","text":"Module easyagents.backends.tfagents This module contains the backend implementation for tf Agents (see https://github.com/tensorflow/agents) View Source \"\"\"This module contains the backend implementation for tf Agents (see https://github.com/tensorflow/agents)\"\"\" from abc import ABCMeta from typing import Dict , Type import math import os # noinspection PyUnresolvedReferences import easyagents.agents from easyagents import core from easyagents.backends import core as bcore from easyagents.backends import monitor # noinspection PyPackageRequirements import tensorflow as tf from tf_agents.agents.ddpg import critic_network from tf_agents.agents.dqn import dqn_agent from tf_agents.agents.ppo import ppo_agent from tf_agents.agents.reinforce import reinforce_agent from tf_agents.agents.sac import sac_agent from tf_agents.drivers import dynamic_step_driver from tf_agents.drivers.dynamic_episode_driver import DynamicEpisodeDriver from tf_agents.environments import gym_wrapper , py_environment , tf_py_environment from tf_agents.networks import actor_distribution_network , normal_projection_network , q_network , value_network from tf_agents.policies import greedy_policy , tf_policy , random_tf_policy , policy_saver from tf_agents.replay_buffers import tf_uniform_replay_buffer from tf_agents.replay_buffers.tf_uniform_replay_buffer import TFUniformReplayBuffer from tf_agents.trajectories import trajectory from tf_agents.utils import common import gym # noinspection PyUnresolvedReferences,PyAbstractClass class TfAgent ( bcore . BackendAgent , metaclass = ABCMeta ): \"\"\"Reinforcement learning agents based on googles tf_agent implementations https://github.com/tensorflow/agents \"\"\" def __init__ ( self , model_config : core . ModelConfig ): super () . __init__ ( model_config = model_config , backend_name = TfAgentAgentFactory . backend_name , tf_eager_execution = True ) self . _trained_policy = None self . _play_env : Optional [ gym . Env ] = None def _create_gym_with_wrapper ( self , discount ): gym_spec = gym . spec ( self . model_config . gym_env_name ) gym_env = gym_spec . make () # simplify_box_bounds: Whether to replace bounds of Box space that are arrays # with identical values with one number and rely on broadcasting. # important, simplify_box_bounds True crashes environments with boundaries with identical values env = gym_wrapper . GymWrapper ( gym_env , discount = discount , simplify_box_bounds = False ) return env def _create_env ( self , discount : float = 1 ) -> tf_py_environment . TFPyEnvironment : \"\"\" creates a new instance of the gym environment and wraps it in a tfagent TFPyEnvironment Args: discount: the reward discount factor \"\"\" assert 0 < discount <= 1 , \"discount not admissible\" self . log_api ( f 'TFPyEnvironment' , f '( suite_gym.load( \"{self.model_config.original_env_name}\", discount={discount}) )' ) # suit_gym.load crashes our environment # py_env = suite_gym.load(self.model_config.gym_env_name, discount=discount) py_env = self . _create_gym_with_wrapper ( discount ) result = tf_py_environment . TFPyEnvironment ( py_env ) return result def _get_gym_env ( self , tf_py_env : tf_py_environment . TFPyEnvironment ) -> monitor . _MonitorEnv : \"\"\" extracts the underlying _MonitorEnv from tf_py_env created by _create_tfagent_env\"\"\" assert isinstance ( tf_py_env , tf_py_environment . TFPyEnvironment ), \\ \"passed tf_py_env is not an instance of TFPyEnvironment\" assert isinstance ( tf_py_env . pyenv , py_environment . PyEnvironment ), \\ \"passed TFPyEnvironment.pyenv does not contain a PyEnvironment\" assert len ( tf_py_env . pyenv . envs ) == 1 , \"passed TFPyEnvironment.pyenv does not contain a unique environment\" result = tf_py_env . pyenv . envs [ 0 ] . gym assert isinstance ( result , monitor . _MonitorEnv ), \"passed TFPyEnvironment does not contain a _MonitorEnv\" return result def play_implementation ( self , play_context : core . PlayContext ): \"\"\"Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used \"\"\" assert play_context , \"play_context not set.\" assert self . _trained_policy , \"trained_policy not set. call train() first.\" if self . _play_env is None : self . _play_env = self . _create_env () gym_env = self . _get_gym_env ( self . _play_env ) while True : self . on_play_episode_begin ( env = gym_env ) time_step = self . _play_env . reset () while not time_step . is_last (): action_step = self . _trained_policy . action ( time_step ) time_step = self . _play_env . step ( action_step . action ) self . on_play_episode_end () if play_context . play_done : break def load_implementation ( self , directory : str ): \"\"\"Loads a previously saved actor policy from the directory Args: directory: the directory to load the policy from. \"\"\" assert directory self . log_api ( 'saved_model.load' , f '({directory})' ) self . _trained_policy = tf . compat . v2 . saved_model . load ( directory ) def save_implementation ( self , directory : str ): \"\"\"Saves the trained actor policy in directory. If no policy was trained yet, no file is written . Args: directory: the directory to save the policy weights to. \"\"\" assert self . _trained_policy , \"no policy trained yet.\" self . log_api ( 'PolicySaver' , f '(trained_policy,seed={self.model_config.seed})' ) saver = policy_saver . PolicySaver ( self . _trained_policy , seed = self . model_config . seed ) self . log_api ( 'policy_saver.save' , f '({directory})' ) saver . save ( directory ) # noinspection PyUnresolvedReferences class TfDqnAgent ( TfAgent ): \"\"\" creates a new agent based on the DQN algorithm using the tfagents implementation. Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. \"\"\" def __init__ ( self , model_config : core . ModelConfig ): super () . __init__ ( model_config = model_config ) def collect_step ( self , env : tf_py_environment . TFPyEnvironment , policy : tf_policy . Base , replay_buffer : TFUniformReplayBuffer ): time_step = env . current_time_step () action_step = policy . action ( time_step ) next_time_step = env . step ( action_step . action ) traj = trajectory . from_transition ( time_step , action_step , next_time_step ) replay_buffer . add_batch ( traj ) # noinspection DuplicatedCode def train_implementation ( self , train_context : core . TrainContext ): \"\"\"Tf-Agents Ppo Implementation of the train loop. The implementation follows https://colab.research.google.com/github/tensorflow/agents/blob/master/tf_agents/colabs/1_dqn_tutorial.ipynb \"\"\" assert isinstance ( train_context , core . StepsTrainContext ) dc : core . StepsTrainContext = train_context train_env = self . _create_env ( discount = dc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () # SetUp Optimizer, Networks and DqnAgent self . log_api ( 'AdamOptimizer' , f '(learning_rate={dc.learning_rate})' ) optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = dc . learning_rate ) self . log_api ( 'QNetwork' , f '(observation_spec, action_spec, fc_layer_params={self.model_config.fc_layers})' ) q_net = q_network . QNetwork ( observation_spec , action_spec , fc_layer_params = self . model_config . fc_layers ) self . log_api ( 'DqnAgent' , '(timestep_spec,action_spec,q_network=..., optimizer=...,td_errors_loss_fn=common.element_wise_squared_loss)' ) tf_agent = dqn_agent . DqnAgent ( timestep_spec , action_spec , q_network = q_net , optimizer = optimizer , td_errors_loss_fn = common . element_wise_squared_loss ) self . log_api ( 'tf_agent.initialize' , f '()' ) tf_agent . initialize () self . _trained_policy = tf_agent . policy # SetUp Data collection & Buffering self . log_api ( 'TFUniformReplayBuffer' , f '(data_spec=..., batch_size={train_env.batch_size}, max_length={dc.max_steps_in_buffer})' ) replay_buffer = TFUniformReplayBuffer ( data_spec = tf_agent . collect_data_spec , batch_size = train_env . batch_size , max_length = dc . max_steps_in_buffer ) self . log_api ( 'RandomTFPolicy' , '()' ) random_policy = random_tf_policy . RandomTFPolicy ( timestep_spec , action_spec ) self . log_api ( 'replay_buffer.add_batch' , '(trajectory)' ) for _ in range ( dc . num_steps_buffer_preload ): self . collect_step ( env = train_env , policy = random_policy , replay_buffer = replay_buffer ) # Train self . log_api ( 'tf_agent.train' , '= common.function(tf_agent.train)' ) tf_agent . train = common . function ( tf_agent . train ) self . log_api ( 'replay_buffer.as_dataset' , f '(num_parallel_calls=3, ' + f 'sample_batch_size={dc.num_steps_sampled_from_buffer}, num_steps=2).prefetch(3)' ) dataset = replay_buffer . as_dataset ( num_parallel_calls = 3 , sample_batch_size = dc . num_steps_sampled_from_buffer , num_steps = 2 ) . prefetch ( 3 ) self . log_api ( 'iter(dataset' , f '{iter(dataset)}' ) iter_dataset = iter ( dataset ) self . log_api ( 'for each iteration' ) self . log_api ( ' replay_buffer.add_batch' , '(trajectory)' ) self . log_api ( ' tf_agent.train' , '(experience=trajectory)' ) while True : self . on_train_iteration_begin () for _ in range ( dc . num_steps_per_iteration ): self . collect_step ( env = train_env , policy = tf_agent . collect_policy , replay_buffer = replay_buffer ) trajectories , _ = next ( iter_dataset ) tf_loss_info = tf_agent . train ( experience = trajectories ) self . on_train_iteration_end ( tf_loss_info . loss ) if train_context . training_done : break return # noinspection PyUnresolvedReferences class TfPpoAgent ( TfAgent ): \"\"\" creates a new agent based on the PPO algorithm using the tfagents implementation. PPO is an actor-critic algorithm using 2 neural networks. The actor network to predict the next action to be taken and the critic network to estimate the value of the game state we are currently in (the expected, discounted sum of future rewards when following the current actor network). Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. \"\"\" def __init__ ( self , model_config : core . ModelConfig ): super () . __init__ ( model_config = model_config ) # noinspection DuplicatedCode def train_implementation ( self , train_context : core . TrainContext ): \"\"\"Tf-Agents Ppo Implementation of the train loop.\"\"\" assert isinstance ( train_context , core . PpoTrainContext ) tc : core . PpoTrainContext = train_context train_env = self . _create_env ( discount = tc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () # SetUp Optimizer, Networks and PpoAgent self . log_api ( 'AdamOptimizer' , '()' ) optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ) self . log_api ( 'ActorDistributionNetwork' , '()' ) actor_net = actor_distribution_network . ActorDistributionNetwork ( observation_spec , action_spec , fc_layer_params = self . model_config . fc_layers ) self . log_api ( 'ValueNetwork' , '()' ) value_net = value_network . ValueNetwork ( observation_spec , fc_layer_params = self . model_config . fc_layers ) self . log_api ( 'PpoAgent' , '()' ) tf_agent = ppo_agent . PPOAgent ( timestep_spec , action_spec , optimizer , actor_net = actor_net , value_net = value_net , num_epochs = tc . num_epochs_per_iteration ) self . log_api ( 'tf_agent.initialize' , '()' ) tf_agent . initialize () self . _trained_policy = tf_agent . policy # SetUp Data collection & Buffering collect_data_spec = tf_agent . collect_data_spec self . log_api ( 'TFUniformReplayBuffer' , '()' ) replay_buffer = TFUniformReplayBuffer ( collect_data_spec , batch_size = 1 , max_length = tc . max_steps_in_buffer ) collect_policy = tf_agent . collect_policy self . log_api ( 'DynamicEpisodeDriver' , '()' ) collect_driver = DynamicEpisodeDriver ( train_env , collect_policy , observers = [ replay_buffer . add_batch ], num_episodes = tc . num_episodes_per_iteration ) # Train collect_driver . run = common . function ( collect_driver . run , autograph = False ) tf_agent . train = common . function ( tf_agent . train , autograph = False ) while True : self . on_train_iteration_begin () self . log_api ( '-----' , f 'iteration {tc.iterations_done_in_training:4} of {tc.num_iterations:<4} -----' ) self . log_api ( 'collect_driver.run' , '()' ) collect_driver . run () self . log_api ( 'replay_buffer.gather_all' , '()' ) trajectories = replay_buffer . gather_all () self . log_api ( 'tf_agent.train' , '(experience=...)' ) loss_info = tf_agent . train ( experience = trajectories ) total_loss = loss_info . loss . numpy () actor_loss = loss_info . extra . policy_gradient_loss . numpy () critic_loss = loss_info . extra . value_estimation_loss . numpy () self . log_api ( '' , f 'loss={total_loss:<7.1f} [actor={actor_loss:<7.1f} critic={critic_loss:<7.1f}]' ) self . log_api ( 'replay_buffer.clear' , '()' ) replay_buffer . clear () self . on_train_iteration_end ( loss = total_loss , actor_loss = actor_loss , critic_loss = critic_loss ) if tc . training_done : break return # noinspection PyUnresolvedReferences class TfRandomAgent ( TfAgent ): \"\"\" creates a new random agent based on uniform random actions. Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. \"\"\" def __init__ ( self , model_config : core . ModelConfig ): super () . __init__ ( model_config = model_config ) self . _set_trained_policy () def _set_trained_policy ( self ): \"\"\"Tf-Agents Random Implementation of the train loop.\"\"\" self . log ( 'Creating environment...' ) train_env = self . _create_env () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () self . log_api ( 'RandomTFPolicy' , 'create' ) self . _trained_policy = random_tf_policy . RandomTFPolicy ( timestep_spec , action_spec ) self . _agent_context . _is_policy_trained = True def load_implementation ( self , directory : str ): \"\"\"NoOps implementation, since we don't save/load random policies.\"\"\" pass def save_implementation ( self , directory : str ): \"\"\"NoOps implementation, since we don't save/load random policies.\"\"\" pass # noinspection DuplicatedCode def train_implementation ( self , train_context : core . TrainContext ): self . log ( \"Training...\" ) train_env = self . _create_env () while True : self . on_train_iteration_begin () # ensure that 1 episode is played during the iteration time_step = train_env . reset () while not time_step . is_last (): action_step = self . _trained_policy . action ( time_step ) time_step = train_env . step ( action_step . action ) self . on_train_iteration_end ( math . nan ) if train_context . training_done : break return # noinspection PyUnresolvedReferences class TfReinforceAgent ( TfAgent ): \"\"\" creates a new agent based on the Reinforce algorithm using the tfagents implementation. Reinforce is a vanilla policy gradient algorithm using a single neural networks to predict the actions. Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. \"\"\" def __init__ ( self , model_config : core . ModelConfig ): super () . __init__ ( model_config = model_config ) # noinspection DuplicatedCode def train_implementation ( self , train_context : core . TrainContext ): \"\"\"Tf-Agents Reinforce Implementation of the train loop.\"\"\" assert isinstance ( train_context , core . EpisodesTrainContext ) tc : core . EpisodesTrainContext = train_context self . log ( 'Creating environment...' ) train_env = self . _create_env ( discount = tc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () # SetUp Optimizer, Networks and PpoAgent self . log_api ( 'AdamOptimizer' , 'create' ) optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ) self . log_api ( 'ActorDistributionNetwork' , 'create' ) actor_net = actor_distribution_network . ActorDistributionNetwork ( observation_spec , action_spec , fc_layer_params = self . model_config . fc_layers ) self . log_api ( 'ReinforceAgent' , 'create' ) tf_agent = reinforce_agent . ReinforceAgent ( timestep_spec , action_spec , actor_network = actor_net , optimizer = optimizer ) self . log_api ( 'tf_agent.initialize()' ) tf_agent . initialize () self . _trained_policy = tf_agent . policy # SetUp Data collection & Buffering collect_data_spec = tf_agent . collect_data_spec self . log_api ( 'TFUniformReplayBuffer' , 'create' ) replay_buffer = TFUniformReplayBuffer ( collect_data_spec , batch_size = 1 , max_length = tc . max_steps_in_buffer ) self . log_api ( 'DynamicEpisodeDriver' , 'create' ) collect_driver = DynamicEpisodeDriver ( train_env , tf_agent . collect_policy , observers = [ replay_buffer . add_batch ], num_episodes = tc . num_episodes_per_iteration ) # Train collect_driver . run = common . function ( collect_driver . run , autograph = False ) tf_agent . train = common . function ( tf_agent . train , autograph = False ) self . log ( 'Starting training...' ) while True : self . on_train_iteration_begin () msg = f 'iteration {tc.iterations_done_in_training:4} of {tc.num_iterations:<4}' self . log_api ( 'collect_driver.run' , msg ) collect_driver . run () self . log_api ( 'replay_buffer.gather_all' , msg ) trajectories = replay_buffer . gather_all () self . log_api ( 'tf_agent.train' , msg ) loss_info = tf_agent . train ( experience = trajectories ) total_loss = loss_info . loss . numpy () self . log_api ( '' , f 'loss={total_loss:<7.1f}' ) self . log_api ( 'replay_buffer.clear' , msg ) replay_buffer . clear () self . on_train_iteration_end ( loss = total_loss ) if tc . training_done : break return # noinspection PyUnresolvedReferences class TfSacAgent ( TfAgent ): \"\"\" creates a new agent based on the SAC algorithm using the tfagents implementation. adapted from https://github.com/tensorflow/agents/blob/master/tf_agents/colabs/7_SAC_minitaur_tutorial.ipynb Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. \"\"\" def __init__ ( self , model_config : core . ModelConfig ): super () . __init__ ( model_config = model_config ) # noinspection DuplicatedCode def train_implementation ( self , train_context : core . TrainContext ): \"\"\"Tf-Agents Ppo Implementation of the train loop.\"\"\" assert isinstance ( train_context , core . StepsTrainContext ) tc : core . StepsTrainContext = train_context train_env = self . _create_env ( discount = tc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () self . log_api ( 'CriticNetwork' , f '(observation_spec, action_spec), observation_fc_layer_params=None, ' + f 'action_fc_layer_params=None, joint_fc_layer_params={self.model_config.fc_layers})' ) critic_net = critic_network . CriticNetwork (( observation_spec , action_spec ), observation_fc_layer_params = None , action_fc_layer_params = None , joint_fc_layer_params = self . model_config . fc_layers ) def normal_projection_net ( action_spec_arg , init_means_output_factor = 0.1 ): return normal_projection_network . NormalProjectionNetwork ( action_spec_arg , mean_transform = None , state_dependent_std = True , init_means_output_factor = init_means_output_factor , std_transform = sac_agent . std_clip_transform , scale_distribution = True ) self . log_api ( 'ActorDistributionNetwork' , f 'observation_spec, action_spec, fc_layer_params={self.model_config.fc_layers}), ' + f 'continuous_projection_net=...)' ) actor_net = actor_distribution_network . ActorDistributionNetwork ( observation_spec , action_spec , fc_layer_params = self . model_config . fc_layers , continuous_projection_net = normal_projection_net ) # self.log_api('tf.compat.v1.train.get_or_create_global_step','()') # global_step = tf.compat.v1.train.get_or_create_global_step() self . log_api ( 'SacAgent' , f '(timestep_spec, action_spec, actor_network=..., critic_network=..., ' + f 'actor_optimizer=AdamOptimizer(learning_rate={tc.learning_rate}), ' + f 'critic_optimizer=AdamOptimizer(learning_rate={tc.learning_rate}), ' + f 'alpha_optimizer=AdamOptimizer(learning_rate={tc.learning_rate}), ' + f 'gamma={tc.reward_discount_gamma})' ) tf_agent = sac_agent . SacAgent ( timestep_spec , action_spec , actor_network = actor_net , critic_network = critic_net , actor_optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ), critic_optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ), alpha_optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ), # target_update_tau=0.005, # target_update_period=1, # td_errors_loss_fn=tf.compat.v1.losses.mean_squared_error, gamma = tc . reward_discount_gamma ) # reward_scale_factor=1.0, # gradient_clipping=None, # train_step_counter=global_step) self . log_api ( 'tf_agent.initialize' , '()' ) tf_agent . initialize () self . _trained_policy = greedy_policy . GreedyPolicy ( tf_agent . policy ) collect_policy = tf_agent . collect_policy # setup and preload replay buffer self . log_api ( 'TFUniformReplayBuffer' , f '(data_spec=tf_agent.collect_data_spec, ' + f 'batch_size={train_env.batch_size}, max_length={tc.max_steps_in_buffer})' ) replay_buffer = tf_uniform_replay_buffer . TFUniformReplayBuffer ( data_spec = tf_agent . collect_data_spec , batch_size = train_env . batch_size , max_length = tc . max_steps_in_buffer ) self . log_api ( 'DynamicStepDriver' , f '(env, collect_policy, observers=[replay_buffer.add_batch], ' + f 'num_steps={tc.num_steps_buffer_preload})' ) initial_collect_driver = dynamic_step_driver . DynamicStepDriver ( train_env , collect_policy , observers = [ replay_buffer . add_batch ], num_steps = tc . num_steps_buffer_preload ) self . log_api ( 'initial_collect_driver.run()' ) initial_collect_driver . run () # Dataset generates trajectories with shape [Bx2x...] dataset = replay_buffer . as_dataset ( num_parallel_calls = 3 , sample_batch_size = tc . num_steps_sampled_from_buffer , num_steps = 2 ) . prefetch ( 3 ) iterator = iter ( dataset ) self . log_api ( 'DynamicStepDriver' , f '(env, collect_policy, observers=[replay_buffer.add_batch], ' + f 'num_steps={tc.num_steps_per_iteration})' ) collect_driver = dynamic_step_driver . DynamicStepDriver ( train_env , collect_policy , observers = [ replay_buffer . add_batch ], num_steps = tc . num_steps_per_iteration ) # (Optional) Optimize by wrapping some of the code in a graph using TF function. tf_agent . train = common . function ( tf_agent . train ) collect_driver . run = common . function ( collect_driver . run ) self . log_api ( 'for each iteration' ) self . log_api ( ' collect_driver.run' , '()' ) self . log_api ( ' tf_agent.train' , '(experience=...)' ) while True : self . on_train_iteration_begin () # Collect a few steps using collect_policy and save to the replay buffer. for _ in range ( tc . num_steps_per_iteration ): collect_driver . run () # Sample a batch of data from the buffer and update the agent's network. experience , _ = next ( iterator ) loss_info = tf_agent . train ( experience ) total_loss = loss_info . loss . numpy () actor_loss = loss_info . extra . actor_loss alpha_loss = loss_info . extra . alpha_loss critic_loss = loss_info . extra . critic_loss self . on_train_iteration_end ( loss = total_loss , actor_loss = actor_loss , critic_loss = critic_loss , alpha_loss = alpha_loss ) if tc . training_done : break return class TfAgentAgentFactory ( bcore . BackendAgentFactory ): \"\"\"Backend for TfAgents. Serves as a factory to create algorithm specific wrappers for the TfAgents implementations. \"\"\" backend_name : str = 'tfagents' def get_algorithms ( self ) -> Dict [ Type , Type [ easyagents . backends . core . BackendAgent ]]: \"\"\"Yields a mapping of EasyAgent types to the implementations provided by this backend.\"\"\" return { easyagents . agents . DqnAgent : TfDqnAgent , easyagents . agents . PpoAgent : TfPpoAgent , easyagents . agents . RandomAgent : TfRandomAgent , easyagents . agents . ReinforceAgent : TfReinforceAgent , easyagents . agents . SacAgent : TfSacAgent } Classes TfAgent class TfAgent ( model_config : easyagents . core . ModelConfig ) Reinforcement learning agents based on googles tf_agent implementations https://github.com/tensorflow/agents View Source class TfAgent ( bcore . BackendAgent , metaclass = ABCMeta ): \"\"\"Reinforcement learning agents based on googles tf_agent implementations https://github.com/tensorflow/agents \"\"\" def __init__ ( self , model_config: core . ModelConfig ): super (). __init__ ( model_config = model_config , backend_name = TfAgentAgentFactory . backend_name , tf_eager_execution = True ) self . _trained_policy = None self . _play_env: Optional [ gym . Env ] = None def _create_gym_with_wrapper ( self , discount ): gym_spec = gym . spec ( self . model_config . gym_env_name ) gym_env = gym_spec . make () # simplify_box_bounds: Whether to replace bounds of Box space that are arrays # with identical values with one number and rely on broadcasting. # important, simplify_box_bounds True crashes environments with boundaries with identical values env = gym_wrapper . GymWrapper ( gym_env , discount = discount , simplify_box_bounds = False ) return env def _create_env ( self , discount: float = 1 ) -> tf_py_environment . TFPyEnvironment: \"\"\" creates a new instance of the gym environment and wraps it in a tfagent TFPyEnvironment Args: discount: the reward discount factor \"\"\" assert 0 < discount <= 1 , \"discount not admissible\" self . log_api ( f'TFPyEnvironment' , f' ( suite_gym . load ( \"{self.model_config.original_env_name}\" , discount ={ discount }) ) ') # suit_gym.load crashes our environment # py_env = suite_gym.load(self.model_config.gym_env_name, discount=discount) py_env = self._create_gym_with_wrapper(discount) result = tf_py_environment.TFPyEnvironment(py_env) return result def _get_gym_env(self, tf_py_env: tf_py_environment.TFPyEnvironment) -> monitor._MonitorEnv: \"\"\" extracts the underlying _MonitorEnv from tf_py_env created by _create_tfagent_env\"\"\" assert isinstance(tf_py_env, tf_py_environment.TFPyEnvironment), \\ \"passed tf_py_env is not an instance of TFPyEnvironment\" assert isinstance(tf_py_env.pyenv, py_environment.PyEnvironment), \\ \"passed TFPyEnvironment.pyenv does not contain a PyEnvironment\" assert len(tf_py_env.pyenv.envs) == 1, \"passed TFPyEnvironment.pyenv does not contain a unique environment\" result = tf_py_env.pyenv.envs[0].gym assert isinstance(result, monitor._MonitorEnv), \"passed TFPyEnvironment does not contain a _MonitorEnv\" return result def play_implementation(self, play_context: core.PlayContext): \"\"\"Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used \"\"\" assert play_context, \"play_context not set.\" assert self._trained_policy, \"trained_policy not set. call train() first.\" if self._play_env is None: self._play_env = self._create_env() gym_env = self._get_gym_env(self._play_env) while True: self.on_play_episode_begin(env=gym_env) time_step = self._play_env.reset() while not time_step.is_last(): action_step = self._trained_policy.action(time_step) time_step = self._play_env.step(action_step.action) self.on_play_episode_end() if play_context.play_done: break def load_implementation(self, directory: str): \"\"\"Loads a previously saved actor policy from the directory Args: directory: the directory to load the policy from. \"\"\" assert directory self.log_api(' saved_model . load' , f' ({ directory }) ') self._trained_policy = tf.compat.v2.saved_model.load(directory) def save_implementation(self, directory: str): \"\"\"Saves the trained actor policy in directory. If no policy was trained yet, no file is written . Args: directory: the directory to save the policy weights to. \"\"\" assert self._trained_policy, \"no policy trained yet.\" self.log_api(' PolicySaver' , f' ( trained_policy , seed ={ self . model_config . seed }) ') saver = policy_saver.PolicySaver(self._trained_policy, seed=self.model_config.seed) self.log_api(' policy_saver . save' , f' ({ directory })') saver . save ( directory ) Ancestors (in MRO) easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC Descendants easyagents.backends.tfagents.TfDqnAgent easyagents.backends.tfagents.TfPpoAgent easyagents.backends.tfagents.TfRandomAgent easyagents.backends.tfagents.TfReinforceAgent easyagents.backends.tfagents.TfSacAgent Methods load def load ( self , directory : str , callbacks : List [ easyagents . core . AgentCallback ] ) Loads a previously trained and saved actor policy from directory. The loaded policy may afterwards be used by calling play(). Args: directory: the directory containing the trained policy callbacks: list of callbacks called during the load. View Source def load ( self , directory : str , callbacks : List [ core . AgentCallback ]): \"\"\"Loads a previously trained and saved actor policy from directory. The loaded policy may afterwards be used by calling play(). Args: directory: the directory containing the trained policy callbacks: list of callbacks called during the load. \"\"\" assert callbacks is not None , \"callbacks not set\" assert directory assert os . path . isdir ( directory ) self . _callbacks = callbacks self . load_implementation ( directory ) self . _agent_context . _is_policy_trained = True self . _callbacks = None load_implementation def load_implementation ( self , directory : str ) Loads a previously saved actor policy from the directory Args: directory: the directory to load the policy from. View Source def load_implementation ( self , directory : str ): \"\"\"Loads a previously saved actor policy from the directory Args: directory: the directory to load the policy from. \"\"\" assert directory self . log_api ( 'saved_model.load' , f '({directory})' ) self . _trained_policy = tf . compat . v2 . saved_model . load ( directory ) log def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ): \"\"\"Logs msg.\"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg ) log_api def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\"Logs a call to api_target with additional log_msg.\"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg ) on_play_episode_begin def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ): \"\"\"Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. \"\"\" assert env , \"env not set.\" assert isinstance ( env , gym . core . Env ), \"env not an an instance of gym.Env.\" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context ) on_play_episode_end def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ): \"\"\"Must be called by play_implementation at the end of an episode\"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context ) on_train_iteration_begin def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ): \"\"\"Must be called by train_implementation at the begining of a new iteration\"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context ) on_train_iteration_end def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\"Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc.episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc.episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context ) play def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None play_implementation def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : core . PlayContext ): \"\"\"Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used \"\"\" assert play_context , \"play_context not set.\" assert self . _trained_policy , \"trained_policy not set. call train() first.\" if self . _play_env is None : self . _play_env = self . _create_env () gym_env = self . _get_gym_env ( self . _play_env ) while True : self . on_play_episode_begin ( env = gym_env ) time_step = self . _play_env . reset () while not time_step . is_last (): action_step = self . _trained_policy . action ( time_step ) time_step = self . _play_env . step ( action_step . action ) self . on_play_episode_end () if play_context . play_done : break save def save ( self , directory : str , callbacks : List [ easyagents . core . AgentCallback ] ) Saves the currently trained actor policy in directory. Only the actor policy is guaranteed to be saved. Thus after a call to load resuming training is not supported. Args: directory: the directory to save the policy weights to. the directory must exist. callbacks: list of callbacks called during policy load. View Source def save ( self , directory : str , callbacks : List [ core . AgentCallback ]): \"\"\"Saves the currently trained actor policy in directory. Only the actor policy is guaranteed to be saved. Thus after a call to load resuming training is not supported. Args: directory: the directory to save the policy weights to. the directory must exist. callbacks: list of callbacks called during policy load. \"\"\" assert callbacks is not None , \"callbacks not set\" assert directory assert os . path . isdir ( directory ) assert self . _agent_context . _is_policy_trained , \"No trained policy available.\" self . _callbacks = callbacks self . save_implementation ( directory ) self . _callbacks = None save_implementation def save_implementation ( self , directory : str ) Saves the trained actor policy in directory. If no policy was trained yet, no file is written . Args: directory: the directory to save the policy weights to. View Source def save_implementation ( self , directory : str ): \"\"\"Saves the trained actor policy in directory. If no policy was trained yet, no file is written . Args: directory: the directory to save the policy weights to. \"\"\" assert self . _trained_policy , \"no policy trained yet.\" self . log_api ( 'PolicySaver' , f '(trained_policy,seed={self.model_config.seed})' ) saver = policy_saver . PolicySaver ( self . _trained_policy , seed = self . model_config . seed ) self . log_api ( 'policy_saver.save' , f '({directory})' ) saver . save ( directory ) train def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . _agent_context . _is_policy_trained = True self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None train_implementation def train_implementation ( self , train_context : easyagents . core . TrainContext ) Agent specific implementation of the train loop. The implementation should have the form: while True: on_iteration_begin for e in num_episodes_per_iterations play episode and record steps (while steps_in_episode < max_steps_per_episode and) train policy for num_epochs_per_iteration epochs on_iteration_end( loss ) if training_done break Args: train_context: context configuring the train loop Hints: o the subclasses training loss is passed through to BackendAgent by on_iteration_end. Thus the subclass must not add the experienced loss to the TrainContext. View Source @abstractmethod def train_implementation ( self , train_context : core . TrainContext ) : \"\"\"Agent specific implementation of the train loop. The implementation should have the form: while True: on_iteration_begin for e in num_episodes_per_iterations play episode and record steps (while steps_in_episode < max_steps_per_episode and) train policy for num_epochs_per_iteration epochs on_iteration_end( loss ) if training_done break Args: train_context: context configuring the train loop Hints: o the subclasses training loss is passed through to BackendAgent by on_iteration_end. Thus the subclass must not add the experienced loss to the TrainContext. \"\"\" TfAgentAgentFactory class TfAgentAgentFactory ( / , * args , ** kwargs ) Backend for TfAgents. Serves as a factory to create algorithm specific wrappers for the TfAgents implementations. View Source class TfAgentAgentFactory ( bcore . BackendAgentFactory ): \"\"\"Backend for TfAgents. Serves as a factory to create algorithm specific wrappers for the TfAgents implementations. \"\"\" backend_name: str = 'tfagents' def get_algorithms ( self ) -> Dict [ Type , Type [ easyagents . backends . core . BackendAgent ]]: \"\"\"Yields a mapping of EasyAgent types to the implementations provided by this backend.\"\"\" return { easyagents . agents . DqnAgent: TfDqnAgent , easyagents . agents . PpoAgent: TfPpoAgent , easyagents . agents . RandomAgent: TfRandomAgent , easyagents . agents . ReinforceAgent: TfReinforceAgent , easyagents . agents . SacAgent: TfSacAgent } Ancestors (in MRO) easyagents.backends.core.BackendAgentFactory abc.ABC Class variables backend_name Methods create_agent def create_agent ( self , easyagent_type : Type , model_config : easyagents . core . ModelConfig ) -> Union [ easyagents . backends . core . _BackendAgent , NoneType ] Creates a backend agent instance implementing the algorithm given by agent_type. Args: easyagent_type: the EasyAgent derived type for which an implementing backend instance will be created model_config: the model_config passed to the constructor of the backend instance. Returns: instance of the agent or None if not implemented by this backend. View Source def create_agent ( self , easyagent_type : Type , model_config : core . ModelConfig ) \\ -> Optional [ _BackendAgent ] : \"\"\"Creates a backend agent instance implementing the algorithm given by agent_type. Args: easyagent_type: the EasyAgent derived type for which an implementing backend instance will be created model_config: the model_config passed to the constructor of the backend instance. Returns: instance of the agent or None if not implemented by this backend. \"\"\" result : Optional [ _BackendAgent ] = None algorithms = self . get_algorithms () if easyagent_type in algorithms : result = algorithms [ easyagent_type ] ( model_config = model_config ) return result get_algorithms def get_algorithms ( self ) -> Dict [ Type , Type [ easyagents . backends . core . BackendAgent ]] Yields a mapping of EasyAgent types to the implementations provided by this backend. View Source def get_algorithms ( self ) -> Dict [ Type , Type [ easyagents . backends . core . BackendAgent ]]: \"\"\"Yields a mapping of EasyAgent types to the implementations provided by this backend.\"\"\" return { easyagents . agents . DqnAgent : TfDqnAgent , easyagents . agents . PpoAgent : TfPpoAgent , easyagents . agents . RandomAgent : TfRandomAgent , easyagents . agents . ReinforceAgent : TfReinforceAgent , easyagents . agents . SacAgent : TfSacAgent } TfDqnAgent class TfDqnAgent ( model_config : easyagents . core . ModelConfig ) creates a new agent based on the DQN algorithm using the tfagents implementation. Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. View Source class TfDqnAgent ( TfAgent ): \"\"\" creates a new agent based on the DQN algorithm using the tfagents implementation. Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. \"\"\" def __init__ ( self , model_config: core . ModelConfig ): super (). __init__ ( model_config = model_config ) def collect_step ( self , env: tf_py_environment . TFPyEnvironment , policy: tf_policy . Base , replay_buffer: TFUniformReplayBuffer ): time_step = env . current_time_step () action_step = policy . action ( time_step ) next_time_step = env . step ( action_step . action ) traj = trajectory . from_transition ( time_step , action_step , next_time_step ) replay_buffer . add_batch ( traj ) # noinspection DuplicatedCode def train_implementation ( self , train_context: core . TrainContext ): \"\"\"Tf-Agents Ppo Implementation of the train loop. The implementation follows https://colab.research.google.com/github/tensorflow/agents/blob/master/tf_agents/colabs/1_dqn_tutorial.ipynb \"\"\" assert isinstance ( train_context , core . StepsTrainContext ) dc: core . StepsTrainContext = train_context train_env = self . _create_env ( discount = dc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () # SetUp Optimizer, Networks and DqnAgent self . log_api ( 'AdamOptimizer' , f' ( learning_rate ={ dc . learning_rate }) ') optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=dc.learning_rate) self.log_api(' QNetwork' , f' ( observation_spec , action_spec , fc_layer_params ={ self . model_config . fc_layers }) ') q_net = q_network.QNetwork(observation_spec, action_spec, fc_layer_params=self.model_config.fc_layers) self.log_api(' DqnAgent' , '(timestep_spec,action_spec,q_network=..., optimizer=...,td_errors_loss_fn=common.element_wise_squared_loss)' ) tf_agent = dqn_agent . DqnAgent ( timestep_spec , action_spec , q_network=q_ net , optimizer = optimizer , td_errors_loss_fn = common . element_wise_squared_loss ) self . log_api ( 'tf_agent.initialize' , f' () ') tf_agent.initialize() self._trained_policy = tf_agent.policy # SetUp Data collection & Buffering self.log_api(' TFUniformReplayBuffer' , f' ( data_spec =..., batch_size ={ train_env . batch_size }, max_length ={ dc . max_steps_in_buffer }) ') replay_buffer = TFUniformReplayBuffer(data_spec=tf_agent.collect_data_spec, batch_size=train_env.batch_size, max_length=dc.max_steps_in_buffer) self.log_api(' RandomTFPolicy' , '()' ) random_policy = random_tf_policy . RandomTFPolicy ( timestep_spec , action_spec ) self . log_api ( 'replay_buffer.add_batch' , '(trajectory)' ) for _ in range ( dc . num_steps_buffer_preload ): self . collect_step ( env = train_env , policy = random_policy , replay_buffer = replay_buffer ) # Train self . log_api ( 'tf_agent.train' , '= common.function(tf_agent.train)' ) tf_agent . train = common . function ( tf_agent . train ) self . log_api ( 'replay_buffer.as_dataset' , f' ( num_parallel_calls = 3 , ' + f' sample_batch_size ={ dc . num_steps_sampled_from_buffer }, num_steps = 2 ). prefetch ( 3 ) ') dataset = replay_buffer.as_dataset(num_parallel_calls=3, sample_batch_size=dc.num_steps_sampled_from_buffer, num_steps=2).prefetch(3) self.log_api(' iter ( dataset' , f' { iter ( dataset )} ') iter_dataset = iter(dataset) self.log_api(' for each iteration' ) self . log_api ( ' replay_buffer.add_batch' , '(trajectory)' ) self . log_api ( ' tf_agent.train' , '(experience=trajectory)' ) while True: self . on_train_iteration_begin () for _ in range ( dc . num_steps_per_iteration ): self . collect_step ( env = train_env , policy = tf_agent . collect_policy , replay_buffer = replay_buffer ) trajectories , _ = next ( iter_dataset ) tf_loss_info = tf_agent . train ( experience = trajectories ) self . on_train_iteration_end ( tf_loss_info . loss ) if train_context . training_done: break return Ancestors (in MRO) easyagents.backends.tfagents.TfAgent easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC Methods collect_step def collect_step ( self , env : tf_agents . environments . tf_py_environment . TFPyEnvironment , policy : tf_agents . policies . tf_policy . Base , replay_buffer : tf_agents . replay_buffers . tf_uniform_replay_buffer . TFUniformReplayBuffer ) View Source def collect_step ( self , env : tf_py_environment . TFPyEnvironment , policy : tf_policy . Base , replay_buffer : TFUniformReplayBuffer ): time_step = env . current_time_step () action_step = policy . action ( time_step ) next_time_step = env . step ( action_step . action ) traj = trajectory . from_transition ( time_step , action_step , next_time_step ) replay_buffer . add_batch ( traj ) load def load ( self , directory : str , callbacks : List [ easyagents . core . AgentCallback ] ) Loads a previously trained and saved actor policy from directory. The loaded policy may afterwards be used by calling play(). Args: directory: the directory containing the trained policy callbacks: list of callbacks called during the load. View Source def load ( self , directory : str , callbacks : List [ core . AgentCallback ]): \"\"\"Loads a previously trained and saved actor policy from directory. The loaded policy may afterwards be used by calling play(). Args: directory: the directory containing the trained policy callbacks: list of callbacks called during the load. \"\"\" assert callbacks is not None , \"callbacks not set\" assert directory assert os . path . isdir ( directory ) self . _callbacks = callbacks self . load_implementation ( directory ) self . _agent_context . _is_policy_trained = True self . _callbacks = None load_implementation def load_implementation ( self , directory : str ) Loads a previously saved actor policy from the directory Args: directory: the directory to load the policy from. View Source def load_implementation ( self , directory : str ): \"\"\"Loads a previously saved actor policy from the directory Args: directory: the directory to load the policy from. \"\"\" assert directory self . log_api ( 'saved_model.load' , f '({directory})' ) self . _trained_policy = tf . compat . v2 . saved_model . load ( directory ) log def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ): \"\"\"Logs msg.\"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg ) log_api def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\"Logs a call to api_target with additional log_msg.\"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg ) on_play_episode_begin def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ): \"\"\"Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. \"\"\" assert env , \"env not set.\" assert isinstance ( env , gym . core . Env ), \"env not an an instance of gym.Env.\" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context ) on_play_episode_end def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ): \"\"\"Must be called by play_implementation at the end of an episode\"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context ) on_train_iteration_begin def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ): \"\"\"Must be called by train_implementation at the begining of a new iteration\"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context ) on_train_iteration_end def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\"Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc.episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc.episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context ) play def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None play_implementation def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : core . PlayContext ): \"\"\"Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used \"\"\" assert play_context , \"play_context not set.\" assert self . _trained_policy , \"trained_policy not set. call train() first.\" if self . _play_env is None : self . _play_env = self . _create_env () gym_env = self . _get_gym_env ( self . _play_env ) while True : self . on_play_episode_begin ( env = gym_env ) time_step = self . _play_env . reset () while not time_step . is_last (): action_step = self . _trained_policy . action ( time_step ) time_step = self . _play_env . step ( action_step . action ) self . on_play_episode_end () if play_context . play_done : break save def save ( self , directory : str , callbacks : List [ easyagents . core . AgentCallback ] ) Saves the currently trained actor policy in directory. Only the actor policy is guaranteed to be saved. Thus after a call to load resuming training is not supported. Args: directory: the directory to save the policy weights to. the directory must exist. callbacks: list of callbacks called during policy load. View Source def save ( self , directory : str , callbacks : List [ core . AgentCallback ]): \"\"\"Saves the currently trained actor policy in directory. Only the actor policy is guaranteed to be saved. Thus after a call to load resuming training is not supported. Args: directory: the directory to save the policy weights to. the directory must exist. callbacks: list of callbacks called during policy load. \"\"\" assert callbacks is not None , \"callbacks not set\" assert directory assert os . path . isdir ( directory ) assert self . _agent_context . _is_policy_trained , \"No trained policy available.\" self . _callbacks = callbacks self . save_implementation ( directory ) self . _callbacks = None save_implementation def save_implementation ( self , directory : str ) Saves the trained actor policy in directory. If no policy was trained yet, no file is written . Args: directory: the directory to save the policy weights to. View Source def save_implementation ( self , directory : str ): \"\"\"Saves the trained actor policy in directory. If no policy was trained yet, no file is written . Args: directory: the directory to save the policy weights to. \"\"\" assert self . _trained_policy , \"no policy trained yet.\" self . log_api ( 'PolicySaver' , f '(trained_policy,seed={self.model_config.seed})' ) saver = policy_saver . PolicySaver ( self . _trained_policy , seed = self . model_config . seed ) self . log_api ( 'policy_saver.save' , f '({directory})' ) saver . save ( directory ) train def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . _agent_context . _is_policy_trained = True self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None train_implementation def train_implementation ( self , train_context : easyagents . core . TrainContext ) Tf-Agents Ppo Implementation of the train loop. The implementation follows https://colab.research.google.com/github/tensorflow/agents/blob/master/tf_agents/colabs/1_dqn_tutorial.ipynb View Source def train_implementation ( self , train_context : core . TrainContext ): \"\"\"Tf-Agents Ppo Implementation of the train loop. The implementation follows https://colab.research.google.com/github/tensorflow/agents/blob/master/tf_agents/colabs/1_dqn_tutorial.ipynb \"\"\" assert isinstance ( train_context , core . StepsTrainContext ) dc : core . StepsTrainContext = train_context train_env = self . _create_env ( discount = dc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () # SetUp Optimizer , Networks and DqnAgent self . log_api ( 'AdamOptimizer' , f '(learning_rate={dc.learning_rate})' ) optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = dc . learning_rate ) self . log_api ( 'QNetwork' , f '(observation_spec, action_spec, fc_layer_params={self.model_config.fc_layers})' ) q_net = q_network . QNetwork ( observation_spec , action_spec , fc_layer_params = self . model_config . fc_layers ) self . log_api ( 'DqnAgent' , '(timestep_spec,action_spec,q_network=..., optimizer=...,td_errors_loss_fn=common.element_wise_squared_loss)' ) tf_agent = dqn_agent . DqnAgent ( timestep_spec , action_spec , q_network = q_net , optimizer = optimizer , td_errors_loss_fn = common . element_wise_squared_loss ) self . log_api ( 'tf_agent.initialize' , f '()' ) tf_agent . initialize () self . _trained_policy = tf_agent . policy # SetUp Data collection & Buffering self . log_api ( 'TFUniformReplayBuffer' , f '(data_spec=..., batch_size={train_env.batch_size}, max_length={dc.max_steps_in_buffer})' ) replay_buffer = TFUniformReplayBuffer ( data_spec = tf_agent . collect_data_spec , batch_size = train_env . batch_size , max_length = dc . max_steps_in_buffer ) self . log_api ( 'RandomTFPolicy' , '()' ) random_policy = random_tf_policy . RandomTFPolicy ( timestep_spec , action_spec ) self . log_api ( 'replay_buffer.add_batch' , '(trajectory)' ) for _ in range ( dc . num_steps_buffer_preload ): self . collect_step ( env = train_env , policy = random_policy , replay_buffer = replay_buffer ) # Train self . log_api ( 'tf_agent.train' , '= common.function(tf_agent.train)' ) tf_agent . train = common . function ( tf_agent . train ) self . log_api ( 'replay_buffer.as_dataset' , f '(num_parallel_calls=3, ' + f 'sample_batch_size={dc.num_steps_sampled_from_buffer}, num_steps=2).prefetch(3)' ) dataset = replay_buffer . as_dataset ( num_parallel_calls = 3 , sample_batch_size = dc . num_steps_sampled_from_buffer , num_steps = 2 ). prefetch ( 3 ) self . log_api ( 'iter(dataset' , f '{iter(dataset)}' ) iter_dataset = iter ( dataset ) self . log_api ( 'for each iteration' ) self . log_api ( ' replay_buffer.add_batch' , '(trajectory)' ) self . log_api ( ' tf_agent.train' , '(experience=trajectory)' ) while True : self . on_train_iteration_begin () for _ in range ( dc . num_steps_per_iteration ): self . collect_step ( env = train_env , policy = tf_agent . collect_policy , replay_buffer = replay_buffer ) trajectories , _ = next ( iter_dataset ) tf_loss_info = tf_agent . train ( experience = trajectories ) self . on_train_iteration_end ( tf_loss_info . loss ) if train_context . training_done : break return TfPpoAgent class TfPpoAgent ( model_config : easyagents . core . ModelConfig ) creates a new agent based on the PPO algorithm using the tfagents implementation. PPO is an actor-critic algorithm using 2 neural networks. The actor network to predict the next action to be taken and the critic network to estimate the value of the game state we are currently in (the expected, discounted sum of future rewards when following the current actor network). Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. View Source class TfPpoAgent ( TfAgent ): \"\"\" creates a new agent based on the PPO algorithm using the tfagents implementation. PPO is an actor-critic algorithm using 2 neural networks. The actor network to predict the next action to be taken and the critic network to estimate the value of the game state we are currently in (the expected, discounted sum of future rewards when following the current actor network). Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. \"\"\" def __init__ ( self , model_config: core . ModelConfig ): super (). __init__ ( model_config = model_config ) # noinspection DuplicatedCode def train_implementation ( self , train_context: core . TrainContext ): \"\"\"Tf-Agents Ppo Implementation of the train loop.\"\"\" assert isinstance ( train_context , core . PpoTrainContext ) tc: core . PpoTrainContext = train_context train_env = self . _create_env ( discount = tc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () # SetUp Optimizer, Networks and PpoAgent self . log_api ( 'AdamOptimizer' , '()' ) optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ) self . log_api ( 'ActorDistributionNetwork' , '()' ) actor_net = actor_distribution_network . ActorDistributionNetwork ( observation_spec , action_spec , fc_layer_params = self . model_config . fc_layers ) self . log_api ( 'ValueNetwork' , '()' ) value_net = value_network . ValueNetwork ( observation_spec , fc_layer_params = self . model_config . fc_layers ) self . log_api ( 'PpoAgent' , '()' ) tf_agent = ppo_agent . PPOAgent ( timestep_spec , action_spec , optimizer , actor_net = actor_net , value_net = value_net , num_epochs = tc . num_epochs_per_iteration ) self . log_api ( 'tf_agent.initialize' , '()' ) tf_agent . initialize () self . _trained_policy = tf_agent . policy # SetUp Data collection & Buffering collect_data_spec = tf_agent . collect_data_spec self . log_api ( 'TFUniformReplayBuffer' , '()' ) replay_buffer = TFUniformReplayBuffer ( collect_data_spec , batch_size = 1 , max_length = tc . max_steps_in_buffer ) collect_policy = tf_agent . collect_policy self . log_api ( 'DynamicEpisodeDriver' , '()' ) collect_driver = DynamicEpisodeDriver ( train_env , collect_policy , observers =[ replay_buffer . add_batch ], num_episodes = tc . num_episodes_per_iteration ) # Train collect_driver . run = common . function ( collect_driver . run , autograph = False ) tf_agent . train = common . function ( tf_agent . train , autograph = False ) while True: self . on_train_iteration_begin () self . log_api ( '-----' , f'iteration { tc . iterations_done_in_training:4 } of { tc . num_iterations: < 4 } ----- ') self.log_api(' collect_driver . run' , '()' ) collect_driver . run () self . log_api ( 'replay_buffer.gather_all' , '()' ) trajectories = replay_buffer . gather_all () self . log_api ( 'tf_agent.train' , '(experience=...)' ) loss_info = tf_agent . train ( experience = trajectories ) total_loss = loss_info . loss . numpy () actor_loss = loss_info . extra . policy_gradient_loss . numpy () critic_loss = loss_info . extra . value_estimation_loss . numpy () self . log_api ( '' , f'loss ={ total_loss: < 7.1 f } [ actor ={ actor_loss: < 7.1 f } critic ={ critic_loss: < 7.1 f }] ') self.log_api(' replay_buffer . clear' , '()' ) replay_buffer . clear () self . on_train_iteration_end ( loss = total_loss , actor_loss = actor_loss , critic_loss = critic_loss ) if tc . training_done: break return Ancestors (in MRO) easyagents.backends.tfagents.TfAgent easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC Methods load def load ( self , directory : str , callbacks : List [ easyagents . core . AgentCallback ] ) Loads a previously trained and saved actor policy from directory. The loaded policy may afterwards be used by calling play(). Args: directory: the directory containing the trained policy callbacks: list of callbacks called during the load. View Source def load ( self , directory : str , callbacks : List [ core . AgentCallback ]): \"\"\"Loads a previously trained and saved actor policy from directory. The loaded policy may afterwards be used by calling play(). Args: directory: the directory containing the trained policy callbacks: list of callbacks called during the load. \"\"\" assert callbacks is not None , \"callbacks not set\" assert directory assert os . path . isdir ( directory ) self . _callbacks = callbacks self . load_implementation ( directory ) self . _agent_context . _is_policy_trained = True self . _callbacks = None load_implementation def load_implementation ( self , directory : str ) Loads a previously saved actor policy from the directory Args: directory: the directory to load the policy from. View Source def load_implementation ( self , directory : str ): \"\"\"Loads a previously saved actor policy from the directory Args: directory: the directory to load the policy from. \"\"\" assert directory self . log_api ( 'saved_model.load' , f '({directory})' ) self . _trained_policy = tf . compat . v2 . saved_model . load ( directory ) log def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ): \"\"\"Logs msg.\"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg ) log_api def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\"Logs a call to api_target with additional log_msg.\"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg ) on_play_episode_begin def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ): \"\"\"Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. \"\"\" assert env , \"env not set.\" assert isinstance ( env , gym . core . Env ), \"env not an an instance of gym.Env.\" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context ) on_play_episode_end def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ): \"\"\"Must be called by play_implementation at the end of an episode\"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context ) on_train_iteration_begin def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ): \"\"\"Must be called by train_implementation at the begining of a new iteration\"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context ) on_train_iteration_end def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\"Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc.episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc.episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context ) play def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None play_implementation def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : core . PlayContext ): \"\"\"Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used \"\"\" assert play_context , \"play_context not set.\" assert self . _trained_policy , \"trained_policy not set. call train() first.\" if self . _play_env is None : self . _play_env = self . _create_env () gym_env = self . _get_gym_env ( self . _play_env ) while True : self . on_play_episode_begin ( env = gym_env ) time_step = self . _play_env . reset () while not time_step . is_last (): action_step = self . _trained_policy . action ( time_step ) time_step = self . _play_env . step ( action_step . action ) self . on_play_episode_end () if play_context . play_done : break save def save ( self , directory : str , callbacks : List [ easyagents . core . AgentCallback ] ) Saves the currently trained actor policy in directory. Only the actor policy is guaranteed to be saved. Thus after a call to load resuming training is not supported. Args: directory: the directory to save the policy weights to. the directory must exist. callbacks: list of callbacks called during policy load. View Source def save ( self , directory : str , callbacks : List [ core . AgentCallback ]): \"\"\"Saves the currently trained actor policy in directory. Only the actor policy is guaranteed to be saved. Thus after a call to load resuming training is not supported. Args: directory: the directory to save the policy weights to. the directory must exist. callbacks: list of callbacks called during policy load. \"\"\" assert callbacks is not None , \"callbacks not set\" assert directory assert os . path . isdir ( directory ) assert self . _agent_context . _is_policy_trained , \"No trained policy available.\" self . _callbacks = callbacks self . save_implementation ( directory ) self . _callbacks = None save_implementation def save_implementation ( self , directory : str ) Saves the trained actor policy in directory. If no policy was trained yet, no file is written . Args: directory: the directory to save the policy weights to. View Source def save_implementation ( self , directory : str ): \"\"\"Saves the trained actor policy in directory. If no policy was trained yet, no file is written . Args: directory: the directory to save the policy weights to. \"\"\" assert self . _trained_policy , \"no policy trained yet.\" self . log_api ( 'PolicySaver' , f '(trained_policy,seed={self.model_config.seed})' ) saver = policy_saver . PolicySaver ( self . _trained_policy , seed = self . model_config . seed ) self . log_api ( 'policy_saver.save' , f '({directory})' ) saver . save ( directory ) train def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . _agent_context . _is_policy_trained = True self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None train_implementation def train_implementation ( self , train_context : easyagents . core . TrainContext ) Tf-Agents Ppo Implementation of the train loop. View Source def train_implementation ( self , train_context : core . TrainContext ): \"\"\"Tf-Agents Ppo Implementation of the train loop.\"\"\" assert isinstance ( train_context , core . PpoTrainContext ) tc : core . PpoTrainContext = train_context train_env = self . _create_env ( discount = tc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () # SetUp Optimizer , Networks and PpoAgent self . log_api ( 'AdamOptimizer' , '()' ) optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ) self . log_api ( 'ActorDistributionNetwork' , '()' ) actor_net = actor_distribution_network . ActorDistributionNetwork ( observation_spec , action_spec , fc_layer_params = self . model_config . fc_layers ) self . log_api ( 'ValueNetwork' , '()' ) value_net = value_network . ValueNetwork ( observation_spec , fc_layer_params = self . model_config . fc_layers ) self . log_api ( 'PpoAgent' , '()' ) tf_agent = ppo_agent . PPOAgent ( timestep_spec , action_spec , optimizer , actor_net = actor_net , value_net = value_net , num_epochs = tc . num_epochs_per_iteration ) self . log_api ( 'tf_agent.initialize' , '()' ) tf_agent . initialize () self . _trained_policy = tf_agent . policy # SetUp Data collection & Buffering collect_data_spec = tf_agent . collect_data_spec self . log_api ( 'TFUniformReplayBuffer' , '()' ) replay_buffer = TFUniformReplayBuffer ( collect_data_spec , batch_size = 1 , max_length = tc . max_steps_in_buffer ) collect_policy = tf_agent . collect_policy self . log_api ( 'DynamicEpisodeDriver' , '()' ) collect_driver = DynamicEpisodeDriver ( train_env , collect_policy , observers = [ replay_buffer . add_batch ], num_episodes = tc . num_episodes_per_iteration ) # Train collect_driver . run = common . function ( collect_driver . run , autograph = False ) tf_agent . train = common . function ( tf_agent . train , autograph = False ) while True : self . on_train_iteration_begin () self . log_api ( '-----' , f 'iteration {tc.iterations_done_in_training:4} of {tc.num_iterations:<4} -----' ) self . log_api ( 'collect_driver.run' , '()' ) collect_driver . run () self . log_api ( 'replay_buffer.gather_all' , '()' ) trajectories = replay_buffer . gather_all () self . log_api ( 'tf_agent.train' , '(experience=...)' ) loss_info = tf_agent . train ( experience = trajectories ) total_loss = loss_info . loss . numpy () actor_loss = loss_info . extra . policy_gradient_loss . numpy () critic_loss = loss_info . extra . value_estimation_loss . numpy () self . log_api ( '' , f 'loss={total_loss:<7.1f} [actor={actor_loss:<7.1f} critic={critic_loss:<7.1f}]' ) self . log_api ( 'replay_buffer.clear' , '()' ) replay_buffer . clear () self . on_train_iteration_end ( loss = total_loss , actor_loss = actor_loss , critic_loss = critic_loss ) if tc . training_done : break return TfRandomAgent class TfRandomAgent ( model_config : easyagents . core . ModelConfig ) creates a new random agent based on uniform random actions. Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. View Source class TfRandomAgent ( TfAgent ): \"\"\" creates a new random agent based on uniform random actions. Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. \"\"\" def __init__ ( self , model_config: core . ModelConfig ): super (). __init__ ( model_config = model_config ) self . _set_trained_policy () def _set_trained_policy ( self ): \"\"\"Tf-Agents Random Implementation of the train loop.\"\"\" self . log ( 'Creating environment...' ) train_env = self . _create_env () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () self . log_api ( 'RandomTFPolicy' , 'create' ) self . _trained_policy = random_tf_policy . RandomTFPolicy ( timestep_spec , action_spec ) self . _agent_context . _is_policy_trained = True def load_implementation ( self , directory: str ): \"\"\"NoOps implementation, since we don't save/load random policies.\"\"\" pass def save_implementation ( self , directory: str ): \"\"\"NoOps implementation, since we don't save/load random policies.\"\"\" pass # noinspection DuplicatedCode def train_implementation ( self , train_context: core . TrainContext ): self . log ( \"Training...\" ) train_env = self . _create_env () while True: self . on_train_iteration_begin () # ensure that 1 episode is played during the iteration time_step = train_env . reset () while not time_step . is_last (): action_step = self . _trained_policy . action ( time_step ) time_step = train_env . step ( action_step . action ) self . on_train_iteration_end ( math . nan ) if train_context . training_done: break return Ancestors (in MRO) easyagents.backends.tfagents.TfAgent easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC Methods load def load ( self , directory : str , callbacks : List [ easyagents . core . AgentCallback ] ) Loads a previously trained and saved actor policy from directory. The loaded policy may afterwards be used by calling play(). Args: directory: the directory containing the trained policy callbacks: list of callbacks called during the load. View Source def load ( self , directory : str , callbacks : List [ core . AgentCallback ]): \"\"\"Loads a previously trained and saved actor policy from directory. The loaded policy may afterwards be used by calling play(). Args: directory: the directory containing the trained policy callbacks: list of callbacks called during the load. \"\"\" assert callbacks is not None , \"callbacks not set\" assert directory assert os . path . isdir ( directory ) self . _callbacks = callbacks self . load_implementation ( directory ) self . _agent_context . _is_policy_trained = True self . _callbacks = None load_implementation def load_implementation ( self , directory : str ) NoOps implementation, since we don't save/load random policies. View Source def load_implementation ( self , directory : str ): \"\"\"NoOps implementation, since we don't save/load random policies.\"\"\" pass log def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ): \"\"\"Logs msg.\"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg ) log_api def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\"Logs a call to api_target with additional log_msg.\"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg ) on_play_episode_begin def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ): \"\"\"Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. \"\"\" assert env , \"env not set.\" assert isinstance ( env , gym . core . Env ), \"env not an an instance of gym.Env.\" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context ) on_play_episode_end def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ): \"\"\"Must be called by play_implementation at the end of an episode\"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context ) on_train_iteration_begin def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ): \"\"\"Must be called by train_implementation at the begining of a new iteration\"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context ) on_train_iteration_end def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\"Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc.episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc.episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context ) play def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None play_implementation def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : core . PlayContext ): \"\"\"Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used \"\"\" assert play_context , \"play_context not set.\" assert self . _trained_policy , \"trained_policy not set. call train() first.\" if self . _play_env is None : self . _play_env = self . _create_env () gym_env = self . _get_gym_env ( self . _play_env ) while True : self . on_play_episode_begin ( env = gym_env ) time_step = self . _play_env . reset () while not time_step . is_last (): action_step = self . _trained_policy . action ( time_step ) time_step = self . _play_env . step ( action_step . action ) self . on_play_episode_end () if play_context . play_done : break save def save ( self , directory : str , callbacks : List [ easyagents . core . AgentCallback ] ) Saves the currently trained actor policy in directory. Only the actor policy is guaranteed to be saved. Thus after a call to load resuming training is not supported. Args: directory: the directory to save the policy weights to. the directory must exist. callbacks: list of callbacks called during policy load. View Source def save ( self , directory : str , callbacks : List [ core . AgentCallback ]): \"\"\"Saves the currently trained actor policy in directory. Only the actor policy is guaranteed to be saved. Thus after a call to load resuming training is not supported. Args: directory: the directory to save the policy weights to. the directory must exist. callbacks: list of callbacks called during policy load. \"\"\" assert callbacks is not None , \"callbacks not set\" assert directory assert os . path . isdir ( directory ) assert self . _agent_context . _is_policy_trained , \"No trained policy available.\" self . _callbacks = callbacks self . save_implementation ( directory ) self . _callbacks = None save_implementation def save_implementation ( self , directory : str ) NoOps implementation, since we don't save/load random policies. View Source def save_implementation ( self , directory : str ): \"\"\"NoOps implementation, since we don't save/load random policies.\"\"\" pass train def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . _agent_context . _is_policy_trained = True self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None train_implementation def train_implementation ( self , train_context : easyagents . core . TrainContext ) Agent specific implementation of the train loop. The implementation should have the form: while True: on_iteration_begin for e in num_episodes_per_iterations play episode and record steps (while steps_in_episode < max_steps_per_episode and) train policy for num_epochs_per_iteration epochs on_iteration_end( loss ) if training_done break Args: train_context: context configuring the train loop Hints: o the subclasses training loss is passed through to BackendAgent by on_iteration_end. Thus the subclass must not add the experienced loss to the TrainContext. View Source def train_implementation ( self , train_context : core . TrainContext ): self . log ( \"Training...\" ) train_env = self . _create_env () while True : self . on_train_iteration_begin () # ensure that 1 episode is played during the iteration time_step = train_env . reset () while not time_step . is_last (): action_step = self . _trained_policy . action ( time_step ) time_step = train_env . step ( action_step . action ) self . on_train_iteration_end ( math . nan ) if train_context . training_done : break return TfReinforceAgent class TfReinforceAgent ( model_config : easyagents . core . ModelConfig ) creates a new agent based on the Reinforce algorithm using the tfagents implementation. Reinforce is a vanilla policy gradient algorithm using a single neural networks to predict the actions. Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. View Source class TfReinforceAgent ( TfAgent ): \"\"\" creates a new agent based on the Reinforce algorithm using the tfagents implementation. Reinforce is a vanilla policy gradient algorithm using a single neural networks to predict the actions. Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. \"\"\" def __init__ ( self , model_config: core . ModelConfig ): super (). __init__ ( model_config = model_config ) # noinspection DuplicatedCode def train_implementation ( self , train_context: core . TrainContext ): \"\"\"Tf-Agents Reinforce Implementation of the train loop.\"\"\" assert isinstance ( train_context , core . EpisodesTrainContext ) tc: core . EpisodesTrainContext = train_context self . log ( 'Creating environment...' ) train_env = self . _create_env ( discount = tc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () # SetUp Optimizer, Networks and PpoAgent self . log_api ( 'AdamOptimizer' , 'create' ) optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ) self . log_api ( 'ActorDistributionNetwork' , 'create' ) actor_net = actor_distribution_network . ActorDistributionNetwork ( observation_spec , action_spec , fc_layer_params = self . model_config . fc_layers ) self . log_api ( 'ReinforceAgent' , 'create' ) tf_agent = reinforce_agent . ReinforceAgent ( timestep_spec , action_spec , actor_network = actor_net , optimizer = optimizer ) self . log_api ( 'tf_agent.initialize()' ) tf_agent . initialize () self . _trained_policy = tf_agent . policy # SetUp Data collection & Buffering collect_data_spec = tf_agent . collect_data_spec self . log_api ( 'TFUniformReplayBuffer' , 'create' ) replay_buffer = TFUniformReplayBuffer ( collect_data_spec , batch_size = 1 , max_length = tc . max_steps_in_buffer ) self . log_api ( 'DynamicEpisodeDriver' , 'create' ) collect_driver = DynamicEpisodeDriver ( train_env , tf_agent . collect_policy , observers =[ replay_buffer . add_batch ], num_episodes = tc . num_episodes_per_iteration ) # Train collect_driver . run = common . function ( collect_driver . run , autograph = False ) tf_agent . train = common . function ( tf_agent . train , autograph = False ) self . log ( 'Starting training...' ) while True: self . on_train_iteration_begin () msg = f'iteration { tc . iterations_done_in_training:4 } of { tc . num_iterations: < 4 } ' self.log_api(' collect_driver . run' , msg ) collect_driver . run () self . log_api ( 'replay_buffer.gather_all' , msg ) trajectories = replay_buffer . gather_all () self . log_api ( 'tf_agent.train' , msg ) loss_info = tf_agent . train ( experience = trajectories ) total_loss = loss_info . loss . numpy () self . log_api ( '' , f'loss ={ total_loss: < 7.1 f } ') self.log_api(' replay_buffer . clear' , msg ) replay_buffer . clear () self . on_train_iteration_end ( loss = total_loss ) if tc . training_done: break return Ancestors (in MRO) easyagents.backends.tfagents.TfAgent easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC Methods load def load ( self , directory : str , callbacks : List [ easyagents . core . AgentCallback ] ) Loads a previously trained and saved actor policy from directory. The loaded policy may afterwards be used by calling play(). Args: directory: the directory containing the trained policy callbacks: list of callbacks called during the load. View Source def load ( self , directory : str , callbacks : List [ core . AgentCallback ]): \"\"\"Loads a previously trained and saved actor policy from directory. The loaded policy may afterwards be used by calling play(). Args: directory: the directory containing the trained policy callbacks: list of callbacks called during the load. \"\"\" assert callbacks is not None , \"callbacks not set\" assert directory assert os . path . isdir ( directory ) self . _callbacks = callbacks self . load_implementation ( directory ) self . _agent_context . _is_policy_trained = True self . _callbacks = None load_implementation def load_implementation ( self , directory : str ) Loads a previously saved actor policy from the directory Args: directory: the directory to load the policy from. View Source def load_implementation ( self , directory : str ): \"\"\"Loads a previously saved actor policy from the directory Args: directory: the directory to load the policy from. \"\"\" assert directory self . log_api ( 'saved_model.load' , f '({directory})' ) self . _trained_policy = tf . compat . v2 . saved_model . load ( directory ) log def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ): \"\"\"Logs msg.\"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg ) log_api def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\"Logs a call to api_target with additional log_msg.\"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg ) on_play_episode_begin def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ): \"\"\"Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. \"\"\" assert env , \"env not set.\" assert isinstance ( env , gym . core . Env ), \"env not an an instance of gym.Env.\" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context ) on_play_episode_end def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ): \"\"\"Must be called by play_implementation at the end of an episode\"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context ) on_train_iteration_begin def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ): \"\"\"Must be called by train_implementation at the begining of a new iteration\"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context ) on_train_iteration_end def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\"Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc.episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc.episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context ) play def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None play_implementation def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : core . PlayContext ): \"\"\"Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used \"\"\" assert play_context , \"play_context not set.\" assert self . _trained_policy , \"trained_policy not set. call train() first.\" if self . _play_env is None : self . _play_env = self . _create_env () gym_env = self . _get_gym_env ( self . _play_env ) while True : self . on_play_episode_begin ( env = gym_env ) time_step = self . _play_env . reset () while not time_step . is_last (): action_step = self . _trained_policy . action ( time_step ) time_step = self . _play_env . step ( action_step . action ) self . on_play_episode_end () if play_context . play_done : break save def save ( self , directory : str , callbacks : List [ easyagents . core . AgentCallback ] ) Saves the currently trained actor policy in directory. Only the actor policy is guaranteed to be saved. Thus after a call to load resuming training is not supported. Args: directory: the directory to save the policy weights to. the directory must exist. callbacks: list of callbacks called during policy load. View Source def save ( self , directory : str , callbacks : List [ core . AgentCallback ]): \"\"\"Saves the currently trained actor policy in directory. Only the actor policy is guaranteed to be saved. Thus after a call to load resuming training is not supported. Args: directory: the directory to save the policy weights to. the directory must exist. callbacks: list of callbacks called during policy load. \"\"\" assert callbacks is not None , \"callbacks not set\" assert directory assert os . path . isdir ( directory ) assert self . _agent_context . _is_policy_trained , \"No trained policy available.\" self . _callbacks = callbacks self . save_implementation ( directory ) self . _callbacks = None save_implementation def save_implementation ( self , directory : str ) Saves the trained actor policy in directory. If no policy was trained yet, no file is written . Args: directory: the directory to save the policy weights to. View Source def save_implementation ( self , directory : str ): \"\"\"Saves the trained actor policy in directory. If no policy was trained yet, no file is written . Args: directory: the directory to save the policy weights to. \"\"\" assert self . _trained_policy , \"no policy trained yet.\" self . log_api ( 'PolicySaver' , f '(trained_policy,seed={self.model_config.seed})' ) saver = policy_saver . PolicySaver ( self . _trained_policy , seed = self . model_config . seed ) self . log_api ( 'policy_saver.save' , f '({directory})' ) saver . save ( directory ) train def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . _agent_context . _is_policy_trained = True self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None train_implementation def train_implementation ( self , train_context : easyagents . core . TrainContext ) Tf-Agents Reinforce Implementation of the train loop. View Source def train_implementation ( self , train_context : core . TrainContext ): \"\"\"Tf-Agents Reinforce Implementation of the train loop.\"\"\" assert isinstance ( train_context , core . EpisodesTrainContext ) tc : core . EpisodesTrainContext = train_context self . log ( 'Creating environment...' ) train_env = self . _create_env ( discount = tc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () # SetUp Optimizer , Networks and PpoAgent self . log_api ( 'AdamOptimizer' , 'create' ) optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ) self . log_api ( 'ActorDistributionNetwork' , 'create' ) actor_net = actor_distribution_network . ActorDistributionNetwork ( observation_spec , action_spec , fc_layer_params = self . model_config . fc_layers ) self . log_api ( 'ReinforceAgent' , 'create' ) tf_agent = reinforce_agent . ReinforceAgent ( timestep_spec , action_spec , actor_network = actor_net , optimizer = optimizer ) self . log_api ( 'tf_agent.initialize()' ) tf_agent . initialize () self . _trained_policy = tf_agent . policy # SetUp Data collection & Buffering collect_data_spec = tf_agent . collect_data_spec self . log_api ( 'TFUniformReplayBuffer' , 'create' ) replay_buffer = TFUniformReplayBuffer ( collect_data_spec , batch_size = 1 , max_length = tc . max_steps_in_buffer ) self . log_api ( 'DynamicEpisodeDriver' , 'create' ) collect_driver = DynamicEpisodeDriver ( train_env , tf_agent . collect_policy , observers = [ replay_buffer . add_batch ], num_episodes = tc . num_episodes_per_iteration ) # Train collect_driver . run = common . function ( collect_driver . run , autograph = False ) tf_agent . train = common . function ( tf_agent . train , autograph = False ) self . log ( 'Starting training...' ) while True : self . on_train_iteration_begin () msg = f 'iteration {tc.iterations_done_in_training:4} of {tc.num_iterations:<4}' self . log_api ( 'collect_driver.run' , msg ) collect_driver . run () self . log_api ( 'replay_buffer.gather_all' , msg ) trajectories = replay_buffer . gather_all () self . log_api ( 'tf_agent.train' , msg ) loss_info = tf_agent . train ( experience = trajectories ) total_loss = loss_info . loss . numpy () self . log_api ( '' , f 'loss={total_loss:<7.1f}' ) self . log_api ( 'replay_buffer.clear' , msg ) replay_buffer . clear () self . on_train_iteration_end ( loss = total_loss ) if tc . training_done : break return TfSacAgent class TfSacAgent ( model_config : easyagents . core . ModelConfig ) creates a new agent based on the SAC algorithm using the tfagents implementation. adapted from https://github.com/tensorflow/agents/blob/master/tf_agents/colabs/7_SAC_minitaur_tutorial.ipynb Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. View Source class TfSacAgent ( TfAgent ): \"\"\" creates a new agent based on the SAC algorithm using the tfagents implementation. adapted from https://github.com/tensorflow/agents/blob/master/tf_agents/colabs/7_SAC_minitaur_tutorial.ipynb Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. \"\"\" def __init__ ( self , model_config: core . ModelConfig ): super (). __init__ ( model_config = model_config ) # noinspection DuplicatedCode def train_implementation ( self , train_context: core . TrainContext ): \"\"\"Tf-Agents Ppo Implementation of the train loop.\"\"\" assert isinstance ( train_context , core . StepsTrainContext ) tc: core . StepsTrainContext = train_context train_env = self . _create_env ( discount = tc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () self . log_api ( 'CriticNetwork' , f' ( observation_spec , action_spec ), observation_fc_layer_params = None , ' + f' action_fc_layer_params = None , joint_fc_layer_params ={ self . model_config . fc_layers }) ') critic_net = critic_network.CriticNetwork((observation_spec, action_spec), observation_fc_layer_params=None, action_fc_layer_params=None, joint_fc_layer_params=self.model_config.fc_layers) def normal_projection_net(action_spec_arg, init_means_output_factor=0.1): return normal_projection_network.NormalProjectionNetwork(action_spec_arg, mean_transform=None, state_dependent_std=True, init_means_output_factor=init_means_output_factor, std_transform=sac_agent.std_clip_transform, scale_distribution=True) self.log_api(' ActorDistributionNetwork' , f'observation_spec , action_spec , fc_layer_params ={ self . model_config . fc_layers }), ' + f' continuous_projection_net =...) ') actor_net = actor_distribution_network.ActorDistributionNetwork(observation_spec, action_spec, fc_layer_params=self.model_config.fc_layers, continuous_projection_net=normal_projection_net) # self.log_api(' tf . compat . v1 . train . get_or_create_global_step' , '()' ) # global_step = tf.compat.v1.train.get_or_create_global_step() self . log_api ( 'SacAgent' , f' ( timestep_spec , action_spec , actor_network =..., critic_network =..., ' + f' actor_optimizer = AdamOptimizer ( learning_rate ={ tc . learning_rate }), ' + f' critic_optimizer = AdamOptimizer ( learning_rate ={ tc . learning_rate }), ' + f' alpha_optimizer = AdamOptimizer ( learning_rate ={ tc . learning_rate }), ' + f' gamma ={ tc . reward_discount_gamma }) ') tf_agent = sac_agent.SacAgent( timestep_spec, action_spec, actor_network=actor_net, critic_network=critic_net, actor_optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=tc.learning_rate), critic_optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=tc.learning_rate), alpha_optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=tc.learning_rate), # target_update_tau=0.005, # target_update_period=1, # td_errors_loss_fn=tf.compat.v1.losses.mean_squared_error, gamma=tc.reward_discount_gamma) # reward_scale_factor=1.0, # gradient_clipping=None, # train_step_counter=global_step) self.log_api(' tf_agent . initialize' , '()' ) tf_agent . initialize () self . _trained_policy = greedy_policy . GreedyPolicy ( tf_agent . policy ) collect_policy = tf_agent . collect_policy # setup and preload replay buffer self . log_api ( 'TFUniformReplayBuffer' , f' ( data_spec = tf_agent . collect_data_spec , ' + f' batch_size ={ train_env . batch_size }, max_length ={ tc . max_steps_in_buffer }) ') replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(data_spec=tf_agent.collect_data_spec, batch_size=train_env.batch_size, max_length=tc.max_steps_in_buffer) self.log_api(' DynamicStepDriver' , f' ( env , collect_policy , observers =[ replay_buffer . add_batch ], ' + f' num_steps ={ tc . num_steps_buffer_preload }) ') initial_collect_driver = dynamic_step_driver.DynamicStepDriver(train_env, collect_policy, observers=[replay_buffer.add_batch], num_steps=tc.num_steps_buffer_preload) self.log_api(' initial_collect_driver . run () ') initial_collect_driver.run() # Dataset generates trajectories with shape [Bx2x...] dataset = replay_buffer.as_dataset(num_parallel_calls=3, sample_batch_size=tc.num_steps_sampled_from_buffer, num_steps=2).prefetch(3) iterator = iter(dataset) self.log_api(' DynamicStepDriver' , f' ( env , collect_policy , observers =[ replay_buffer . add_batch ], ' + f' num_steps ={ tc . num_steps_per_iteration }) ') collect_driver = dynamic_step_driver.DynamicStepDriver(train_env, collect_policy, observers=[replay_buffer.add_batch], num_steps=tc.num_steps_per_iteration) # (Optional) Optimize by wrapping some of the code in a graph using TF function. tf_agent.train = common.function(tf_agent.train) collect_driver.run = common.function(collect_driver.run) self.log_api(' for each iteration' ) self . log_api ( ' collect_driver.run' , '()' ) self . log_api ( ' tf_agent.train' , '(experience=...)' ) while True: self . on_train_iteration_begin () # Collect a few steps using collect_policy and save to the replay buffer. for _ in range ( tc . num_steps_per_iteration ): collect_driver . run () # Sample a batch of data from the buffer and update the agent's network. experience , _ = next ( iterator ) loss_info = tf_agent . train ( experience ) total_loss = loss_info . loss . numpy () actor_loss = loss_info . extra . actor_loss alpha_loss = loss_info . extra . alpha_loss critic_loss = loss_info . extra . critic_loss self . on_train_iteration_end ( loss = total_loss , actor_loss = actor_loss , critic_loss = critic_loss , alpha_loss = alpha_loss ) if tc . training_done: break return Ancestors (in MRO) easyagents.backends.tfagents.TfAgent easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC Methods load def load ( self , directory : str , callbacks : List [ easyagents . core . AgentCallback ] ) Loads a previously trained and saved actor policy from directory. The loaded policy may afterwards be used by calling play(). Args: directory: the directory containing the trained policy callbacks: list of callbacks called during the load. View Source def load ( self , directory : str , callbacks : List [ core . AgentCallback ]): \"\"\"Loads a previously trained and saved actor policy from directory. The loaded policy may afterwards be used by calling play(). Args: directory: the directory containing the trained policy callbacks: list of callbacks called during the load. \"\"\" assert callbacks is not None , \"callbacks not set\" assert directory assert os . path . isdir ( directory ) self . _callbacks = callbacks self . load_implementation ( directory ) self . _agent_context . _is_policy_trained = True self . _callbacks = None load_implementation def load_implementation ( self , directory : str ) Loads a previously saved actor policy from the directory Args: directory: the directory to load the policy from. View Source def load_implementation ( self , directory : str ): \"\"\"Loads a previously saved actor policy from the directory Args: directory: the directory to load the policy from. \"\"\" assert directory self . log_api ( 'saved_model.load' , f '({directory})' ) self . _trained_policy = tf . compat . v2 . saved_model . load ( directory ) log def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ): \"\"\"Logs msg.\"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg ) log_api def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\"Logs a call to api_target with additional log_msg.\"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg ) on_play_episode_begin def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ): \"\"\"Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. \"\"\" assert env , \"env not set.\" assert isinstance ( env , gym . core . Env ), \"env not an an instance of gym.Env.\" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context ) on_play_episode_end def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ): \"\"\"Must be called by play_implementation at the end of an episode\"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context ) on_train_iteration_begin def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ): \"\"\"Must be called by train_implementation at the begining of a new iteration\"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context ) on_train_iteration_end def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\"Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc.episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc.episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context ) play def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None play_implementation def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : core . PlayContext ): \"\"\"Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used \"\"\" assert play_context , \"play_context not set.\" assert self . _trained_policy , \"trained_policy not set. call train() first.\" if self . _play_env is None : self . _play_env = self . _create_env () gym_env = self . _get_gym_env ( self . _play_env ) while True : self . on_play_episode_begin ( env = gym_env ) time_step = self . _play_env . reset () while not time_step . is_last (): action_step = self . _trained_policy . action ( time_step ) time_step = self . _play_env . step ( action_step . action ) self . on_play_episode_end () if play_context . play_done : break save def save ( self , directory : str , callbacks : List [ easyagents . core . AgentCallback ] ) Saves the currently trained actor policy in directory. Only the actor policy is guaranteed to be saved. Thus after a call to load resuming training is not supported. Args: directory: the directory to save the policy weights to. the directory must exist. callbacks: list of callbacks called during policy load. View Source def save ( self , directory : str , callbacks : List [ core . AgentCallback ]): \"\"\"Saves the currently trained actor policy in directory. Only the actor policy is guaranteed to be saved. Thus after a call to load resuming training is not supported. Args: directory: the directory to save the policy weights to. the directory must exist. callbacks: list of callbacks called during policy load. \"\"\" assert callbacks is not None , \"callbacks not set\" assert directory assert os . path . isdir ( directory ) assert self . _agent_context . _is_policy_trained , \"No trained policy available.\" self . _callbacks = callbacks self . save_implementation ( directory ) self . _callbacks = None save_implementation def save_implementation ( self , directory : str ) Saves the trained actor policy in directory. If no policy was trained yet, no file is written . Args: directory: the directory to save the policy weights to. View Source def save_implementation ( self , directory : str ): \"\"\"Saves the trained actor policy in directory. If no policy was trained yet, no file is written . Args: directory: the directory to save the policy weights to. \"\"\" assert self . _trained_policy , \"no policy trained yet.\" self . log_api ( 'PolicySaver' , f '(trained_policy,seed={self.model_config.seed})' ) saver = policy_saver . PolicySaver ( self . _trained_policy , seed = self . model_config . seed ) self . log_api ( 'policy_saver.save' , f '({directory})' ) saver . save ( directory ) train def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . _agent_context . _is_policy_trained = True self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None train_implementation def train_implementation ( self , train_context : easyagents . core . TrainContext ) Tf-Agents Ppo Implementation of the train loop. View Source def train_implementation ( self , train_context : core . TrainContext ): \"\"\"Tf-Agents Ppo Implementation of the train loop.\"\"\" assert isinstance ( train_context , core . StepsTrainContext ) tc : core . StepsTrainContext = train_context train_env = self . _create_env ( discount = tc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () self . log_api ( 'CriticNetwork' , f '(observation_spec, action_spec), observation_fc_layer_params=None, ' + f 'action_fc_layer_params=None, joint_fc_layer_params={self.model_config.fc_layers})' ) critic_net = critic_network . CriticNetwork (( observation_spec , action_spec ), observation_fc_layer_params = None , action_fc_layer_params = None , joint_fc_layer_params = self . model_config . fc_layers ) def normal_projection_net ( action_spec_arg , init_means_output_factor = 0 . 1 ): return normal_projection_network . NormalProjectionNetwork ( action_spec_arg , mean_transform = None , state_dependent_std = True , init_means_output_factor = init_means_output_factor , std_transform = sac_agent . std_clip_transform , scale_distribution = True ) self . log_api ( 'ActorDistributionNetwork' , f 'observation_spec, action_spec, fc_layer_params={self.model_config.fc_layers}), ' + f 'continuous_projection_net=...)' ) actor_net = actor_distribution_network . ActorDistributionNetwork ( observation_spec , action_spec , fc_layer_params = self . model_config . fc_layers , continuous_projection_net = normal_projection_net ) # self . log_api ( 'tf.compat.v1.train.get_or_create_global_step' , '()' ) # global_step = tf . compat . v1 . train . get_or_create_global_step () self . log_api ( 'SacAgent' , f '(timestep_spec, action_spec, actor_network=..., critic_network=..., ' + f 'actor_optimizer=AdamOptimizer(learning_rate={tc.learning_rate}), ' + f 'critic_optimizer=AdamOptimizer(learning_rate={tc.learning_rate}), ' + f 'alpha_optimizer=AdamOptimizer(learning_rate={tc.learning_rate}), ' + f 'gamma={tc.reward_discount_gamma})' ) tf_agent = sac_agent . SacAgent ( timestep_spec , action_spec , actor_network = actor_net , critic_network = critic_net , actor_optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ), critic_optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ), alpha_optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ), # target_update_tau = 0 . 005 , # target_update_period = 1 , # td_errors_loss_fn = tf . compat . v1 . losses . mean_squared_error , gamma = tc . reward_discount_gamma ) # reward_scale_factor = 1 . 0 , # gradient_clipping = None , # train_step_counter = global_step ) self . log_api ( 'tf_agent.initialize' , '()' ) tf_agent . initialize () self . _trained_policy = greedy_policy . GreedyPolicy ( tf_agent . policy ) collect_policy = tf_agent . collect_policy # setup and preload replay buffer self . log_api ( 'TFUniformReplayBuffer' , f '(data_spec=tf_agent.collect_data_spec, ' + f 'batch_size={train_env.batch_size}, max_length={tc.max_steps_in_buffer})' ) replay_buffer = tf_uniform_replay_buffer . TFUniformReplayBuffer ( data_spec = tf_agent . collect_data_spec , batch_size = train_env . batch_size , max_length = tc . max_steps_in_buffer ) self . log_api ( 'DynamicStepDriver' , f '(env, collect_policy, observers=[replay_buffer.add_batch], ' + f 'num_steps={tc.num_steps_buffer_preload})' ) initial_collect_driver = dynamic_step_driver . DynamicStepDriver ( train_env , collect_policy , observers = [ replay_buffer . add_batch ], num_steps = tc . num_steps_buffer_preload ) self . log_api ( 'initial_collect_driver.run()' ) initial_collect_driver . run () # Dataset generates trajectories with shape [ Bx2x ...] dataset = replay_buffer . as_dataset ( num_parallel_calls = 3 , sample_batch_size = tc . num_steps_sampled_from_buffer , num_steps = 2 ). prefetch ( 3 ) iterator = iter ( dataset ) self . log_api ( 'DynamicStepDriver' , f '(env, collect_policy, observers=[replay_buffer.add_batch], ' + f 'num_steps={tc.num_steps_per_iteration})' ) collect_driver = dynamic_step_driver . DynamicStepDriver ( train_env , collect_policy , observers = [ replay_buffer . add_batch ], num_steps = tc . num_steps_per_iteration ) # ( Optional ) Optimize by wrapping some of the code in a graph using TF function . tf_agent . train = common . function ( tf_agent . train ) collect_driver . run = common . function ( collect_driver . run ) self . log_api ( 'for each iteration' ) self . log_api ( ' collect_driver.run' , '()' ) self . log_api ( ' tf_agent.train' , '(experience=...)' ) while True : self . on_train_iteration_begin () # Collect a few steps using collect_policy and save to the replay buffer . for _ in range ( tc . num_steps_per_iteration ): collect_driver . run () # Sample a batch of data from the buffer and update the agent ' s network . experience , _ = next ( iterator ) loss_info = tf_agent . train ( experience ) total_loss = loss_info . loss . numpy () actor_loss = loss_info . extra . actor_loss alpha_loss = loss_info . extra . alpha_loss critic_loss = loss_info . extra . critic_loss self . on_train_iteration_end ( loss = total_loss , actor_loss = actor_loss , critic_loss = critic_loss , alpha_loss = alpha_loss ) if tc . training_done : break return","title":"Tfagents"},{"location":"reference/easyagents/backends/tfagents/#module-easyagentsbackendstfagents","text":"This module contains the backend implementation for tf Agents (see https://github.com/tensorflow/agents) View Source \"\"\"This module contains the backend implementation for tf Agents (see https://github.com/tensorflow/agents)\"\"\" from abc import ABCMeta from typing import Dict , Type import math import os # noinspection PyUnresolvedReferences import easyagents.agents from easyagents import core from easyagents.backends import core as bcore from easyagents.backends import monitor # noinspection PyPackageRequirements import tensorflow as tf from tf_agents.agents.ddpg import critic_network from tf_agents.agents.dqn import dqn_agent from tf_agents.agents.ppo import ppo_agent from tf_agents.agents.reinforce import reinforce_agent from tf_agents.agents.sac import sac_agent from tf_agents.drivers import dynamic_step_driver from tf_agents.drivers.dynamic_episode_driver import DynamicEpisodeDriver from tf_agents.environments import gym_wrapper , py_environment , tf_py_environment from tf_agents.networks import actor_distribution_network , normal_projection_network , q_network , value_network from tf_agents.policies import greedy_policy , tf_policy , random_tf_policy , policy_saver from tf_agents.replay_buffers import tf_uniform_replay_buffer from tf_agents.replay_buffers.tf_uniform_replay_buffer import TFUniformReplayBuffer from tf_agents.trajectories import trajectory from tf_agents.utils import common import gym # noinspection PyUnresolvedReferences,PyAbstractClass class TfAgent ( bcore . BackendAgent , metaclass = ABCMeta ): \"\"\"Reinforcement learning agents based on googles tf_agent implementations https://github.com/tensorflow/agents \"\"\" def __init__ ( self , model_config : core . ModelConfig ): super () . __init__ ( model_config = model_config , backend_name = TfAgentAgentFactory . backend_name , tf_eager_execution = True ) self . _trained_policy = None self . _play_env : Optional [ gym . Env ] = None def _create_gym_with_wrapper ( self , discount ): gym_spec = gym . spec ( self . model_config . gym_env_name ) gym_env = gym_spec . make () # simplify_box_bounds: Whether to replace bounds of Box space that are arrays # with identical values with one number and rely on broadcasting. # important, simplify_box_bounds True crashes environments with boundaries with identical values env = gym_wrapper . GymWrapper ( gym_env , discount = discount , simplify_box_bounds = False ) return env def _create_env ( self , discount : float = 1 ) -> tf_py_environment . TFPyEnvironment : \"\"\" creates a new instance of the gym environment and wraps it in a tfagent TFPyEnvironment Args: discount: the reward discount factor \"\"\" assert 0 < discount <= 1 , \"discount not admissible\" self . log_api ( f 'TFPyEnvironment' , f '( suite_gym.load( \"{self.model_config.original_env_name}\", discount={discount}) )' ) # suit_gym.load crashes our environment # py_env = suite_gym.load(self.model_config.gym_env_name, discount=discount) py_env = self . _create_gym_with_wrapper ( discount ) result = tf_py_environment . TFPyEnvironment ( py_env ) return result def _get_gym_env ( self , tf_py_env : tf_py_environment . TFPyEnvironment ) -> monitor . _MonitorEnv : \"\"\" extracts the underlying _MonitorEnv from tf_py_env created by _create_tfagent_env\"\"\" assert isinstance ( tf_py_env , tf_py_environment . TFPyEnvironment ), \\ \"passed tf_py_env is not an instance of TFPyEnvironment\" assert isinstance ( tf_py_env . pyenv , py_environment . PyEnvironment ), \\ \"passed TFPyEnvironment.pyenv does not contain a PyEnvironment\" assert len ( tf_py_env . pyenv . envs ) == 1 , \"passed TFPyEnvironment.pyenv does not contain a unique environment\" result = tf_py_env . pyenv . envs [ 0 ] . gym assert isinstance ( result , monitor . _MonitorEnv ), \"passed TFPyEnvironment does not contain a _MonitorEnv\" return result def play_implementation ( self , play_context : core . PlayContext ): \"\"\"Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used \"\"\" assert play_context , \"play_context not set.\" assert self . _trained_policy , \"trained_policy not set. call train() first.\" if self . _play_env is None : self . _play_env = self . _create_env () gym_env = self . _get_gym_env ( self . _play_env ) while True : self . on_play_episode_begin ( env = gym_env ) time_step = self . _play_env . reset () while not time_step . is_last (): action_step = self . _trained_policy . action ( time_step ) time_step = self . _play_env . step ( action_step . action ) self . on_play_episode_end () if play_context . play_done : break def load_implementation ( self , directory : str ): \"\"\"Loads a previously saved actor policy from the directory Args: directory: the directory to load the policy from. \"\"\" assert directory self . log_api ( 'saved_model.load' , f '({directory})' ) self . _trained_policy = tf . compat . v2 . saved_model . load ( directory ) def save_implementation ( self , directory : str ): \"\"\"Saves the trained actor policy in directory. If no policy was trained yet, no file is written . Args: directory: the directory to save the policy weights to. \"\"\" assert self . _trained_policy , \"no policy trained yet.\" self . log_api ( 'PolicySaver' , f '(trained_policy,seed={self.model_config.seed})' ) saver = policy_saver . PolicySaver ( self . _trained_policy , seed = self . model_config . seed ) self . log_api ( 'policy_saver.save' , f '({directory})' ) saver . save ( directory ) # noinspection PyUnresolvedReferences class TfDqnAgent ( TfAgent ): \"\"\" creates a new agent based on the DQN algorithm using the tfagents implementation. Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. \"\"\" def __init__ ( self , model_config : core . ModelConfig ): super () . __init__ ( model_config = model_config ) def collect_step ( self , env : tf_py_environment . TFPyEnvironment , policy : tf_policy . Base , replay_buffer : TFUniformReplayBuffer ): time_step = env . current_time_step () action_step = policy . action ( time_step ) next_time_step = env . step ( action_step . action ) traj = trajectory . from_transition ( time_step , action_step , next_time_step ) replay_buffer . add_batch ( traj ) # noinspection DuplicatedCode def train_implementation ( self , train_context : core . TrainContext ): \"\"\"Tf-Agents Ppo Implementation of the train loop. The implementation follows https://colab.research.google.com/github/tensorflow/agents/blob/master/tf_agents/colabs/1_dqn_tutorial.ipynb \"\"\" assert isinstance ( train_context , core . StepsTrainContext ) dc : core . StepsTrainContext = train_context train_env = self . _create_env ( discount = dc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () # SetUp Optimizer, Networks and DqnAgent self . log_api ( 'AdamOptimizer' , f '(learning_rate={dc.learning_rate})' ) optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = dc . learning_rate ) self . log_api ( 'QNetwork' , f '(observation_spec, action_spec, fc_layer_params={self.model_config.fc_layers})' ) q_net = q_network . QNetwork ( observation_spec , action_spec , fc_layer_params = self . model_config . fc_layers ) self . log_api ( 'DqnAgent' , '(timestep_spec,action_spec,q_network=..., optimizer=...,td_errors_loss_fn=common.element_wise_squared_loss)' ) tf_agent = dqn_agent . DqnAgent ( timestep_spec , action_spec , q_network = q_net , optimizer = optimizer , td_errors_loss_fn = common . element_wise_squared_loss ) self . log_api ( 'tf_agent.initialize' , f '()' ) tf_agent . initialize () self . _trained_policy = tf_agent . policy # SetUp Data collection & Buffering self . log_api ( 'TFUniformReplayBuffer' , f '(data_spec=..., batch_size={train_env.batch_size}, max_length={dc.max_steps_in_buffer})' ) replay_buffer = TFUniformReplayBuffer ( data_spec = tf_agent . collect_data_spec , batch_size = train_env . batch_size , max_length = dc . max_steps_in_buffer ) self . log_api ( 'RandomTFPolicy' , '()' ) random_policy = random_tf_policy . RandomTFPolicy ( timestep_spec , action_spec ) self . log_api ( 'replay_buffer.add_batch' , '(trajectory)' ) for _ in range ( dc . num_steps_buffer_preload ): self . collect_step ( env = train_env , policy = random_policy , replay_buffer = replay_buffer ) # Train self . log_api ( 'tf_agent.train' , '= common.function(tf_agent.train)' ) tf_agent . train = common . function ( tf_agent . train ) self . log_api ( 'replay_buffer.as_dataset' , f '(num_parallel_calls=3, ' + f 'sample_batch_size={dc.num_steps_sampled_from_buffer}, num_steps=2).prefetch(3)' ) dataset = replay_buffer . as_dataset ( num_parallel_calls = 3 , sample_batch_size = dc . num_steps_sampled_from_buffer , num_steps = 2 ) . prefetch ( 3 ) self . log_api ( 'iter(dataset' , f '{iter(dataset)}' ) iter_dataset = iter ( dataset ) self . log_api ( 'for each iteration' ) self . log_api ( ' replay_buffer.add_batch' , '(trajectory)' ) self . log_api ( ' tf_agent.train' , '(experience=trajectory)' ) while True : self . on_train_iteration_begin () for _ in range ( dc . num_steps_per_iteration ): self . collect_step ( env = train_env , policy = tf_agent . collect_policy , replay_buffer = replay_buffer ) trajectories , _ = next ( iter_dataset ) tf_loss_info = tf_agent . train ( experience = trajectories ) self . on_train_iteration_end ( tf_loss_info . loss ) if train_context . training_done : break return # noinspection PyUnresolvedReferences class TfPpoAgent ( TfAgent ): \"\"\" creates a new agent based on the PPO algorithm using the tfagents implementation. PPO is an actor-critic algorithm using 2 neural networks. The actor network to predict the next action to be taken and the critic network to estimate the value of the game state we are currently in (the expected, discounted sum of future rewards when following the current actor network). Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. \"\"\" def __init__ ( self , model_config : core . ModelConfig ): super () . __init__ ( model_config = model_config ) # noinspection DuplicatedCode def train_implementation ( self , train_context : core . TrainContext ): \"\"\"Tf-Agents Ppo Implementation of the train loop.\"\"\" assert isinstance ( train_context , core . PpoTrainContext ) tc : core . PpoTrainContext = train_context train_env = self . _create_env ( discount = tc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () # SetUp Optimizer, Networks and PpoAgent self . log_api ( 'AdamOptimizer' , '()' ) optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ) self . log_api ( 'ActorDistributionNetwork' , '()' ) actor_net = actor_distribution_network . ActorDistributionNetwork ( observation_spec , action_spec , fc_layer_params = self . model_config . fc_layers ) self . log_api ( 'ValueNetwork' , '()' ) value_net = value_network . ValueNetwork ( observation_spec , fc_layer_params = self . model_config . fc_layers ) self . log_api ( 'PpoAgent' , '()' ) tf_agent = ppo_agent . PPOAgent ( timestep_spec , action_spec , optimizer , actor_net = actor_net , value_net = value_net , num_epochs = tc . num_epochs_per_iteration ) self . log_api ( 'tf_agent.initialize' , '()' ) tf_agent . initialize () self . _trained_policy = tf_agent . policy # SetUp Data collection & Buffering collect_data_spec = tf_agent . collect_data_spec self . log_api ( 'TFUniformReplayBuffer' , '()' ) replay_buffer = TFUniformReplayBuffer ( collect_data_spec , batch_size = 1 , max_length = tc . max_steps_in_buffer ) collect_policy = tf_agent . collect_policy self . log_api ( 'DynamicEpisodeDriver' , '()' ) collect_driver = DynamicEpisodeDriver ( train_env , collect_policy , observers = [ replay_buffer . add_batch ], num_episodes = tc . num_episodes_per_iteration ) # Train collect_driver . run = common . function ( collect_driver . run , autograph = False ) tf_agent . train = common . function ( tf_agent . train , autograph = False ) while True : self . on_train_iteration_begin () self . log_api ( '-----' , f 'iteration {tc.iterations_done_in_training:4} of {tc.num_iterations:<4} -----' ) self . log_api ( 'collect_driver.run' , '()' ) collect_driver . run () self . log_api ( 'replay_buffer.gather_all' , '()' ) trajectories = replay_buffer . gather_all () self . log_api ( 'tf_agent.train' , '(experience=...)' ) loss_info = tf_agent . train ( experience = trajectories ) total_loss = loss_info . loss . numpy () actor_loss = loss_info . extra . policy_gradient_loss . numpy () critic_loss = loss_info . extra . value_estimation_loss . numpy () self . log_api ( '' , f 'loss={total_loss:<7.1f} [actor={actor_loss:<7.1f} critic={critic_loss:<7.1f}]' ) self . log_api ( 'replay_buffer.clear' , '()' ) replay_buffer . clear () self . on_train_iteration_end ( loss = total_loss , actor_loss = actor_loss , critic_loss = critic_loss ) if tc . training_done : break return # noinspection PyUnresolvedReferences class TfRandomAgent ( TfAgent ): \"\"\" creates a new random agent based on uniform random actions. Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. \"\"\" def __init__ ( self , model_config : core . ModelConfig ): super () . __init__ ( model_config = model_config ) self . _set_trained_policy () def _set_trained_policy ( self ): \"\"\"Tf-Agents Random Implementation of the train loop.\"\"\" self . log ( 'Creating environment...' ) train_env = self . _create_env () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () self . log_api ( 'RandomTFPolicy' , 'create' ) self . _trained_policy = random_tf_policy . RandomTFPolicy ( timestep_spec , action_spec ) self . _agent_context . _is_policy_trained = True def load_implementation ( self , directory : str ): \"\"\"NoOps implementation, since we don't save/load random policies.\"\"\" pass def save_implementation ( self , directory : str ): \"\"\"NoOps implementation, since we don't save/load random policies.\"\"\" pass # noinspection DuplicatedCode def train_implementation ( self , train_context : core . TrainContext ): self . log ( \"Training...\" ) train_env = self . _create_env () while True : self . on_train_iteration_begin () # ensure that 1 episode is played during the iteration time_step = train_env . reset () while not time_step . is_last (): action_step = self . _trained_policy . action ( time_step ) time_step = train_env . step ( action_step . action ) self . on_train_iteration_end ( math . nan ) if train_context . training_done : break return # noinspection PyUnresolvedReferences class TfReinforceAgent ( TfAgent ): \"\"\" creates a new agent based on the Reinforce algorithm using the tfagents implementation. Reinforce is a vanilla policy gradient algorithm using a single neural networks to predict the actions. Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. \"\"\" def __init__ ( self , model_config : core . ModelConfig ): super () . __init__ ( model_config = model_config ) # noinspection DuplicatedCode def train_implementation ( self , train_context : core . TrainContext ): \"\"\"Tf-Agents Reinforce Implementation of the train loop.\"\"\" assert isinstance ( train_context , core . EpisodesTrainContext ) tc : core . EpisodesTrainContext = train_context self . log ( 'Creating environment...' ) train_env = self . _create_env ( discount = tc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () # SetUp Optimizer, Networks and PpoAgent self . log_api ( 'AdamOptimizer' , 'create' ) optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ) self . log_api ( 'ActorDistributionNetwork' , 'create' ) actor_net = actor_distribution_network . ActorDistributionNetwork ( observation_spec , action_spec , fc_layer_params = self . model_config . fc_layers ) self . log_api ( 'ReinforceAgent' , 'create' ) tf_agent = reinforce_agent . ReinforceAgent ( timestep_spec , action_spec , actor_network = actor_net , optimizer = optimizer ) self . log_api ( 'tf_agent.initialize()' ) tf_agent . initialize () self . _trained_policy = tf_agent . policy # SetUp Data collection & Buffering collect_data_spec = tf_agent . collect_data_spec self . log_api ( 'TFUniformReplayBuffer' , 'create' ) replay_buffer = TFUniformReplayBuffer ( collect_data_spec , batch_size = 1 , max_length = tc . max_steps_in_buffer ) self . log_api ( 'DynamicEpisodeDriver' , 'create' ) collect_driver = DynamicEpisodeDriver ( train_env , tf_agent . collect_policy , observers = [ replay_buffer . add_batch ], num_episodes = tc . num_episodes_per_iteration ) # Train collect_driver . run = common . function ( collect_driver . run , autograph = False ) tf_agent . train = common . function ( tf_agent . train , autograph = False ) self . log ( 'Starting training...' ) while True : self . on_train_iteration_begin () msg = f 'iteration {tc.iterations_done_in_training:4} of {tc.num_iterations:<4}' self . log_api ( 'collect_driver.run' , msg ) collect_driver . run () self . log_api ( 'replay_buffer.gather_all' , msg ) trajectories = replay_buffer . gather_all () self . log_api ( 'tf_agent.train' , msg ) loss_info = tf_agent . train ( experience = trajectories ) total_loss = loss_info . loss . numpy () self . log_api ( '' , f 'loss={total_loss:<7.1f}' ) self . log_api ( 'replay_buffer.clear' , msg ) replay_buffer . clear () self . on_train_iteration_end ( loss = total_loss ) if tc . training_done : break return # noinspection PyUnresolvedReferences class TfSacAgent ( TfAgent ): \"\"\" creates a new agent based on the SAC algorithm using the tfagents implementation. adapted from https://github.com/tensorflow/agents/blob/master/tf_agents/colabs/7_SAC_minitaur_tutorial.ipynb Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. \"\"\" def __init__ ( self , model_config : core . ModelConfig ): super () . __init__ ( model_config = model_config ) # noinspection DuplicatedCode def train_implementation ( self , train_context : core . TrainContext ): \"\"\"Tf-Agents Ppo Implementation of the train loop.\"\"\" assert isinstance ( train_context , core . StepsTrainContext ) tc : core . StepsTrainContext = train_context train_env = self . _create_env ( discount = tc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () self . log_api ( 'CriticNetwork' , f '(observation_spec, action_spec), observation_fc_layer_params=None, ' + f 'action_fc_layer_params=None, joint_fc_layer_params={self.model_config.fc_layers})' ) critic_net = critic_network . CriticNetwork (( observation_spec , action_spec ), observation_fc_layer_params = None , action_fc_layer_params = None , joint_fc_layer_params = self . model_config . fc_layers ) def normal_projection_net ( action_spec_arg , init_means_output_factor = 0.1 ): return normal_projection_network . NormalProjectionNetwork ( action_spec_arg , mean_transform = None , state_dependent_std = True , init_means_output_factor = init_means_output_factor , std_transform = sac_agent . std_clip_transform , scale_distribution = True ) self . log_api ( 'ActorDistributionNetwork' , f 'observation_spec, action_spec, fc_layer_params={self.model_config.fc_layers}), ' + f 'continuous_projection_net=...)' ) actor_net = actor_distribution_network . ActorDistributionNetwork ( observation_spec , action_spec , fc_layer_params = self . model_config . fc_layers , continuous_projection_net = normal_projection_net ) # self.log_api('tf.compat.v1.train.get_or_create_global_step','()') # global_step = tf.compat.v1.train.get_or_create_global_step() self . log_api ( 'SacAgent' , f '(timestep_spec, action_spec, actor_network=..., critic_network=..., ' + f 'actor_optimizer=AdamOptimizer(learning_rate={tc.learning_rate}), ' + f 'critic_optimizer=AdamOptimizer(learning_rate={tc.learning_rate}), ' + f 'alpha_optimizer=AdamOptimizer(learning_rate={tc.learning_rate}), ' + f 'gamma={tc.reward_discount_gamma})' ) tf_agent = sac_agent . SacAgent ( timestep_spec , action_spec , actor_network = actor_net , critic_network = critic_net , actor_optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ), critic_optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ), alpha_optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ), # target_update_tau=0.005, # target_update_period=1, # td_errors_loss_fn=tf.compat.v1.losses.mean_squared_error, gamma = tc . reward_discount_gamma ) # reward_scale_factor=1.0, # gradient_clipping=None, # train_step_counter=global_step) self . log_api ( 'tf_agent.initialize' , '()' ) tf_agent . initialize () self . _trained_policy = greedy_policy . GreedyPolicy ( tf_agent . policy ) collect_policy = tf_agent . collect_policy # setup and preload replay buffer self . log_api ( 'TFUniformReplayBuffer' , f '(data_spec=tf_agent.collect_data_spec, ' + f 'batch_size={train_env.batch_size}, max_length={tc.max_steps_in_buffer})' ) replay_buffer = tf_uniform_replay_buffer . TFUniformReplayBuffer ( data_spec = tf_agent . collect_data_spec , batch_size = train_env . batch_size , max_length = tc . max_steps_in_buffer ) self . log_api ( 'DynamicStepDriver' , f '(env, collect_policy, observers=[replay_buffer.add_batch], ' + f 'num_steps={tc.num_steps_buffer_preload})' ) initial_collect_driver = dynamic_step_driver . DynamicStepDriver ( train_env , collect_policy , observers = [ replay_buffer . add_batch ], num_steps = tc . num_steps_buffer_preload ) self . log_api ( 'initial_collect_driver.run()' ) initial_collect_driver . run () # Dataset generates trajectories with shape [Bx2x...] dataset = replay_buffer . as_dataset ( num_parallel_calls = 3 , sample_batch_size = tc . num_steps_sampled_from_buffer , num_steps = 2 ) . prefetch ( 3 ) iterator = iter ( dataset ) self . log_api ( 'DynamicStepDriver' , f '(env, collect_policy, observers=[replay_buffer.add_batch], ' + f 'num_steps={tc.num_steps_per_iteration})' ) collect_driver = dynamic_step_driver . DynamicStepDriver ( train_env , collect_policy , observers = [ replay_buffer . add_batch ], num_steps = tc . num_steps_per_iteration ) # (Optional) Optimize by wrapping some of the code in a graph using TF function. tf_agent . train = common . function ( tf_agent . train ) collect_driver . run = common . function ( collect_driver . run ) self . log_api ( 'for each iteration' ) self . log_api ( ' collect_driver.run' , '()' ) self . log_api ( ' tf_agent.train' , '(experience=...)' ) while True : self . on_train_iteration_begin () # Collect a few steps using collect_policy and save to the replay buffer. for _ in range ( tc . num_steps_per_iteration ): collect_driver . run () # Sample a batch of data from the buffer and update the agent's network. experience , _ = next ( iterator ) loss_info = tf_agent . train ( experience ) total_loss = loss_info . loss . numpy () actor_loss = loss_info . extra . actor_loss alpha_loss = loss_info . extra . alpha_loss critic_loss = loss_info . extra . critic_loss self . on_train_iteration_end ( loss = total_loss , actor_loss = actor_loss , critic_loss = critic_loss , alpha_loss = alpha_loss ) if tc . training_done : break return class TfAgentAgentFactory ( bcore . BackendAgentFactory ): \"\"\"Backend for TfAgents. Serves as a factory to create algorithm specific wrappers for the TfAgents implementations. \"\"\" backend_name : str = 'tfagents' def get_algorithms ( self ) -> Dict [ Type , Type [ easyagents . backends . core . BackendAgent ]]: \"\"\"Yields a mapping of EasyAgent types to the implementations provided by this backend.\"\"\" return { easyagents . agents . DqnAgent : TfDqnAgent , easyagents . agents . PpoAgent : TfPpoAgent , easyagents . agents . RandomAgent : TfRandomAgent , easyagents . agents . ReinforceAgent : TfReinforceAgent , easyagents . agents . SacAgent : TfSacAgent }","title":"Module easyagents.backends.tfagents"},{"location":"reference/easyagents/backends/tfagents/#classes","text":"","title":"Classes"},{"location":"reference/easyagents/backends/tfagents/#tfagent","text":"class TfAgent ( model_config : easyagents . core . ModelConfig ) Reinforcement learning agents based on googles tf_agent implementations https://github.com/tensorflow/agents View Source class TfAgent ( bcore . BackendAgent , metaclass = ABCMeta ): \"\"\"Reinforcement learning agents based on googles tf_agent implementations https://github.com/tensorflow/agents \"\"\" def __init__ ( self , model_config: core . ModelConfig ): super (). __init__ ( model_config = model_config , backend_name = TfAgentAgentFactory . backend_name , tf_eager_execution = True ) self . _trained_policy = None self . _play_env: Optional [ gym . Env ] = None def _create_gym_with_wrapper ( self , discount ): gym_spec = gym . spec ( self . model_config . gym_env_name ) gym_env = gym_spec . make () # simplify_box_bounds: Whether to replace bounds of Box space that are arrays # with identical values with one number and rely on broadcasting. # important, simplify_box_bounds True crashes environments with boundaries with identical values env = gym_wrapper . GymWrapper ( gym_env , discount = discount , simplify_box_bounds = False ) return env def _create_env ( self , discount: float = 1 ) -> tf_py_environment . TFPyEnvironment: \"\"\" creates a new instance of the gym environment and wraps it in a tfagent TFPyEnvironment Args: discount: the reward discount factor \"\"\" assert 0 < discount <= 1 , \"discount not admissible\" self . log_api ( f'TFPyEnvironment' , f' ( suite_gym . load ( \"{self.model_config.original_env_name}\" , discount ={ discount }) ) ') # suit_gym.load crashes our environment # py_env = suite_gym.load(self.model_config.gym_env_name, discount=discount) py_env = self._create_gym_with_wrapper(discount) result = tf_py_environment.TFPyEnvironment(py_env) return result def _get_gym_env(self, tf_py_env: tf_py_environment.TFPyEnvironment) -> monitor._MonitorEnv: \"\"\" extracts the underlying _MonitorEnv from tf_py_env created by _create_tfagent_env\"\"\" assert isinstance(tf_py_env, tf_py_environment.TFPyEnvironment), \\ \"passed tf_py_env is not an instance of TFPyEnvironment\" assert isinstance(tf_py_env.pyenv, py_environment.PyEnvironment), \\ \"passed TFPyEnvironment.pyenv does not contain a PyEnvironment\" assert len(tf_py_env.pyenv.envs) == 1, \"passed TFPyEnvironment.pyenv does not contain a unique environment\" result = tf_py_env.pyenv.envs[0].gym assert isinstance(result, monitor._MonitorEnv), \"passed TFPyEnvironment does not contain a _MonitorEnv\" return result def play_implementation(self, play_context: core.PlayContext): \"\"\"Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used \"\"\" assert play_context, \"play_context not set.\" assert self._trained_policy, \"trained_policy not set. call train() first.\" if self._play_env is None: self._play_env = self._create_env() gym_env = self._get_gym_env(self._play_env) while True: self.on_play_episode_begin(env=gym_env) time_step = self._play_env.reset() while not time_step.is_last(): action_step = self._trained_policy.action(time_step) time_step = self._play_env.step(action_step.action) self.on_play_episode_end() if play_context.play_done: break def load_implementation(self, directory: str): \"\"\"Loads a previously saved actor policy from the directory Args: directory: the directory to load the policy from. \"\"\" assert directory self.log_api(' saved_model . load' , f' ({ directory }) ') self._trained_policy = tf.compat.v2.saved_model.load(directory) def save_implementation(self, directory: str): \"\"\"Saves the trained actor policy in directory. If no policy was trained yet, no file is written . Args: directory: the directory to save the policy weights to. \"\"\" assert self._trained_policy, \"no policy trained yet.\" self.log_api(' PolicySaver' , f' ( trained_policy , seed ={ self . model_config . seed }) ') saver = policy_saver.PolicySaver(self._trained_policy, seed=self.model_config.seed) self.log_api(' policy_saver . save' , f' ({ directory })') saver . save ( directory )","title":"TfAgent"},{"location":"reference/easyagents/backends/tfagents/#ancestors-in-mro","text":"easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/backends/tfagents/#descendants","text":"easyagents.backends.tfagents.TfDqnAgent easyagents.backends.tfagents.TfPpoAgent easyagents.backends.tfagents.TfRandomAgent easyagents.backends.tfagents.TfReinforceAgent easyagents.backends.tfagents.TfSacAgent","title":"Descendants"},{"location":"reference/easyagents/backends/tfagents/#methods","text":"","title":"Methods"},{"location":"reference/easyagents/backends/tfagents/#load","text":"def load ( self , directory : str , callbacks : List [ easyagents . core . AgentCallback ] ) Loads a previously trained and saved actor policy from directory. The loaded policy may afterwards be used by calling play(). Args: directory: the directory containing the trained policy callbacks: list of callbacks called during the load. View Source def load ( self , directory : str , callbacks : List [ core . AgentCallback ]): \"\"\"Loads a previously trained and saved actor policy from directory. The loaded policy may afterwards be used by calling play(). Args: directory: the directory containing the trained policy callbacks: list of callbacks called during the load. \"\"\" assert callbacks is not None , \"callbacks not set\" assert directory assert os . path . isdir ( directory ) self . _callbacks = callbacks self . load_implementation ( directory ) self . _agent_context . _is_policy_trained = True self . _callbacks = None","title":"load"},{"location":"reference/easyagents/backends/tfagents/#load_implementation","text":"def load_implementation ( self , directory : str ) Loads a previously saved actor policy from the directory Args: directory: the directory to load the policy from. View Source def load_implementation ( self , directory : str ): \"\"\"Loads a previously saved actor policy from the directory Args: directory: the directory to load the policy from. \"\"\" assert directory self . log_api ( 'saved_model.load' , f '({directory})' ) self . _trained_policy = tf . compat . v2 . saved_model . load ( directory )","title":"load_implementation"},{"location":"reference/easyagents/backends/tfagents/#log","text":"def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ): \"\"\"Logs msg.\"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg )","title":"log"},{"location":"reference/easyagents/backends/tfagents/#log_api","text":"def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\"Logs a call to api_target with additional log_msg.\"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg )","title":"log_api"},{"location":"reference/easyagents/backends/tfagents/#on_play_episode_begin","text":"def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ): \"\"\"Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. \"\"\" assert env , \"env not set.\" assert isinstance ( env , gym . core . Env ), \"env not an an instance of gym.Env.\" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context )","title":"on_play_episode_begin"},{"location":"reference/easyagents/backends/tfagents/#on_play_episode_end","text":"def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ): \"\"\"Must be called by play_implementation at the end of an episode\"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context )","title":"on_play_episode_end"},{"location":"reference/easyagents/backends/tfagents/#on_train_iteration_begin","text":"def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ): \"\"\"Must be called by train_implementation at the begining of a new iteration\"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context )","title":"on_train_iteration_begin"},{"location":"reference/easyagents/backends/tfagents/#on_train_iteration_end","text":"def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\"Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc.episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc.episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context )","title":"on_train_iteration_end"},{"location":"reference/easyagents/backends/tfagents/#play","text":"def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None","title":"play"},{"location":"reference/easyagents/backends/tfagents/#play_implementation","text":"def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : core . PlayContext ): \"\"\"Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used \"\"\" assert play_context , \"play_context not set.\" assert self . _trained_policy , \"trained_policy not set. call train() first.\" if self . _play_env is None : self . _play_env = self . _create_env () gym_env = self . _get_gym_env ( self . _play_env ) while True : self . on_play_episode_begin ( env = gym_env ) time_step = self . _play_env . reset () while not time_step . is_last (): action_step = self . _trained_policy . action ( time_step ) time_step = self . _play_env . step ( action_step . action ) self . on_play_episode_end () if play_context . play_done : break","title":"play_implementation"},{"location":"reference/easyagents/backends/tfagents/#save","text":"def save ( self , directory : str , callbacks : List [ easyagents . core . AgentCallback ] ) Saves the currently trained actor policy in directory. Only the actor policy is guaranteed to be saved. Thus after a call to load resuming training is not supported. Args: directory: the directory to save the policy weights to. the directory must exist. callbacks: list of callbacks called during policy load. View Source def save ( self , directory : str , callbacks : List [ core . AgentCallback ]): \"\"\"Saves the currently trained actor policy in directory. Only the actor policy is guaranteed to be saved. Thus after a call to load resuming training is not supported. Args: directory: the directory to save the policy weights to. the directory must exist. callbacks: list of callbacks called during policy load. \"\"\" assert callbacks is not None , \"callbacks not set\" assert directory assert os . path . isdir ( directory ) assert self . _agent_context . _is_policy_trained , \"No trained policy available.\" self . _callbacks = callbacks self . save_implementation ( directory ) self . _callbacks = None","title":"save"},{"location":"reference/easyagents/backends/tfagents/#save_implementation","text":"def save_implementation ( self , directory : str ) Saves the trained actor policy in directory. If no policy was trained yet, no file is written . Args: directory: the directory to save the policy weights to. View Source def save_implementation ( self , directory : str ): \"\"\"Saves the trained actor policy in directory. If no policy was trained yet, no file is written . Args: directory: the directory to save the policy weights to. \"\"\" assert self . _trained_policy , \"no policy trained yet.\" self . log_api ( 'PolicySaver' , f '(trained_policy,seed={self.model_config.seed})' ) saver = policy_saver . PolicySaver ( self . _trained_policy , seed = self . model_config . seed ) self . log_api ( 'policy_saver.save' , f '({directory})' ) saver . save ( directory )","title":"save_implementation"},{"location":"reference/easyagents/backends/tfagents/#train","text":"def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . _agent_context . _is_policy_trained = True self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None","title":"train"},{"location":"reference/easyagents/backends/tfagents/#train_implementation","text":"def train_implementation ( self , train_context : easyagents . core . TrainContext ) Agent specific implementation of the train loop. The implementation should have the form: while True: on_iteration_begin for e in num_episodes_per_iterations play episode and record steps (while steps_in_episode < max_steps_per_episode and) train policy for num_epochs_per_iteration epochs on_iteration_end( loss ) if training_done break Args: train_context: context configuring the train loop Hints: o the subclasses training loss is passed through to BackendAgent by on_iteration_end. Thus the subclass must not add the experienced loss to the TrainContext. View Source @abstractmethod def train_implementation ( self , train_context : core . TrainContext ) : \"\"\"Agent specific implementation of the train loop. The implementation should have the form: while True: on_iteration_begin for e in num_episodes_per_iterations play episode and record steps (while steps_in_episode < max_steps_per_episode and) train policy for num_epochs_per_iteration epochs on_iteration_end( loss ) if training_done break Args: train_context: context configuring the train loop Hints: o the subclasses training loss is passed through to BackendAgent by on_iteration_end. Thus the subclass must not add the experienced loss to the TrainContext. \"\"\"","title":"train_implementation"},{"location":"reference/easyagents/backends/tfagents/#tfagentagentfactory","text":"class TfAgentAgentFactory ( / , * args , ** kwargs ) Backend for TfAgents. Serves as a factory to create algorithm specific wrappers for the TfAgents implementations. View Source class TfAgentAgentFactory ( bcore . BackendAgentFactory ): \"\"\"Backend for TfAgents. Serves as a factory to create algorithm specific wrappers for the TfAgents implementations. \"\"\" backend_name: str = 'tfagents' def get_algorithms ( self ) -> Dict [ Type , Type [ easyagents . backends . core . BackendAgent ]]: \"\"\"Yields a mapping of EasyAgent types to the implementations provided by this backend.\"\"\" return { easyagents . agents . DqnAgent: TfDqnAgent , easyagents . agents . PpoAgent: TfPpoAgent , easyagents . agents . RandomAgent: TfRandomAgent , easyagents . agents . ReinforceAgent: TfReinforceAgent , easyagents . agents . SacAgent: TfSacAgent }","title":"TfAgentAgentFactory"},{"location":"reference/easyagents/backends/tfagents/#ancestors-in-mro_1","text":"easyagents.backends.core.BackendAgentFactory abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/backends/tfagents/#class-variables","text":"backend_name","title":"Class variables"},{"location":"reference/easyagents/backends/tfagents/#methods_1","text":"","title":"Methods"},{"location":"reference/easyagents/backends/tfagents/#create_agent","text":"def create_agent ( self , easyagent_type : Type , model_config : easyagents . core . ModelConfig ) -> Union [ easyagents . backends . core . _BackendAgent , NoneType ] Creates a backend agent instance implementing the algorithm given by agent_type. Args: easyagent_type: the EasyAgent derived type for which an implementing backend instance will be created model_config: the model_config passed to the constructor of the backend instance. Returns: instance of the agent or None if not implemented by this backend. View Source def create_agent ( self , easyagent_type : Type , model_config : core . ModelConfig ) \\ -> Optional [ _BackendAgent ] : \"\"\"Creates a backend agent instance implementing the algorithm given by agent_type. Args: easyagent_type: the EasyAgent derived type for which an implementing backend instance will be created model_config: the model_config passed to the constructor of the backend instance. Returns: instance of the agent or None if not implemented by this backend. \"\"\" result : Optional [ _BackendAgent ] = None algorithms = self . get_algorithms () if easyagent_type in algorithms : result = algorithms [ easyagent_type ] ( model_config = model_config ) return result","title":"create_agent"},{"location":"reference/easyagents/backends/tfagents/#get_algorithms","text":"def get_algorithms ( self ) -> Dict [ Type , Type [ easyagents . backends . core . BackendAgent ]] Yields a mapping of EasyAgent types to the implementations provided by this backend. View Source def get_algorithms ( self ) -> Dict [ Type , Type [ easyagents . backends . core . BackendAgent ]]: \"\"\"Yields a mapping of EasyAgent types to the implementations provided by this backend.\"\"\" return { easyagents . agents . DqnAgent : TfDqnAgent , easyagents . agents . PpoAgent : TfPpoAgent , easyagents . agents . RandomAgent : TfRandomAgent , easyagents . agents . ReinforceAgent : TfReinforceAgent , easyagents . agents . SacAgent : TfSacAgent }","title":"get_algorithms"},{"location":"reference/easyagents/backends/tfagents/#tfdqnagent","text":"class TfDqnAgent ( model_config : easyagents . core . ModelConfig ) creates a new agent based on the DQN algorithm using the tfagents implementation. Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. View Source class TfDqnAgent ( TfAgent ): \"\"\" creates a new agent based on the DQN algorithm using the tfagents implementation. Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. \"\"\" def __init__ ( self , model_config: core . ModelConfig ): super (). __init__ ( model_config = model_config ) def collect_step ( self , env: tf_py_environment . TFPyEnvironment , policy: tf_policy . Base , replay_buffer: TFUniformReplayBuffer ): time_step = env . current_time_step () action_step = policy . action ( time_step ) next_time_step = env . step ( action_step . action ) traj = trajectory . from_transition ( time_step , action_step , next_time_step ) replay_buffer . add_batch ( traj ) # noinspection DuplicatedCode def train_implementation ( self , train_context: core . TrainContext ): \"\"\"Tf-Agents Ppo Implementation of the train loop. The implementation follows https://colab.research.google.com/github/tensorflow/agents/blob/master/tf_agents/colabs/1_dqn_tutorial.ipynb \"\"\" assert isinstance ( train_context , core . StepsTrainContext ) dc: core . StepsTrainContext = train_context train_env = self . _create_env ( discount = dc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () # SetUp Optimizer, Networks and DqnAgent self . log_api ( 'AdamOptimizer' , f' ( learning_rate ={ dc . learning_rate }) ') optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=dc.learning_rate) self.log_api(' QNetwork' , f' ( observation_spec , action_spec , fc_layer_params ={ self . model_config . fc_layers }) ') q_net = q_network.QNetwork(observation_spec, action_spec, fc_layer_params=self.model_config.fc_layers) self.log_api(' DqnAgent' , '(timestep_spec,action_spec,q_network=..., optimizer=...,td_errors_loss_fn=common.element_wise_squared_loss)' ) tf_agent = dqn_agent . DqnAgent ( timestep_spec , action_spec , q_network=q_ net , optimizer = optimizer , td_errors_loss_fn = common . element_wise_squared_loss ) self . log_api ( 'tf_agent.initialize' , f' () ') tf_agent.initialize() self._trained_policy = tf_agent.policy # SetUp Data collection & Buffering self.log_api(' TFUniformReplayBuffer' , f' ( data_spec =..., batch_size ={ train_env . batch_size }, max_length ={ dc . max_steps_in_buffer }) ') replay_buffer = TFUniformReplayBuffer(data_spec=tf_agent.collect_data_spec, batch_size=train_env.batch_size, max_length=dc.max_steps_in_buffer) self.log_api(' RandomTFPolicy' , '()' ) random_policy = random_tf_policy . RandomTFPolicy ( timestep_spec , action_spec ) self . log_api ( 'replay_buffer.add_batch' , '(trajectory)' ) for _ in range ( dc . num_steps_buffer_preload ): self . collect_step ( env = train_env , policy = random_policy , replay_buffer = replay_buffer ) # Train self . log_api ( 'tf_agent.train' , '= common.function(tf_agent.train)' ) tf_agent . train = common . function ( tf_agent . train ) self . log_api ( 'replay_buffer.as_dataset' , f' ( num_parallel_calls = 3 , ' + f' sample_batch_size ={ dc . num_steps_sampled_from_buffer }, num_steps = 2 ). prefetch ( 3 ) ') dataset = replay_buffer.as_dataset(num_parallel_calls=3, sample_batch_size=dc.num_steps_sampled_from_buffer, num_steps=2).prefetch(3) self.log_api(' iter ( dataset' , f' { iter ( dataset )} ') iter_dataset = iter(dataset) self.log_api(' for each iteration' ) self . log_api ( ' replay_buffer.add_batch' , '(trajectory)' ) self . log_api ( ' tf_agent.train' , '(experience=trajectory)' ) while True: self . on_train_iteration_begin () for _ in range ( dc . num_steps_per_iteration ): self . collect_step ( env = train_env , policy = tf_agent . collect_policy , replay_buffer = replay_buffer ) trajectories , _ = next ( iter_dataset ) tf_loss_info = tf_agent . train ( experience = trajectories ) self . on_train_iteration_end ( tf_loss_info . loss ) if train_context . training_done: break return","title":"TfDqnAgent"},{"location":"reference/easyagents/backends/tfagents/#ancestors-in-mro_2","text":"easyagents.backends.tfagents.TfAgent easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/backends/tfagents/#methods_2","text":"","title":"Methods"},{"location":"reference/easyagents/backends/tfagents/#collect_step","text":"def collect_step ( self , env : tf_agents . environments . tf_py_environment . TFPyEnvironment , policy : tf_agents . policies . tf_policy . Base , replay_buffer : tf_agents . replay_buffers . tf_uniform_replay_buffer . TFUniformReplayBuffer ) View Source def collect_step ( self , env : tf_py_environment . TFPyEnvironment , policy : tf_policy . Base , replay_buffer : TFUniformReplayBuffer ): time_step = env . current_time_step () action_step = policy . action ( time_step ) next_time_step = env . step ( action_step . action ) traj = trajectory . from_transition ( time_step , action_step , next_time_step ) replay_buffer . add_batch ( traj )","title":"collect_step"},{"location":"reference/easyagents/backends/tfagents/#load_1","text":"def load ( self , directory : str , callbacks : List [ easyagents . core . AgentCallback ] ) Loads a previously trained and saved actor policy from directory. The loaded policy may afterwards be used by calling play(). Args: directory: the directory containing the trained policy callbacks: list of callbacks called during the load. View Source def load ( self , directory : str , callbacks : List [ core . AgentCallback ]): \"\"\"Loads a previously trained and saved actor policy from directory. The loaded policy may afterwards be used by calling play(). Args: directory: the directory containing the trained policy callbacks: list of callbacks called during the load. \"\"\" assert callbacks is not None , \"callbacks not set\" assert directory assert os . path . isdir ( directory ) self . _callbacks = callbacks self . load_implementation ( directory ) self . _agent_context . _is_policy_trained = True self . _callbacks = None","title":"load"},{"location":"reference/easyagents/backends/tfagents/#load_implementation_1","text":"def load_implementation ( self , directory : str ) Loads a previously saved actor policy from the directory Args: directory: the directory to load the policy from. View Source def load_implementation ( self , directory : str ): \"\"\"Loads a previously saved actor policy from the directory Args: directory: the directory to load the policy from. \"\"\" assert directory self . log_api ( 'saved_model.load' , f '({directory})' ) self . _trained_policy = tf . compat . v2 . saved_model . load ( directory )","title":"load_implementation"},{"location":"reference/easyagents/backends/tfagents/#log_1","text":"def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ): \"\"\"Logs msg.\"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg )","title":"log"},{"location":"reference/easyagents/backends/tfagents/#log_api_1","text":"def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\"Logs a call to api_target with additional log_msg.\"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg )","title":"log_api"},{"location":"reference/easyagents/backends/tfagents/#on_play_episode_begin_1","text":"def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ): \"\"\"Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. \"\"\" assert env , \"env not set.\" assert isinstance ( env , gym . core . Env ), \"env not an an instance of gym.Env.\" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context )","title":"on_play_episode_begin"},{"location":"reference/easyagents/backends/tfagents/#on_play_episode_end_1","text":"def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ): \"\"\"Must be called by play_implementation at the end of an episode\"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context )","title":"on_play_episode_end"},{"location":"reference/easyagents/backends/tfagents/#on_train_iteration_begin_1","text":"def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ): \"\"\"Must be called by train_implementation at the begining of a new iteration\"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context )","title":"on_train_iteration_begin"},{"location":"reference/easyagents/backends/tfagents/#on_train_iteration_end_1","text":"def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\"Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc.episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc.episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context )","title":"on_train_iteration_end"},{"location":"reference/easyagents/backends/tfagents/#play_1","text":"def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None","title":"play"},{"location":"reference/easyagents/backends/tfagents/#play_implementation_1","text":"def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : core . PlayContext ): \"\"\"Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used \"\"\" assert play_context , \"play_context not set.\" assert self . _trained_policy , \"trained_policy not set. call train() first.\" if self . _play_env is None : self . _play_env = self . _create_env () gym_env = self . _get_gym_env ( self . _play_env ) while True : self . on_play_episode_begin ( env = gym_env ) time_step = self . _play_env . reset () while not time_step . is_last (): action_step = self . _trained_policy . action ( time_step ) time_step = self . _play_env . step ( action_step . action ) self . on_play_episode_end () if play_context . play_done : break","title":"play_implementation"},{"location":"reference/easyagents/backends/tfagents/#save_1","text":"def save ( self , directory : str , callbacks : List [ easyagents . core . AgentCallback ] ) Saves the currently trained actor policy in directory. Only the actor policy is guaranteed to be saved. Thus after a call to load resuming training is not supported. Args: directory: the directory to save the policy weights to. the directory must exist. callbacks: list of callbacks called during policy load. View Source def save ( self , directory : str , callbacks : List [ core . AgentCallback ]): \"\"\"Saves the currently trained actor policy in directory. Only the actor policy is guaranteed to be saved. Thus after a call to load resuming training is not supported. Args: directory: the directory to save the policy weights to. the directory must exist. callbacks: list of callbacks called during policy load. \"\"\" assert callbacks is not None , \"callbacks not set\" assert directory assert os . path . isdir ( directory ) assert self . _agent_context . _is_policy_trained , \"No trained policy available.\" self . _callbacks = callbacks self . save_implementation ( directory ) self . _callbacks = None","title":"save"},{"location":"reference/easyagents/backends/tfagents/#save_implementation_1","text":"def save_implementation ( self , directory : str ) Saves the trained actor policy in directory. If no policy was trained yet, no file is written . Args: directory: the directory to save the policy weights to. View Source def save_implementation ( self , directory : str ): \"\"\"Saves the trained actor policy in directory. If no policy was trained yet, no file is written . Args: directory: the directory to save the policy weights to. \"\"\" assert self . _trained_policy , \"no policy trained yet.\" self . log_api ( 'PolicySaver' , f '(trained_policy,seed={self.model_config.seed})' ) saver = policy_saver . PolicySaver ( self . _trained_policy , seed = self . model_config . seed ) self . log_api ( 'policy_saver.save' , f '({directory})' ) saver . save ( directory )","title":"save_implementation"},{"location":"reference/easyagents/backends/tfagents/#train_1","text":"def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . _agent_context . _is_policy_trained = True self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None","title":"train"},{"location":"reference/easyagents/backends/tfagents/#train_implementation_1","text":"def train_implementation ( self , train_context : easyagents . core . TrainContext ) Tf-Agents Ppo Implementation of the train loop. The implementation follows https://colab.research.google.com/github/tensorflow/agents/blob/master/tf_agents/colabs/1_dqn_tutorial.ipynb View Source def train_implementation ( self , train_context : core . TrainContext ): \"\"\"Tf-Agents Ppo Implementation of the train loop. The implementation follows https://colab.research.google.com/github/tensorflow/agents/blob/master/tf_agents/colabs/1_dqn_tutorial.ipynb \"\"\" assert isinstance ( train_context , core . StepsTrainContext ) dc : core . StepsTrainContext = train_context train_env = self . _create_env ( discount = dc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () # SetUp Optimizer , Networks and DqnAgent self . log_api ( 'AdamOptimizer' , f '(learning_rate={dc.learning_rate})' ) optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = dc . learning_rate ) self . log_api ( 'QNetwork' , f '(observation_spec, action_spec, fc_layer_params={self.model_config.fc_layers})' ) q_net = q_network . QNetwork ( observation_spec , action_spec , fc_layer_params = self . model_config . fc_layers ) self . log_api ( 'DqnAgent' , '(timestep_spec,action_spec,q_network=..., optimizer=...,td_errors_loss_fn=common.element_wise_squared_loss)' ) tf_agent = dqn_agent . DqnAgent ( timestep_spec , action_spec , q_network = q_net , optimizer = optimizer , td_errors_loss_fn = common . element_wise_squared_loss ) self . log_api ( 'tf_agent.initialize' , f '()' ) tf_agent . initialize () self . _trained_policy = tf_agent . policy # SetUp Data collection & Buffering self . log_api ( 'TFUniformReplayBuffer' , f '(data_spec=..., batch_size={train_env.batch_size}, max_length={dc.max_steps_in_buffer})' ) replay_buffer = TFUniformReplayBuffer ( data_spec = tf_agent . collect_data_spec , batch_size = train_env . batch_size , max_length = dc . max_steps_in_buffer ) self . log_api ( 'RandomTFPolicy' , '()' ) random_policy = random_tf_policy . RandomTFPolicy ( timestep_spec , action_spec ) self . log_api ( 'replay_buffer.add_batch' , '(trajectory)' ) for _ in range ( dc . num_steps_buffer_preload ): self . collect_step ( env = train_env , policy = random_policy , replay_buffer = replay_buffer ) # Train self . log_api ( 'tf_agent.train' , '= common.function(tf_agent.train)' ) tf_agent . train = common . function ( tf_agent . train ) self . log_api ( 'replay_buffer.as_dataset' , f '(num_parallel_calls=3, ' + f 'sample_batch_size={dc.num_steps_sampled_from_buffer}, num_steps=2).prefetch(3)' ) dataset = replay_buffer . as_dataset ( num_parallel_calls = 3 , sample_batch_size = dc . num_steps_sampled_from_buffer , num_steps = 2 ). prefetch ( 3 ) self . log_api ( 'iter(dataset' , f '{iter(dataset)}' ) iter_dataset = iter ( dataset ) self . log_api ( 'for each iteration' ) self . log_api ( ' replay_buffer.add_batch' , '(trajectory)' ) self . log_api ( ' tf_agent.train' , '(experience=trajectory)' ) while True : self . on_train_iteration_begin () for _ in range ( dc . num_steps_per_iteration ): self . collect_step ( env = train_env , policy = tf_agent . collect_policy , replay_buffer = replay_buffer ) trajectories , _ = next ( iter_dataset ) tf_loss_info = tf_agent . train ( experience = trajectories ) self . on_train_iteration_end ( tf_loss_info . loss ) if train_context . training_done : break return","title":"train_implementation"},{"location":"reference/easyagents/backends/tfagents/#tfppoagent","text":"class TfPpoAgent ( model_config : easyagents . core . ModelConfig ) creates a new agent based on the PPO algorithm using the tfagents implementation. PPO is an actor-critic algorithm using 2 neural networks. The actor network to predict the next action to be taken and the critic network to estimate the value of the game state we are currently in (the expected, discounted sum of future rewards when following the current actor network). Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. View Source class TfPpoAgent ( TfAgent ): \"\"\" creates a new agent based on the PPO algorithm using the tfagents implementation. PPO is an actor-critic algorithm using 2 neural networks. The actor network to predict the next action to be taken and the critic network to estimate the value of the game state we are currently in (the expected, discounted sum of future rewards when following the current actor network). Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. \"\"\" def __init__ ( self , model_config: core . ModelConfig ): super (). __init__ ( model_config = model_config ) # noinspection DuplicatedCode def train_implementation ( self , train_context: core . TrainContext ): \"\"\"Tf-Agents Ppo Implementation of the train loop.\"\"\" assert isinstance ( train_context , core . PpoTrainContext ) tc: core . PpoTrainContext = train_context train_env = self . _create_env ( discount = tc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () # SetUp Optimizer, Networks and PpoAgent self . log_api ( 'AdamOptimizer' , '()' ) optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ) self . log_api ( 'ActorDistributionNetwork' , '()' ) actor_net = actor_distribution_network . ActorDistributionNetwork ( observation_spec , action_spec , fc_layer_params = self . model_config . fc_layers ) self . log_api ( 'ValueNetwork' , '()' ) value_net = value_network . ValueNetwork ( observation_spec , fc_layer_params = self . model_config . fc_layers ) self . log_api ( 'PpoAgent' , '()' ) tf_agent = ppo_agent . PPOAgent ( timestep_spec , action_spec , optimizer , actor_net = actor_net , value_net = value_net , num_epochs = tc . num_epochs_per_iteration ) self . log_api ( 'tf_agent.initialize' , '()' ) tf_agent . initialize () self . _trained_policy = tf_agent . policy # SetUp Data collection & Buffering collect_data_spec = tf_agent . collect_data_spec self . log_api ( 'TFUniformReplayBuffer' , '()' ) replay_buffer = TFUniformReplayBuffer ( collect_data_spec , batch_size = 1 , max_length = tc . max_steps_in_buffer ) collect_policy = tf_agent . collect_policy self . log_api ( 'DynamicEpisodeDriver' , '()' ) collect_driver = DynamicEpisodeDriver ( train_env , collect_policy , observers =[ replay_buffer . add_batch ], num_episodes = tc . num_episodes_per_iteration ) # Train collect_driver . run = common . function ( collect_driver . run , autograph = False ) tf_agent . train = common . function ( tf_agent . train , autograph = False ) while True: self . on_train_iteration_begin () self . log_api ( '-----' , f'iteration { tc . iterations_done_in_training:4 } of { tc . num_iterations: < 4 } ----- ') self.log_api(' collect_driver . run' , '()' ) collect_driver . run () self . log_api ( 'replay_buffer.gather_all' , '()' ) trajectories = replay_buffer . gather_all () self . log_api ( 'tf_agent.train' , '(experience=...)' ) loss_info = tf_agent . train ( experience = trajectories ) total_loss = loss_info . loss . numpy () actor_loss = loss_info . extra . policy_gradient_loss . numpy () critic_loss = loss_info . extra . value_estimation_loss . numpy () self . log_api ( '' , f'loss ={ total_loss: < 7.1 f } [ actor ={ actor_loss: < 7.1 f } critic ={ critic_loss: < 7.1 f }] ') self.log_api(' replay_buffer . clear' , '()' ) replay_buffer . clear () self . on_train_iteration_end ( loss = total_loss , actor_loss = actor_loss , critic_loss = critic_loss ) if tc . training_done: break return","title":"TfPpoAgent"},{"location":"reference/easyagents/backends/tfagents/#ancestors-in-mro_3","text":"easyagents.backends.tfagents.TfAgent easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/backends/tfagents/#methods_3","text":"","title":"Methods"},{"location":"reference/easyagents/backends/tfagents/#load_2","text":"def load ( self , directory : str , callbacks : List [ easyagents . core . AgentCallback ] ) Loads a previously trained and saved actor policy from directory. The loaded policy may afterwards be used by calling play(). Args: directory: the directory containing the trained policy callbacks: list of callbacks called during the load. View Source def load ( self , directory : str , callbacks : List [ core . AgentCallback ]): \"\"\"Loads a previously trained and saved actor policy from directory. The loaded policy may afterwards be used by calling play(). Args: directory: the directory containing the trained policy callbacks: list of callbacks called during the load. \"\"\" assert callbacks is not None , \"callbacks not set\" assert directory assert os . path . isdir ( directory ) self . _callbacks = callbacks self . load_implementation ( directory ) self . _agent_context . _is_policy_trained = True self . _callbacks = None","title":"load"},{"location":"reference/easyagents/backends/tfagents/#load_implementation_2","text":"def load_implementation ( self , directory : str ) Loads a previously saved actor policy from the directory Args: directory: the directory to load the policy from. View Source def load_implementation ( self , directory : str ): \"\"\"Loads a previously saved actor policy from the directory Args: directory: the directory to load the policy from. \"\"\" assert directory self . log_api ( 'saved_model.load' , f '({directory})' ) self . _trained_policy = tf . compat . v2 . saved_model . load ( directory )","title":"load_implementation"},{"location":"reference/easyagents/backends/tfagents/#log_2","text":"def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ): \"\"\"Logs msg.\"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg )","title":"log"},{"location":"reference/easyagents/backends/tfagents/#log_api_2","text":"def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\"Logs a call to api_target with additional log_msg.\"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg )","title":"log_api"},{"location":"reference/easyagents/backends/tfagents/#on_play_episode_begin_2","text":"def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ): \"\"\"Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. \"\"\" assert env , \"env not set.\" assert isinstance ( env , gym . core . Env ), \"env not an an instance of gym.Env.\" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context )","title":"on_play_episode_begin"},{"location":"reference/easyagents/backends/tfagents/#on_play_episode_end_2","text":"def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ): \"\"\"Must be called by play_implementation at the end of an episode\"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context )","title":"on_play_episode_end"},{"location":"reference/easyagents/backends/tfagents/#on_train_iteration_begin_2","text":"def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ): \"\"\"Must be called by train_implementation at the begining of a new iteration\"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context )","title":"on_train_iteration_begin"},{"location":"reference/easyagents/backends/tfagents/#on_train_iteration_end_2","text":"def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\"Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc.episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc.episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context )","title":"on_train_iteration_end"},{"location":"reference/easyagents/backends/tfagents/#play_2","text":"def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None","title":"play"},{"location":"reference/easyagents/backends/tfagents/#play_implementation_2","text":"def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : core . PlayContext ): \"\"\"Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used \"\"\" assert play_context , \"play_context not set.\" assert self . _trained_policy , \"trained_policy not set. call train() first.\" if self . _play_env is None : self . _play_env = self . _create_env () gym_env = self . _get_gym_env ( self . _play_env ) while True : self . on_play_episode_begin ( env = gym_env ) time_step = self . _play_env . reset () while not time_step . is_last (): action_step = self . _trained_policy . action ( time_step ) time_step = self . _play_env . step ( action_step . action ) self . on_play_episode_end () if play_context . play_done : break","title":"play_implementation"},{"location":"reference/easyagents/backends/tfagents/#save_2","text":"def save ( self , directory : str , callbacks : List [ easyagents . core . AgentCallback ] ) Saves the currently trained actor policy in directory. Only the actor policy is guaranteed to be saved. Thus after a call to load resuming training is not supported. Args: directory: the directory to save the policy weights to. the directory must exist. callbacks: list of callbacks called during policy load. View Source def save ( self , directory : str , callbacks : List [ core . AgentCallback ]): \"\"\"Saves the currently trained actor policy in directory. Only the actor policy is guaranteed to be saved. Thus after a call to load resuming training is not supported. Args: directory: the directory to save the policy weights to. the directory must exist. callbacks: list of callbacks called during policy load. \"\"\" assert callbacks is not None , \"callbacks not set\" assert directory assert os . path . isdir ( directory ) assert self . _agent_context . _is_policy_trained , \"No trained policy available.\" self . _callbacks = callbacks self . save_implementation ( directory ) self . _callbacks = None","title":"save"},{"location":"reference/easyagents/backends/tfagents/#save_implementation_2","text":"def save_implementation ( self , directory : str ) Saves the trained actor policy in directory. If no policy was trained yet, no file is written . Args: directory: the directory to save the policy weights to. View Source def save_implementation ( self , directory : str ): \"\"\"Saves the trained actor policy in directory. If no policy was trained yet, no file is written . Args: directory: the directory to save the policy weights to. \"\"\" assert self . _trained_policy , \"no policy trained yet.\" self . log_api ( 'PolicySaver' , f '(trained_policy,seed={self.model_config.seed})' ) saver = policy_saver . PolicySaver ( self . _trained_policy , seed = self . model_config . seed ) self . log_api ( 'policy_saver.save' , f '({directory})' ) saver . save ( directory )","title":"save_implementation"},{"location":"reference/easyagents/backends/tfagents/#train_2","text":"def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . _agent_context . _is_policy_trained = True self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None","title":"train"},{"location":"reference/easyagents/backends/tfagents/#train_implementation_2","text":"def train_implementation ( self , train_context : easyagents . core . TrainContext ) Tf-Agents Ppo Implementation of the train loop. View Source def train_implementation ( self , train_context : core . TrainContext ): \"\"\"Tf-Agents Ppo Implementation of the train loop.\"\"\" assert isinstance ( train_context , core . PpoTrainContext ) tc : core . PpoTrainContext = train_context train_env = self . _create_env ( discount = tc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () # SetUp Optimizer , Networks and PpoAgent self . log_api ( 'AdamOptimizer' , '()' ) optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ) self . log_api ( 'ActorDistributionNetwork' , '()' ) actor_net = actor_distribution_network . ActorDistributionNetwork ( observation_spec , action_spec , fc_layer_params = self . model_config . fc_layers ) self . log_api ( 'ValueNetwork' , '()' ) value_net = value_network . ValueNetwork ( observation_spec , fc_layer_params = self . model_config . fc_layers ) self . log_api ( 'PpoAgent' , '()' ) tf_agent = ppo_agent . PPOAgent ( timestep_spec , action_spec , optimizer , actor_net = actor_net , value_net = value_net , num_epochs = tc . num_epochs_per_iteration ) self . log_api ( 'tf_agent.initialize' , '()' ) tf_agent . initialize () self . _trained_policy = tf_agent . policy # SetUp Data collection & Buffering collect_data_spec = tf_agent . collect_data_spec self . log_api ( 'TFUniformReplayBuffer' , '()' ) replay_buffer = TFUniformReplayBuffer ( collect_data_spec , batch_size = 1 , max_length = tc . max_steps_in_buffer ) collect_policy = tf_agent . collect_policy self . log_api ( 'DynamicEpisodeDriver' , '()' ) collect_driver = DynamicEpisodeDriver ( train_env , collect_policy , observers = [ replay_buffer . add_batch ], num_episodes = tc . num_episodes_per_iteration ) # Train collect_driver . run = common . function ( collect_driver . run , autograph = False ) tf_agent . train = common . function ( tf_agent . train , autograph = False ) while True : self . on_train_iteration_begin () self . log_api ( '-----' , f 'iteration {tc.iterations_done_in_training:4} of {tc.num_iterations:<4} -----' ) self . log_api ( 'collect_driver.run' , '()' ) collect_driver . run () self . log_api ( 'replay_buffer.gather_all' , '()' ) trajectories = replay_buffer . gather_all () self . log_api ( 'tf_agent.train' , '(experience=...)' ) loss_info = tf_agent . train ( experience = trajectories ) total_loss = loss_info . loss . numpy () actor_loss = loss_info . extra . policy_gradient_loss . numpy () critic_loss = loss_info . extra . value_estimation_loss . numpy () self . log_api ( '' , f 'loss={total_loss:<7.1f} [actor={actor_loss:<7.1f} critic={critic_loss:<7.1f}]' ) self . log_api ( 'replay_buffer.clear' , '()' ) replay_buffer . clear () self . on_train_iteration_end ( loss = total_loss , actor_loss = actor_loss , critic_loss = critic_loss ) if tc . training_done : break return","title":"train_implementation"},{"location":"reference/easyagents/backends/tfagents/#tfrandomagent","text":"class TfRandomAgent ( model_config : easyagents . core . ModelConfig ) creates a new random agent based on uniform random actions. Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. View Source class TfRandomAgent ( TfAgent ): \"\"\" creates a new random agent based on uniform random actions. Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. \"\"\" def __init__ ( self , model_config: core . ModelConfig ): super (). __init__ ( model_config = model_config ) self . _set_trained_policy () def _set_trained_policy ( self ): \"\"\"Tf-Agents Random Implementation of the train loop.\"\"\" self . log ( 'Creating environment...' ) train_env = self . _create_env () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () self . log_api ( 'RandomTFPolicy' , 'create' ) self . _trained_policy = random_tf_policy . RandomTFPolicy ( timestep_spec , action_spec ) self . _agent_context . _is_policy_trained = True def load_implementation ( self , directory: str ): \"\"\"NoOps implementation, since we don't save/load random policies.\"\"\" pass def save_implementation ( self , directory: str ): \"\"\"NoOps implementation, since we don't save/load random policies.\"\"\" pass # noinspection DuplicatedCode def train_implementation ( self , train_context: core . TrainContext ): self . log ( \"Training...\" ) train_env = self . _create_env () while True: self . on_train_iteration_begin () # ensure that 1 episode is played during the iteration time_step = train_env . reset () while not time_step . is_last (): action_step = self . _trained_policy . action ( time_step ) time_step = train_env . step ( action_step . action ) self . on_train_iteration_end ( math . nan ) if train_context . training_done: break return","title":"TfRandomAgent"},{"location":"reference/easyagents/backends/tfagents/#ancestors-in-mro_4","text":"easyagents.backends.tfagents.TfAgent easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/backends/tfagents/#methods_4","text":"","title":"Methods"},{"location":"reference/easyagents/backends/tfagents/#load_3","text":"def load ( self , directory : str , callbacks : List [ easyagents . core . AgentCallback ] ) Loads a previously trained and saved actor policy from directory. The loaded policy may afterwards be used by calling play(). Args: directory: the directory containing the trained policy callbacks: list of callbacks called during the load. View Source def load ( self , directory : str , callbacks : List [ core . AgentCallback ]): \"\"\"Loads a previously trained and saved actor policy from directory. The loaded policy may afterwards be used by calling play(). Args: directory: the directory containing the trained policy callbacks: list of callbacks called during the load. \"\"\" assert callbacks is not None , \"callbacks not set\" assert directory assert os . path . isdir ( directory ) self . _callbacks = callbacks self . load_implementation ( directory ) self . _agent_context . _is_policy_trained = True self . _callbacks = None","title":"load"},{"location":"reference/easyagents/backends/tfagents/#load_implementation_3","text":"def load_implementation ( self , directory : str ) NoOps implementation, since we don't save/load random policies. View Source def load_implementation ( self , directory : str ): \"\"\"NoOps implementation, since we don't save/load random policies.\"\"\" pass","title":"load_implementation"},{"location":"reference/easyagents/backends/tfagents/#log_3","text":"def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ): \"\"\"Logs msg.\"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg )","title":"log"},{"location":"reference/easyagents/backends/tfagents/#log_api_3","text":"def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\"Logs a call to api_target with additional log_msg.\"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg )","title":"log_api"},{"location":"reference/easyagents/backends/tfagents/#on_play_episode_begin_3","text":"def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ): \"\"\"Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. \"\"\" assert env , \"env not set.\" assert isinstance ( env , gym . core . Env ), \"env not an an instance of gym.Env.\" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context )","title":"on_play_episode_begin"},{"location":"reference/easyagents/backends/tfagents/#on_play_episode_end_3","text":"def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ): \"\"\"Must be called by play_implementation at the end of an episode\"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context )","title":"on_play_episode_end"},{"location":"reference/easyagents/backends/tfagents/#on_train_iteration_begin_3","text":"def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ): \"\"\"Must be called by train_implementation at the begining of a new iteration\"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context )","title":"on_train_iteration_begin"},{"location":"reference/easyagents/backends/tfagents/#on_train_iteration_end_3","text":"def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\"Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc.episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc.episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context )","title":"on_train_iteration_end"},{"location":"reference/easyagents/backends/tfagents/#play_3","text":"def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None","title":"play"},{"location":"reference/easyagents/backends/tfagents/#play_implementation_3","text":"def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : core . PlayContext ): \"\"\"Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used \"\"\" assert play_context , \"play_context not set.\" assert self . _trained_policy , \"trained_policy not set. call train() first.\" if self . _play_env is None : self . _play_env = self . _create_env () gym_env = self . _get_gym_env ( self . _play_env ) while True : self . on_play_episode_begin ( env = gym_env ) time_step = self . _play_env . reset () while not time_step . is_last (): action_step = self . _trained_policy . action ( time_step ) time_step = self . _play_env . step ( action_step . action ) self . on_play_episode_end () if play_context . play_done : break","title":"play_implementation"},{"location":"reference/easyagents/backends/tfagents/#save_3","text":"def save ( self , directory : str , callbacks : List [ easyagents . core . AgentCallback ] ) Saves the currently trained actor policy in directory. Only the actor policy is guaranteed to be saved. Thus after a call to load resuming training is not supported. Args: directory: the directory to save the policy weights to. the directory must exist. callbacks: list of callbacks called during policy load. View Source def save ( self , directory : str , callbacks : List [ core . AgentCallback ]): \"\"\"Saves the currently trained actor policy in directory. Only the actor policy is guaranteed to be saved. Thus after a call to load resuming training is not supported. Args: directory: the directory to save the policy weights to. the directory must exist. callbacks: list of callbacks called during policy load. \"\"\" assert callbacks is not None , \"callbacks not set\" assert directory assert os . path . isdir ( directory ) assert self . _agent_context . _is_policy_trained , \"No trained policy available.\" self . _callbacks = callbacks self . save_implementation ( directory ) self . _callbacks = None","title":"save"},{"location":"reference/easyagents/backends/tfagents/#save_implementation_3","text":"def save_implementation ( self , directory : str ) NoOps implementation, since we don't save/load random policies. View Source def save_implementation ( self , directory : str ): \"\"\"NoOps implementation, since we don't save/load random policies.\"\"\" pass","title":"save_implementation"},{"location":"reference/easyagents/backends/tfagents/#train_3","text":"def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . _agent_context . _is_policy_trained = True self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None","title":"train"},{"location":"reference/easyagents/backends/tfagents/#train_implementation_3","text":"def train_implementation ( self , train_context : easyagents . core . TrainContext ) Agent specific implementation of the train loop. The implementation should have the form: while True: on_iteration_begin for e in num_episodes_per_iterations play episode and record steps (while steps_in_episode < max_steps_per_episode and) train policy for num_epochs_per_iteration epochs on_iteration_end( loss ) if training_done break Args: train_context: context configuring the train loop Hints: o the subclasses training loss is passed through to BackendAgent by on_iteration_end. Thus the subclass must not add the experienced loss to the TrainContext. View Source def train_implementation ( self , train_context : core . TrainContext ): self . log ( \"Training...\" ) train_env = self . _create_env () while True : self . on_train_iteration_begin () # ensure that 1 episode is played during the iteration time_step = train_env . reset () while not time_step . is_last (): action_step = self . _trained_policy . action ( time_step ) time_step = train_env . step ( action_step . action ) self . on_train_iteration_end ( math . nan ) if train_context . training_done : break return","title":"train_implementation"},{"location":"reference/easyagents/backends/tfagents/#tfreinforceagent","text":"class TfReinforceAgent ( model_config : easyagents . core . ModelConfig ) creates a new agent based on the Reinforce algorithm using the tfagents implementation. Reinforce is a vanilla policy gradient algorithm using a single neural networks to predict the actions. Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. View Source class TfReinforceAgent ( TfAgent ): \"\"\" creates a new agent based on the Reinforce algorithm using the tfagents implementation. Reinforce is a vanilla policy gradient algorithm using a single neural networks to predict the actions. Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. \"\"\" def __init__ ( self , model_config: core . ModelConfig ): super (). __init__ ( model_config = model_config ) # noinspection DuplicatedCode def train_implementation ( self , train_context: core . TrainContext ): \"\"\"Tf-Agents Reinforce Implementation of the train loop.\"\"\" assert isinstance ( train_context , core . EpisodesTrainContext ) tc: core . EpisodesTrainContext = train_context self . log ( 'Creating environment...' ) train_env = self . _create_env ( discount = tc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () # SetUp Optimizer, Networks and PpoAgent self . log_api ( 'AdamOptimizer' , 'create' ) optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ) self . log_api ( 'ActorDistributionNetwork' , 'create' ) actor_net = actor_distribution_network . ActorDistributionNetwork ( observation_spec , action_spec , fc_layer_params = self . model_config . fc_layers ) self . log_api ( 'ReinforceAgent' , 'create' ) tf_agent = reinforce_agent . ReinforceAgent ( timestep_spec , action_spec , actor_network = actor_net , optimizer = optimizer ) self . log_api ( 'tf_agent.initialize()' ) tf_agent . initialize () self . _trained_policy = tf_agent . policy # SetUp Data collection & Buffering collect_data_spec = tf_agent . collect_data_spec self . log_api ( 'TFUniformReplayBuffer' , 'create' ) replay_buffer = TFUniformReplayBuffer ( collect_data_spec , batch_size = 1 , max_length = tc . max_steps_in_buffer ) self . log_api ( 'DynamicEpisodeDriver' , 'create' ) collect_driver = DynamicEpisodeDriver ( train_env , tf_agent . collect_policy , observers =[ replay_buffer . add_batch ], num_episodes = tc . num_episodes_per_iteration ) # Train collect_driver . run = common . function ( collect_driver . run , autograph = False ) tf_agent . train = common . function ( tf_agent . train , autograph = False ) self . log ( 'Starting training...' ) while True: self . on_train_iteration_begin () msg = f'iteration { tc . iterations_done_in_training:4 } of { tc . num_iterations: < 4 } ' self.log_api(' collect_driver . run' , msg ) collect_driver . run () self . log_api ( 'replay_buffer.gather_all' , msg ) trajectories = replay_buffer . gather_all () self . log_api ( 'tf_agent.train' , msg ) loss_info = tf_agent . train ( experience = trajectories ) total_loss = loss_info . loss . numpy () self . log_api ( '' , f'loss ={ total_loss: < 7.1 f } ') self.log_api(' replay_buffer . clear' , msg ) replay_buffer . clear () self . on_train_iteration_end ( loss = total_loss ) if tc . training_done: break return","title":"TfReinforceAgent"},{"location":"reference/easyagents/backends/tfagents/#ancestors-in-mro_5","text":"easyagents.backends.tfagents.TfAgent easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/backends/tfagents/#methods_5","text":"","title":"Methods"},{"location":"reference/easyagents/backends/tfagents/#load_4","text":"def load ( self , directory : str , callbacks : List [ easyagents . core . AgentCallback ] ) Loads a previously trained and saved actor policy from directory. The loaded policy may afterwards be used by calling play(). Args: directory: the directory containing the trained policy callbacks: list of callbacks called during the load. View Source def load ( self , directory : str , callbacks : List [ core . AgentCallback ]): \"\"\"Loads a previously trained and saved actor policy from directory. The loaded policy may afterwards be used by calling play(). Args: directory: the directory containing the trained policy callbacks: list of callbacks called during the load. \"\"\" assert callbacks is not None , \"callbacks not set\" assert directory assert os . path . isdir ( directory ) self . _callbacks = callbacks self . load_implementation ( directory ) self . _agent_context . _is_policy_trained = True self . _callbacks = None","title":"load"},{"location":"reference/easyagents/backends/tfagents/#load_implementation_4","text":"def load_implementation ( self , directory : str ) Loads a previously saved actor policy from the directory Args: directory: the directory to load the policy from. View Source def load_implementation ( self , directory : str ): \"\"\"Loads a previously saved actor policy from the directory Args: directory: the directory to load the policy from. \"\"\" assert directory self . log_api ( 'saved_model.load' , f '({directory})' ) self . _trained_policy = tf . compat . v2 . saved_model . load ( directory )","title":"load_implementation"},{"location":"reference/easyagents/backends/tfagents/#log_4","text":"def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ): \"\"\"Logs msg.\"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg )","title":"log"},{"location":"reference/easyagents/backends/tfagents/#log_api_4","text":"def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\"Logs a call to api_target with additional log_msg.\"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg )","title":"log_api"},{"location":"reference/easyagents/backends/tfagents/#on_play_episode_begin_4","text":"def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ): \"\"\"Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. \"\"\" assert env , \"env not set.\" assert isinstance ( env , gym . core . Env ), \"env not an an instance of gym.Env.\" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context )","title":"on_play_episode_begin"},{"location":"reference/easyagents/backends/tfagents/#on_play_episode_end_4","text":"def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ): \"\"\"Must be called by play_implementation at the end of an episode\"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context )","title":"on_play_episode_end"},{"location":"reference/easyagents/backends/tfagents/#on_train_iteration_begin_4","text":"def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ): \"\"\"Must be called by train_implementation at the begining of a new iteration\"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context )","title":"on_train_iteration_begin"},{"location":"reference/easyagents/backends/tfagents/#on_train_iteration_end_4","text":"def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\"Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc.episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc.episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context )","title":"on_train_iteration_end"},{"location":"reference/easyagents/backends/tfagents/#play_4","text":"def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None","title":"play"},{"location":"reference/easyagents/backends/tfagents/#play_implementation_4","text":"def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : core . PlayContext ): \"\"\"Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used \"\"\" assert play_context , \"play_context not set.\" assert self . _trained_policy , \"trained_policy not set. call train() first.\" if self . _play_env is None : self . _play_env = self . _create_env () gym_env = self . _get_gym_env ( self . _play_env ) while True : self . on_play_episode_begin ( env = gym_env ) time_step = self . _play_env . reset () while not time_step . is_last (): action_step = self . _trained_policy . action ( time_step ) time_step = self . _play_env . step ( action_step . action ) self . on_play_episode_end () if play_context . play_done : break","title":"play_implementation"},{"location":"reference/easyagents/backends/tfagents/#save_4","text":"def save ( self , directory : str , callbacks : List [ easyagents . core . AgentCallback ] ) Saves the currently trained actor policy in directory. Only the actor policy is guaranteed to be saved. Thus after a call to load resuming training is not supported. Args: directory: the directory to save the policy weights to. the directory must exist. callbacks: list of callbacks called during policy load. View Source def save ( self , directory : str , callbacks : List [ core . AgentCallback ]): \"\"\"Saves the currently trained actor policy in directory. Only the actor policy is guaranteed to be saved. Thus after a call to load resuming training is not supported. Args: directory: the directory to save the policy weights to. the directory must exist. callbacks: list of callbacks called during policy load. \"\"\" assert callbacks is not None , \"callbacks not set\" assert directory assert os . path . isdir ( directory ) assert self . _agent_context . _is_policy_trained , \"No trained policy available.\" self . _callbacks = callbacks self . save_implementation ( directory ) self . _callbacks = None","title":"save"},{"location":"reference/easyagents/backends/tfagents/#save_implementation_4","text":"def save_implementation ( self , directory : str ) Saves the trained actor policy in directory. If no policy was trained yet, no file is written . Args: directory: the directory to save the policy weights to. View Source def save_implementation ( self , directory : str ): \"\"\"Saves the trained actor policy in directory. If no policy was trained yet, no file is written . Args: directory: the directory to save the policy weights to. \"\"\" assert self . _trained_policy , \"no policy trained yet.\" self . log_api ( 'PolicySaver' , f '(trained_policy,seed={self.model_config.seed})' ) saver = policy_saver . PolicySaver ( self . _trained_policy , seed = self . model_config . seed ) self . log_api ( 'policy_saver.save' , f '({directory})' ) saver . save ( directory )","title":"save_implementation"},{"location":"reference/easyagents/backends/tfagents/#train_4","text":"def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . _agent_context . _is_policy_trained = True self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None","title":"train"},{"location":"reference/easyagents/backends/tfagents/#train_implementation_4","text":"def train_implementation ( self , train_context : easyagents . core . TrainContext ) Tf-Agents Reinforce Implementation of the train loop. View Source def train_implementation ( self , train_context : core . TrainContext ): \"\"\"Tf-Agents Reinforce Implementation of the train loop.\"\"\" assert isinstance ( train_context , core . EpisodesTrainContext ) tc : core . EpisodesTrainContext = train_context self . log ( 'Creating environment...' ) train_env = self . _create_env ( discount = tc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () # SetUp Optimizer , Networks and PpoAgent self . log_api ( 'AdamOptimizer' , 'create' ) optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ) self . log_api ( 'ActorDistributionNetwork' , 'create' ) actor_net = actor_distribution_network . ActorDistributionNetwork ( observation_spec , action_spec , fc_layer_params = self . model_config . fc_layers ) self . log_api ( 'ReinforceAgent' , 'create' ) tf_agent = reinforce_agent . ReinforceAgent ( timestep_spec , action_spec , actor_network = actor_net , optimizer = optimizer ) self . log_api ( 'tf_agent.initialize()' ) tf_agent . initialize () self . _trained_policy = tf_agent . policy # SetUp Data collection & Buffering collect_data_spec = tf_agent . collect_data_spec self . log_api ( 'TFUniformReplayBuffer' , 'create' ) replay_buffer = TFUniformReplayBuffer ( collect_data_spec , batch_size = 1 , max_length = tc . max_steps_in_buffer ) self . log_api ( 'DynamicEpisodeDriver' , 'create' ) collect_driver = DynamicEpisodeDriver ( train_env , tf_agent . collect_policy , observers = [ replay_buffer . add_batch ], num_episodes = tc . num_episodes_per_iteration ) # Train collect_driver . run = common . function ( collect_driver . run , autograph = False ) tf_agent . train = common . function ( tf_agent . train , autograph = False ) self . log ( 'Starting training...' ) while True : self . on_train_iteration_begin () msg = f 'iteration {tc.iterations_done_in_training:4} of {tc.num_iterations:<4}' self . log_api ( 'collect_driver.run' , msg ) collect_driver . run () self . log_api ( 'replay_buffer.gather_all' , msg ) trajectories = replay_buffer . gather_all () self . log_api ( 'tf_agent.train' , msg ) loss_info = tf_agent . train ( experience = trajectories ) total_loss = loss_info . loss . numpy () self . log_api ( '' , f 'loss={total_loss:<7.1f}' ) self . log_api ( 'replay_buffer.clear' , msg ) replay_buffer . clear () self . on_train_iteration_end ( loss = total_loss ) if tc . training_done : break return","title":"train_implementation"},{"location":"reference/easyagents/backends/tfagents/#tfsacagent","text":"class TfSacAgent ( model_config : easyagents . core . ModelConfig ) creates a new agent based on the SAC algorithm using the tfagents implementation. adapted from https://github.com/tensorflow/agents/blob/master/tf_agents/colabs/7_SAC_minitaur_tutorial.ipynb Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. View Source class TfSacAgent ( TfAgent ): \"\"\" creates a new agent based on the SAC algorithm using the tfagents implementation. adapted from https://github.com/tensorflow/agents/blob/master/tf_agents/colabs/7_SAC_minitaur_tutorial.ipynb Args: model_config: the model configuration including the name of the target gym environment as well as the neural network architecture. \"\"\" def __init__ ( self , model_config: core . ModelConfig ): super (). __init__ ( model_config = model_config ) # noinspection DuplicatedCode def train_implementation ( self , train_context: core . TrainContext ): \"\"\"Tf-Agents Ppo Implementation of the train loop.\"\"\" assert isinstance ( train_context , core . StepsTrainContext ) tc: core . StepsTrainContext = train_context train_env = self . _create_env ( discount = tc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () self . log_api ( 'CriticNetwork' , f' ( observation_spec , action_spec ), observation_fc_layer_params = None , ' + f' action_fc_layer_params = None , joint_fc_layer_params ={ self . model_config . fc_layers }) ') critic_net = critic_network.CriticNetwork((observation_spec, action_spec), observation_fc_layer_params=None, action_fc_layer_params=None, joint_fc_layer_params=self.model_config.fc_layers) def normal_projection_net(action_spec_arg, init_means_output_factor=0.1): return normal_projection_network.NormalProjectionNetwork(action_spec_arg, mean_transform=None, state_dependent_std=True, init_means_output_factor=init_means_output_factor, std_transform=sac_agent.std_clip_transform, scale_distribution=True) self.log_api(' ActorDistributionNetwork' , f'observation_spec , action_spec , fc_layer_params ={ self . model_config . fc_layers }), ' + f' continuous_projection_net =...) ') actor_net = actor_distribution_network.ActorDistributionNetwork(observation_spec, action_spec, fc_layer_params=self.model_config.fc_layers, continuous_projection_net=normal_projection_net) # self.log_api(' tf . compat . v1 . train . get_or_create_global_step' , '()' ) # global_step = tf.compat.v1.train.get_or_create_global_step() self . log_api ( 'SacAgent' , f' ( timestep_spec , action_spec , actor_network =..., critic_network =..., ' + f' actor_optimizer = AdamOptimizer ( learning_rate ={ tc . learning_rate }), ' + f' critic_optimizer = AdamOptimizer ( learning_rate ={ tc . learning_rate }), ' + f' alpha_optimizer = AdamOptimizer ( learning_rate ={ tc . learning_rate }), ' + f' gamma ={ tc . reward_discount_gamma }) ') tf_agent = sac_agent.SacAgent( timestep_spec, action_spec, actor_network=actor_net, critic_network=critic_net, actor_optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=tc.learning_rate), critic_optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=tc.learning_rate), alpha_optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=tc.learning_rate), # target_update_tau=0.005, # target_update_period=1, # td_errors_loss_fn=tf.compat.v1.losses.mean_squared_error, gamma=tc.reward_discount_gamma) # reward_scale_factor=1.0, # gradient_clipping=None, # train_step_counter=global_step) self.log_api(' tf_agent . initialize' , '()' ) tf_agent . initialize () self . _trained_policy = greedy_policy . GreedyPolicy ( tf_agent . policy ) collect_policy = tf_agent . collect_policy # setup and preload replay buffer self . log_api ( 'TFUniformReplayBuffer' , f' ( data_spec = tf_agent . collect_data_spec , ' + f' batch_size ={ train_env . batch_size }, max_length ={ tc . max_steps_in_buffer }) ') replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(data_spec=tf_agent.collect_data_spec, batch_size=train_env.batch_size, max_length=tc.max_steps_in_buffer) self.log_api(' DynamicStepDriver' , f' ( env , collect_policy , observers =[ replay_buffer . add_batch ], ' + f' num_steps ={ tc . num_steps_buffer_preload }) ') initial_collect_driver = dynamic_step_driver.DynamicStepDriver(train_env, collect_policy, observers=[replay_buffer.add_batch], num_steps=tc.num_steps_buffer_preload) self.log_api(' initial_collect_driver . run () ') initial_collect_driver.run() # Dataset generates trajectories with shape [Bx2x...] dataset = replay_buffer.as_dataset(num_parallel_calls=3, sample_batch_size=tc.num_steps_sampled_from_buffer, num_steps=2).prefetch(3) iterator = iter(dataset) self.log_api(' DynamicStepDriver' , f' ( env , collect_policy , observers =[ replay_buffer . add_batch ], ' + f' num_steps ={ tc . num_steps_per_iteration }) ') collect_driver = dynamic_step_driver.DynamicStepDriver(train_env, collect_policy, observers=[replay_buffer.add_batch], num_steps=tc.num_steps_per_iteration) # (Optional) Optimize by wrapping some of the code in a graph using TF function. tf_agent.train = common.function(tf_agent.train) collect_driver.run = common.function(collect_driver.run) self.log_api(' for each iteration' ) self . log_api ( ' collect_driver.run' , '()' ) self . log_api ( ' tf_agent.train' , '(experience=...)' ) while True: self . on_train_iteration_begin () # Collect a few steps using collect_policy and save to the replay buffer. for _ in range ( tc . num_steps_per_iteration ): collect_driver . run () # Sample a batch of data from the buffer and update the agent's network. experience , _ = next ( iterator ) loss_info = tf_agent . train ( experience ) total_loss = loss_info . loss . numpy () actor_loss = loss_info . extra . actor_loss alpha_loss = loss_info . extra . alpha_loss critic_loss = loss_info . extra . critic_loss self . on_train_iteration_end ( loss = total_loss , actor_loss = actor_loss , critic_loss = critic_loss , alpha_loss = alpha_loss ) if tc . training_done: break return","title":"TfSacAgent"},{"location":"reference/easyagents/backends/tfagents/#ancestors-in-mro_6","text":"easyagents.backends.tfagents.TfAgent easyagents.backends.core.BackendAgent easyagents.backends.core._BackendAgent abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/backends/tfagents/#methods_6","text":"","title":"Methods"},{"location":"reference/easyagents/backends/tfagents/#load_5","text":"def load ( self , directory : str , callbacks : List [ easyagents . core . AgentCallback ] ) Loads a previously trained and saved actor policy from directory. The loaded policy may afterwards be used by calling play(). Args: directory: the directory containing the trained policy callbacks: list of callbacks called during the load. View Source def load ( self , directory : str , callbacks : List [ core . AgentCallback ]): \"\"\"Loads a previously trained and saved actor policy from directory. The loaded policy may afterwards be used by calling play(). Args: directory: the directory containing the trained policy callbacks: list of callbacks called during the load. \"\"\" assert callbacks is not None , \"callbacks not set\" assert directory assert os . path . isdir ( directory ) self . _callbacks = callbacks self . load_implementation ( directory ) self . _agent_context . _is_policy_trained = True self . _callbacks = None","title":"load"},{"location":"reference/easyagents/backends/tfagents/#load_implementation_5","text":"def load_implementation ( self , directory : str ) Loads a previously saved actor policy from the directory Args: directory: the directory to load the policy from. View Source def load_implementation ( self , directory : str ): \"\"\"Loads a previously saved actor policy from the directory Args: directory: the directory to load the policy from. \"\"\" assert directory self . log_api ( 'saved_model.load' , f '({directory})' ) self . _trained_policy = tf . compat . v2 . saved_model . load ( directory )","title":"load_implementation"},{"location":"reference/easyagents/backends/tfagents/#log_5","text":"def log ( self , log_msg : str ) Logs msg. View Source def log ( self , log_msg : str ): \"\"\"Logs msg.\"\"\" self . _agent_context . gym . _monitor_env = None if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_log ( self . _agent_context , log_msg = log_msg )","title":"log"},{"location":"reference/easyagents/backends/tfagents/#log_api_5","text":"def log_api ( self , api_target : str , log_msg : Union [ str , NoneType ] = None ) Logs a call to api_target with additional log_msg. View Source def log_api ( self , api_target : str , log_msg : Optional [ str ] = None ) : \"\"\"Logs a call to api_target with additional log_msg.\"\"\" self . _agent_context . gym . _monitor_env = None if api_target is None : api_target = '' if log_msg is None : log_msg = '' for c in self . _callbacks : c . on_api_log ( self . _agent_context , api_target , log_msg = log_msg )","title":"log_api"},{"location":"reference/easyagents/backends/tfagents/#on_play_episode_begin_5","text":"def on_play_episode_begin ( self , env : gym . core . Env ) Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. View Source def on_play_episode_begin ( self , env : gym . core . Env ): \"\"\"Must be called by play_implementation at the beginning of a new episode Args: env: the gym environment used to play the episode. \"\"\" assert env , \"env not set.\" assert isinstance ( env , gym . core . Env ), \"env not an an instance of gym.Env.\" pc = self . _agent_context . play pc . gym_env = env pc . steps_done_in_episode = 0 pc . actions [ pc . episodes_done + 1 ] = [] pc . rewards [ pc . episodes_done + 1 ] = [] pc . sum_of_rewards [ pc . episodes_done + 1 ] = 0 for c in self . _callbacks : c . on_play_episode_begin ( self . _agent_context )","title":"on_play_episode_begin"},{"location":"reference/easyagents/backends/tfagents/#on_play_episode_end_5","text":"def on_play_episode_end ( self ) Must be called by play_implementation at the end of an episode View Source def on_play_episode_end ( self ): \"\"\"Must be called by play_implementation at the end of an episode\"\"\" pc = self . _agent_context . play pc . episodes_done += 1 if pc . num_episodes and pc . episodes_done >= pc . num_episodes : pc . play_done = True for c in self . _callbacks : c . on_play_episode_end ( self . _agent_context )","title":"on_play_episode_end"},{"location":"reference/easyagents/backends/tfagents/#on_train_iteration_begin_5","text":"def on_train_iteration_begin ( self ) Must be called by train_implementation at the begining of a new iteration View Source def on_train_iteration_begin ( self ): \"\"\"Must be called by train_implementation at the begining of a new iteration\"\"\" tc = self . _agent_context . train tc . episodes_done_in_iteration = 0 tc . steps_done_in_iteration = 0 if tc . iterations_done_in_training == 0 : self . _eval_current_policy () self . _train_total_episodes_on_iteration_begin = self . _agent_context . gym . _totals . episodes_done for c in self . _callbacks : c . on_train_iteration_begin ( self . _agent_context )","title":"on_train_iteration_begin"},{"location":"reference/easyagents/backends/tfagents/#on_train_iteration_end_5","text":"def on_train_iteration_end ( self , loss : float , ** kwargs ) Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. View Source def on_train_iteration_end ( self , loss : float , ** kwargs ) : \"\"\"Must be called by train_implementation at the end of an iteration Evaluates the current policy. Use kwargs to set additional dict values in train context. Eg for an ActorCriticTrainContext the losses may be set like this: on_train_iteration(loss=123,actor_loss=456,critic_loss=789) Args: loss: loss after the training of the model in this iteration or math.nan if the loss is not available **kwargs: if a keyword matches a dict property of the TrainContext instance, then the dict[episodes_done_in_training] is set to the arg. \"\"\" tc = self . _agent_context . train totals = self . _agent_context . gym . _totals tc . episodes_done_in_iteration = ( totals . episodes_done - self . _train_total_episodes_on_iteration_begin ) tc . episodes_done_in_training += tc . episodes_done_in_iteration tc . loss [ tc.episodes_done_in_training ] = loss # set traincontext dict from kwargs : for prop_name in kwargs : prop_instance = getattr ( tc , prop_name , None ) prop_value = kwargs [ prop_name ] if prop_instance is not None and isinstance ( prop_instance , dict ) : prop_instance [ tc.episodes_done_in_training ] = prop_value tc . iterations_done_in_training += 1 if tc . num_iterations is not None : tc . training_done = tc . iterations_done_in_training >= tc . num_iterations self . _train_total_episodes_on_iteration_begin = 0 if tc . num_iterations_between_eval and ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : self . _eval_current_policy () for c in self . _callbacks : c . on_train_iteration_end ( self . _agent_context )","title":"on_train_iteration_end"},{"location":"reference/easyagents/backends/tfagents/#play_5","text":"def play ( self , play_context : easyagents . core . PlayContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. View Source def play ( self , play_context : core . PlayContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to play_implementation overriden by the subclass. Args: play_context: play configuration to be used callbacks: list of callbacks called during play. \"\"\" assert callbacks is not None , \"callbacks not set\" assert play_context , \"play_context not set\" assert self . _agent_context . play is None , \"play_context already set in agent_context\" play_context . _reset () play_context . _validate () self . _agent_context . play = play_context old_callbacks = self . _callbacks self . _callbacks = callbacks try : monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_play_begin () self . play_implementation ( self . _agent_context . play ) self . _on_play_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = old_callbacks self . _agent_context . play = None","title":"play"},{"location":"reference/easyagents/backends/tfagents/#play_implementation_5","text":"def play_implementation ( self , play_context : easyagents . core . PlayContext ) Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used View Source def play_implementation ( self , play_context : core . PlayContext ): \"\"\"Agent specific implementation of playing a single episodes with the current policy. Args: play_context: play configuration to be used \"\"\" assert play_context , \"play_context not set.\" assert self . _trained_policy , \"trained_policy not set. call train() first.\" if self . _play_env is None : self . _play_env = self . _create_env () gym_env = self . _get_gym_env ( self . _play_env ) while True : self . on_play_episode_begin ( env = gym_env ) time_step = self . _play_env . reset () while not time_step . is_last (): action_step = self . _trained_policy . action ( time_step ) time_step = self . _play_env . step ( action_step . action ) self . on_play_episode_end () if play_context . play_done : break","title":"play_implementation"},{"location":"reference/easyagents/backends/tfagents/#save_5","text":"def save ( self , directory : str , callbacks : List [ easyagents . core . AgentCallback ] ) Saves the currently trained actor policy in directory. Only the actor policy is guaranteed to be saved. Thus after a call to load resuming training is not supported. Args: directory: the directory to save the policy weights to. the directory must exist. callbacks: list of callbacks called during policy load. View Source def save ( self , directory : str , callbacks : List [ core . AgentCallback ]): \"\"\"Saves the currently trained actor policy in directory. Only the actor policy is guaranteed to be saved. Thus after a call to load resuming training is not supported. Args: directory: the directory to save the policy weights to. the directory must exist. callbacks: list of callbacks called during policy load. \"\"\" assert callbacks is not None , \"callbacks not set\" assert directory assert os . path . isdir ( directory ) assert self . _agent_context . _is_policy_trained , \"No trained policy available.\" self . _callbacks = callbacks self . save_implementation ( directory ) self . _callbacks = None","title":"save"},{"location":"reference/easyagents/backends/tfagents/#save_implementation_5","text":"def save_implementation ( self , directory : str ) Saves the trained actor policy in directory. If no policy was trained yet, no file is written . Args: directory: the directory to save the policy weights to. View Source def save_implementation ( self , directory : str ): \"\"\"Saves the trained actor policy in directory. If no policy was trained yet, no file is written . Args: directory: the directory to save the policy weights to. \"\"\" assert self . _trained_policy , \"no policy trained yet.\" self . log_api ( 'PolicySaver' , f '(trained_policy,seed={self.model_config.seed})' ) saver = policy_saver . PolicySaver ( self . _trained_policy , seed = self . model_config . seed ) self . log_api ( 'policy_saver.save' , f '({directory})' ) saver . save ( directory )","title":"save_implementation"},{"location":"reference/easyagents/backends/tfagents/#train_5","text":"def train ( self , train_context : easyagents . core . TrainContext , callbacks : List [ easyagents . core . AgentCallback ] ) Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. View Source def train ( self , train_context : core . TrainContext , callbacks : List [ core . AgentCallback ]): \"\"\"Forwarding to train_implementation overriden by the subclass Args: train_context: training configuration to be used callbacks: list of callbacks called during the training and evaluation. \"\"\" assert callbacks is not None , \"callbacks not set\" assert train_context , \"train_context not set\" train_context . _reset () train_context . _validate () self . _agent_context . train = train_context self . _agent_context . play = None self . _callbacks = callbacks try : self . log_api ( f 'backend_name' , f '{self._backend_name}' ) self . _set_seed () monitor . _MonitorEnv . _register_backend_agent ( self ) self . _on_train_begin () self . _agent_context . _is_policy_trained = True self . train_implementation ( self . _agent_context . train ) self . _on_train_end () finally : monitor . _MonitorEnv . _register_backend_agent ( None ) self . _callbacks = None self . _agent_context . play = None self . _agent_context . train = None","title":"train"},{"location":"reference/easyagents/backends/tfagents/#train_implementation_5","text":"def train_implementation ( self , train_context : easyagents . core . TrainContext ) Tf-Agents Ppo Implementation of the train loop. View Source def train_implementation ( self , train_context : core . TrainContext ): \"\"\"Tf-Agents Ppo Implementation of the train loop.\"\"\" assert isinstance ( train_context , core . StepsTrainContext ) tc : core . StepsTrainContext = train_context train_env = self . _create_env ( discount = tc . reward_discount_gamma ) observation_spec = train_env . observation_spec () action_spec = train_env . action_spec () timestep_spec = train_env . time_step_spec () self . log_api ( 'CriticNetwork' , f '(observation_spec, action_spec), observation_fc_layer_params=None, ' + f 'action_fc_layer_params=None, joint_fc_layer_params={self.model_config.fc_layers})' ) critic_net = critic_network . CriticNetwork (( observation_spec , action_spec ), observation_fc_layer_params = None , action_fc_layer_params = None , joint_fc_layer_params = self . model_config . fc_layers ) def normal_projection_net ( action_spec_arg , init_means_output_factor = 0 . 1 ): return normal_projection_network . NormalProjectionNetwork ( action_spec_arg , mean_transform = None , state_dependent_std = True , init_means_output_factor = init_means_output_factor , std_transform = sac_agent . std_clip_transform , scale_distribution = True ) self . log_api ( 'ActorDistributionNetwork' , f 'observation_spec, action_spec, fc_layer_params={self.model_config.fc_layers}), ' + f 'continuous_projection_net=...)' ) actor_net = actor_distribution_network . ActorDistributionNetwork ( observation_spec , action_spec , fc_layer_params = self . model_config . fc_layers , continuous_projection_net = normal_projection_net ) # self . log_api ( 'tf.compat.v1.train.get_or_create_global_step' , '()' ) # global_step = tf . compat . v1 . train . get_or_create_global_step () self . log_api ( 'SacAgent' , f '(timestep_spec, action_spec, actor_network=..., critic_network=..., ' + f 'actor_optimizer=AdamOptimizer(learning_rate={tc.learning_rate}), ' + f 'critic_optimizer=AdamOptimizer(learning_rate={tc.learning_rate}), ' + f 'alpha_optimizer=AdamOptimizer(learning_rate={tc.learning_rate}), ' + f 'gamma={tc.reward_discount_gamma})' ) tf_agent = sac_agent . SacAgent ( timestep_spec , action_spec , actor_network = actor_net , critic_network = critic_net , actor_optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ), critic_optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ), alpha_optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate = tc . learning_rate ), # target_update_tau = 0 . 005 , # target_update_period = 1 , # td_errors_loss_fn = tf . compat . v1 . losses . mean_squared_error , gamma = tc . reward_discount_gamma ) # reward_scale_factor = 1 . 0 , # gradient_clipping = None , # train_step_counter = global_step ) self . log_api ( 'tf_agent.initialize' , '()' ) tf_agent . initialize () self . _trained_policy = greedy_policy . GreedyPolicy ( tf_agent . policy ) collect_policy = tf_agent . collect_policy # setup and preload replay buffer self . log_api ( 'TFUniformReplayBuffer' , f '(data_spec=tf_agent.collect_data_spec, ' + f 'batch_size={train_env.batch_size}, max_length={tc.max_steps_in_buffer})' ) replay_buffer = tf_uniform_replay_buffer . TFUniformReplayBuffer ( data_spec = tf_agent . collect_data_spec , batch_size = train_env . batch_size , max_length = tc . max_steps_in_buffer ) self . log_api ( 'DynamicStepDriver' , f '(env, collect_policy, observers=[replay_buffer.add_batch], ' + f 'num_steps={tc.num_steps_buffer_preload})' ) initial_collect_driver = dynamic_step_driver . DynamicStepDriver ( train_env , collect_policy , observers = [ replay_buffer . add_batch ], num_steps = tc . num_steps_buffer_preload ) self . log_api ( 'initial_collect_driver.run()' ) initial_collect_driver . run () # Dataset generates trajectories with shape [ Bx2x ...] dataset = replay_buffer . as_dataset ( num_parallel_calls = 3 , sample_batch_size = tc . num_steps_sampled_from_buffer , num_steps = 2 ). prefetch ( 3 ) iterator = iter ( dataset ) self . log_api ( 'DynamicStepDriver' , f '(env, collect_policy, observers=[replay_buffer.add_batch], ' + f 'num_steps={tc.num_steps_per_iteration})' ) collect_driver = dynamic_step_driver . DynamicStepDriver ( train_env , collect_policy , observers = [ replay_buffer . add_batch ], num_steps = tc . num_steps_per_iteration ) # ( Optional ) Optimize by wrapping some of the code in a graph using TF function . tf_agent . train = common . function ( tf_agent . train ) collect_driver . run = common . function ( collect_driver . run ) self . log_api ( 'for each iteration' ) self . log_api ( ' collect_driver.run' , '()' ) self . log_api ( ' tf_agent.train' , '(experience=...)' ) while True : self . on_train_iteration_begin () # Collect a few steps using collect_policy and save to the replay buffer . for _ in range ( tc . num_steps_per_iteration ): collect_driver . run () # Sample a batch of data from the buffer and update the agent ' s network . experience , _ = next ( iterator ) loss_info = tf_agent . train ( experience ) total_loss = loss_info . loss . numpy () actor_loss = loss_info . extra . actor_loss alpha_loss = loss_info . extra . alpha_loss critic_loss = loss_info . extra . critic_loss self . on_train_iteration_end ( loss = total_loss , actor_loss = actor_loss , critic_loss = critic_loss , alpha_loss = alpha_loss ) if tc . training_done : break return","title":"train_implementation"},{"location":"reference/easyagents/callbacks/duration/","text":"Module easyagents.callbacks.duration View Source import math import easyagents.core as core class Fast ( core . AgentCallback ): \"\"\"Train for small number of episodes / steps in order to do a dry run of the algorithms or callbacks.\"\"\" def __init__ ( self , num_iterations = None ): self . _num_iterations = num_iterations self . _num_episodes_per_iteration = 3 self . _max_steps_per_episode = 50 def on_play_begin ( self , agent_context : core . AgentContext ): agent_context . play . max_steps_per_episode = self . _max_steps_per_episode if isinstance ( agent_context , core . EpisodesTrainContext ): agent_context . num_episodes_per_iteration = self . _num_episodes_per_iteration def on_train_begin ( self , agent_context : core . AgentContext ): tc = agent_context . train if self . _num_iterations is None : self . _num_iterations = 10 if isinstance ( tc , core . EpisodesTrainContext ): ec : core . EpisodesTrainContext = tc ec . num_episodes_per_iteration = self . _num_episodes_per_iteration ec . num_epochs_per_iteration = 1 if isinstance ( tc , core . StepsTrainContext ): dc : core . StepsTrainContext = tc if self . _num_iterations is None : self . _num_iterations = 5 * self . _num_episodes_per_iteration * self . _max_steps_per_episode dc . num_steps_buffer_preload = math . ceil ( self . _num_iterations / 10 ) tc . num_iterations = self . _num_iterations tc . num_iterations_between_eval = math . ceil ( tc . num_iterations / 3 ) tc . num_episodes_per_eval = self . _num_episodes_per_iteration tc . max_steps_per_episode = self . _max_steps_per_episode class _SingleEpisode ( Fast ): \"\"\"Train / Play only for 1 episode (no evaluation in training, max. 10 steps).\"\"\" def __init__ ( self ): super () . __init__ ( num_iterations = 1 ) self . _num_episodes_per_iteration = 1 self . _max_steps_per_episode = 10 def on_train_begin ( self , agent_context : core . AgentContext ): super () . on_train_begin ( agent_context ) tc = agent_context . train if isinstance ( tc , core . StepsTrainContext ): tc . num_iterations = self . _max_steps_per_episode tc . num_iterations_between_eval = 0 tc . num_episodes_per_eval = 0 class _SingleIteration ( Fast ): \"\"\"Train / play for a single iteration with 3 episodes, and evaluation of 2 episodes\"\"\" def __init__ ( self ): super () . __init__ ( num_iterations = 1 ) Classes Fast class Fast ( num_iterations = None ) Train for small number of episodes / steps in order to do a dry run of the algorithms or callbacks. View Source class Fast ( core . AgentCallback ): \"\"\"Train for small number of episodes / steps in order to do a dry run of the algorithms or callbacks.\"\"\" def __init__ ( self , num_iterations = None ): self . _num_iterations = num_iterations self . _num_episodes_per_iteration = 3 self . _max_steps_per_episode = 50 def on_play_begin ( self , agent_context: core . AgentContext ): agent_context . play . max_steps_per_episode = self . _max_steps_per_episode if isinstance ( agent_context , core . EpisodesTrainContext ): agent_context . num_episodes_per_iteration = self . _num_episodes_per_iteration def on_train_begin ( self , agent_context: core . AgentContext ): tc = agent_context . train if self . _num_iterations is None: self . _num_iterations = 10 if isinstance ( tc , core . EpisodesTrainContext ): ec: core . EpisodesTrainContext = tc ec . num_episodes_per_iteration = self . _num_episodes_per_iteration ec . num_epochs_per_iteration = 1 if isinstance ( tc , core . StepsTrainContext ): dc: core . StepsTrainContext = tc if self . _num_iterations is None: self . _num_iterations = 5 * self . _num_episodes_per_iteration * self . _max_steps_per_episode dc . num_steps_buffer_preload = math . ceil ( self . _num_iterations / 10 ) tc . num_iterations = self . _num_iterations tc . num_iterations_between_eval = math . ceil ( tc . num_iterations / 3 ) tc . num_episodes_per_eval = self . _num_episodes_per_iteration tc . max_steps_per_episode = self . _max_steps_per_episode Ancestors (in MRO) easyagents.core.AgentCallback abc.ABC Descendants easyagents.callbacks.duration._SingleEpisode easyagents.callbacks.duration._SingleIteration Methods on_api_log def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass on_gym_init_begin def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" on_gym_init_end def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass on_gym_reset_begin def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\" on_gym_reset_end def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass on_gym_step_begin def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass on_gym_step_end def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass on_log def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass on_play_begin def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : core . AgentContext ): agent_context . play . max_steps_per_episode = self . _max_steps_per_episode if isinstance ( agent_context , core . EpisodesTrainContext ): agent_context . num_episodes_per_iteration = self . _num_episodes_per_iteration on_play_end def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.play() call (during play or eval, but not during train)\"\"\" on_play_episode_begin def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\" on_play_episode_end def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : AgentContext ): \"\"\"Called once after an episode is done or stopped (during play or eval, but not during train).\"\"\" on_play_step_begin def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\" on_play_step_end def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"Called once after a step is completed in the current episode (during play or eval, but not during train).\"\"\" on_train_begin def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : core . AgentContext ): tc = agent_context . train if self . _num_iterations is None : self . _num_iterations = 10 if isinstance ( tc , core . EpisodesTrainContext ): ec : core . EpisodesTrainContext = tc ec . num_episodes_per_iteration = self . _num_episodes_per_iteration ec . num_epochs_per_iteration = 1 if isinstance ( tc , core . StepsTrainContext ): dc : core . StepsTrainContext = tc if self . _num_iterations is None : self . _num_iterations = 5 * self . _num_episodes_per_iteration * self . _max_steps_per_episode dc . num_steps_buffer_preload = math . ceil ( self . _num_iterations / 10 ) tc . num_iterations = self . _num_iterations tc . num_iterations_between_eval = math . ceil ( tc . num_iterations / 3 ) tc . num_episodes_per_eval = self . _num_episodes_per_iteration tc . max_steps_per_episode = self . _max_steps_per_episode on_train_end def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\" on_train_iteration_begin def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\" on_train_iteration_end def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : AgentContext ): \"\"\"Called once after the current iteration is completed\"\"\"","title":"Duration"},{"location":"reference/easyagents/callbacks/duration/#module-easyagentscallbacksduration","text":"View Source import math import easyagents.core as core class Fast ( core . AgentCallback ): \"\"\"Train for small number of episodes / steps in order to do a dry run of the algorithms or callbacks.\"\"\" def __init__ ( self , num_iterations = None ): self . _num_iterations = num_iterations self . _num_episodes_per_iteration = 3 self . _max_steps_per_episode = 50 def on_play_begin ( self , agent_context : core . AgentContext ): agent_context . play . max_steps_per_episode = self . _max_steps_per_episode if isinstance ( agent_context , core . EpisodesTrainContext ): agent_context . num_episodes_per_iteration = self . _num_episodes_per_iteration def on_train_begin ( self , agent_context : core . AgentContext ): tc = agent_context . train if self . _num_iterations is None : self . _num_iterations = 10 if isinstance ( tc , core . EpisodesTrainContext ): ec : core . EpisodesTrainContext = tc ec . num_episodes_per_iteration = self . _num_episodes_per_iteration ec . num_epochs_per_iteration = 1 if isinstance ( tc , core . StepsTrainContext ): dc : core . StepsTrainContext = tc if self . _num_iterations is None : self . _num_iterations = 5 * self . _num_episodes_per_iteration * self . _max_steps_per_episode dc . num_steps_buffer_preload = math . ceil ( self . _num_iterations / 10 ) tc . num_iterations = self . _num_iterations tc . num_iterations_between_eval = math . ceil ( tc . num_iterations / 3 ) tc . num_episodes_per_eval = self . _num_episodes_per_iteration tc . max_steps_per_episode = self . _max_steps_per_episode class _SingleEpisode ( Fast ): \"\"\"Train / Play only for 1 episode (no evaluation in training, max. 10 steps).\"\"\" def __init__ ( self ): super () . __init__ ( num_iterations = 1 ) self . _num_episodes_per_iteration = 1 self . _max_steps_per_episode = 10 def on_train_begin ( self , agent_context : core . AgentContext ): super () . on_train_begin ( agent_context ) tc = agent_context . train if isinstance ( tc , core . StepsTrainContext ): tc . num_iterations = self . _max_steps_per_episode tc . num_iterations_between_eval = 0 tc . num_episodes_per_eval = 0 class _SingleIteration ( Fast ): \"\"\"Train / play for a single iteration with 3 episodes, and evaluation of 2 episodes\"\"\" def __init__ ( self ): super () . __init__ ( num_iterations = 1 )","title":"Module easyagents.callbacks.duration"},{"location":"reference/easyagents/callbacks/duration/#classes","text":"","title":"Classes"},{"location":"reference/easyagents/callbacks/duration/#fast","text":"class Fast ( num_iterations = None ) Train for small number of episodes / steps in order to do a dry run of the algorithms or callbacks. View Source class Fast ( core . AgentCallback ): \"\"\"Train for small number of episodes / steps in order to do a dry run of the algorithms or callbacks.\"\"\" def __init__ ( self , num_iterations = None ): self . _num_iterations = num_iterations self . _num_episodes_per_iteration = 3 self . _max_steps_per_episode = 50 def on_play_begin ( self , agent_context: core . AgentContext ): agent_context . play . max_steps_per_episode = self . _max_steps_per_episode if isinstance ( agent_context , core . EpisodesTrainContext ): agent_context . num_episodes_per_iteration = self . _num_episodes_per_iteration def on_train_begin ( self , agent_context: core . AgentContext ): tc = agent_context . train if self . _num_iterations is None: self . _num_iterations = 10 if isinstance ( tc , core . EpisodesTrainContext ): ec: core . EpisodesTrainContext = tc ec . num_episodes_per_iteration = self . _num_episodes_per_iteration ec . num_epochs_per_iteration = 1 if isinstance ( tc , core . StepsTrainContext ): dc: core . StepsTrainContext = tc if self . _num_iterations is None: self . _num_iterations = 5 * self . _num_episodes_per_iteration * self . _max_steps_per_episode dc . num_steps_buffer_preload = math . ceil ( self . _num_iterations / 10 ) tc . num_iterations = self . _num_iterations tc . num_iterations_between_eval = math . ceil ( tc . num_iterations / 3 ) tc . num_episodes_per_eval = self . _num_episodes_per_iteration tc . max_steps_per_episode = self . _max_steps_per_episode","title":"Fast"},{"location":"reference/easyagents/callbacks/duration/#ancestors-in-mro","text":"easyagents.core.AgentCallback abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/callbacks/duration/#descendants","text":"easyagents.callbacks.duration._SingleEpisode easyagents.callbacks.duration._SingleIteration","title":"Descendants"},{"location":"reference/easyagents/callbacks/duration/#methods","text":"","title":"Methods"},{"location":"reference/easyagents/callbacks/duration/#on_api_log","text":"def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass","title":"on_api_log"},{"location":"reference/easyagents/callbacks/duration/#on_gym_init_begin","text":"def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\"","title":"on_gym_init_begin"},{"location":"reference/easyagents/callbacks/duration/#on_gym_init_end","text":"def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass","title":"on_gym_init_end"},{"location":"reference/easyagents/callbacks/duration/#on_gym_reset_begin","text":"def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\"","title":"on_gym_reset_begin"},{"location":"reference/easyagents/callbacks/duration/#on_gym_reset_end","text":"def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass","title":"on_gym_reset_end"},{"location":"reference/easyagents/callbacks/duration/#on_gym_step_begin","text":"def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass","title":"on_gym_step_begin"},{"location":"reference/easyagents/callbacks/duration/#on_gym_step_end","text":"def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass","title":"on_gym_step_end"},{"location":"reference/easyagents/callbacks/duration/#on_log","text":"def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass","title":"on_log"},{"location":"reference/easyagents/callbacks/duration/#on_play_begin","text":"def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : core . AgentContext ): agent_context . play . max_steps_per_episode = self . _max_steps_per_episode if isinstance ( agent_context , core . EpisodesTrainContext ): agent_context . num_episodes_per_iteration = self . _num_episodes_per_iteration","title":"on_play_begin"},{"location":"reference/easyagents/callbacks/duration/#on_play_end","text":"def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.play() call (during play or eval, but not during train)\"\"\"","title":"on_play_end"},{"location":"reference/easyagents/callbacks/duration/#on_play_episode_begin","text":"def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\"","title":"on_play_episode_begin"},{"location":"reference/easyagents/callbacks/duration/#on_play_episode_end","text":"def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : AgentContext ): \"\"\"Called once after an episode is done or stopped (during play or eval, but not during train).\"\"\"","title":"on_play_episode_end"},{"location":"reference/easyagents/callbacks/duration/#on_play_step_begin","text":"def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\"","title":"on_play_step_begin"},{"location":"reference/easyagents/callbacks/duration/#on_play_step_end","text":"def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"Called once after a step is completed in the current episode (during play or eval, but not during train).\"\"\"","title":"on_play_step_end"},{"location":"reference/easyagents/callbacks/duration/#on_train_begin","text":"def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : core . AgentContext ): tc = agent_context . train if self . _num_iterations is None : self . _num_iterations = 10 if isinstance ( tc , core . EpisodesTrainContext ): ec : core . EpisodesTrainContext = tc ec . num_episodes_per_iteration = self . _num_episodes_per_iteration ec . num_epochs_per_iteration = 1 if isinstance ( tc , core . StepsTrainContext ): dc : core . StepsTrainContext = tc if self . _num_iterations is None : self . _num_iterations = 5 * self . _num_episodes_per_iteration * self . _max_steps_per_episode dc . num_steps_buffer_preload = math . ceil ( self . _num_iterations / 10 ) tc . num_iterations = self . _num_iterations tc . num_iterations_between_eval = math . ceil ( tc . num_iterations / 3 ) tc . num_episodes_per_eval = self . _num_episodes_per_iteration tc . max_steps_per_episode = self . _max_steps_per_episode","title":"on_train_begin"},{"location":"reference/easyagents/callbacks/duration/#on_train_end","text":"def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\"","title":"on_train_end"},{"location":"reference/easyagents/callbacks/duration/#on_train_iteration_begin","text":"def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\"","title":"on_train_iteration_begin"},{"location":"reference/easyagents/callbacks/duration/#on_train_iteration_end","text":"def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : AgentContext ): \"\"\"Called once after the current iteration is completed\"\"\"","title":"on_train_iteration_end"},{"location":"reference/easyagents/callbacks/log/","text":"Module easyagents.callbacks.log View Source from typing import Tuple import logging import math from easyagents import core class _LogCallbackBase ( core . AgentCallback ): \"\"\"Base class for Callback loggers\"\"\" def __init__ ( self , logger : logging . Logger = None , prefix : str = None ): \"\"\"Writes all calls to logger with the given prefix. Args: logger: the logger to log (if None a new logger with level debug is created) prefix: a string written in front of each log msg \"\"\" self . _logger = logger if self . _logger is None : self . _logger = logging . getLogger () self . _prefix = prefix if self . _prefix is None : self . _prefix = '' def log ( self , msg_id : str , * args ): msg = self . _prefix + f '{msg_id:<25}' for arg in args : if arg is not None : msg += str ( arg ) + ' ' self . _logger . warning ( msg ) class _Callbacks ( _LogCallbackBase ): \"\"\"Logs all AgentCallback calls to a Logger\"\"\" def __init__ ( self , logger : logging . Logger = None , prefix : str = None ): \"\"\"Writes all calls to a callback function to logger with the given prefix. Args: logger: the logger to log (if None a new logger with level debug is created) prefix: a string written in front of each log msg \"\"\" super () . __init__ ( logger = logger , prefix = prefix ) def on_api_log ( self , agent_context : core . AgentContext , api_target : str , log_msg : str ): msg = f '{api_target:<30}' if log_msg : msg += ' ' + log_msg self . log ( 'on_api_log' , msg ) def on_log ( self , agent_context : core . AgentContext , log_msg : str ): self . log ( 'on_log' , log_msg ) def on_gym_init_begin ( self , agent_context : core . AgentContext ): self . log ( 'on_gym_init_begin' , agent_context ) def on_gym_init_end ( self , agent_context : core . AgentContext ): self . log ( 'on_gym_init_end' , agent_context ) def on_gym_reset_begin ( self , agent_context : core . AgentContext , ** kwargs ): self . log ( 'on_gym_reset_begin' , agent_context ) def on_gym_reset_end ( self , agent_context : core . AgentContext , reset_result : Tuple , ** kwargs ): self . log ( 'on_gym_reset_end' , agent_context ) def on_gym_step_begin ( self , agent_context : core . AgentContext , action ): self . log ( 'on_gym_step_begin' , agent_context ) def on_gym_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): self . log ( 'on_gym_step_end' , agent_context ) def on_play_begin ( self , agent_context : core . AgentContext ): self . log ( 'on_play_begin' , agent_context ) def on_play_end ( self , agent_context : core . AgentContext ): self . log ( 'on_play_end' , agent_context ) def on_play_episode_begin ( self , agent_context : core . AgentContext ): self . log ( 'on_play_episode_begin' , agent_context ) def on_play_episode_end ( self , agent_context : core . AgentContext ): self . log ( 'on_play_episode_end' , agent_context ) def on_play_step_begin ( self , agent_context : core . AgentContext , action ): self . log ( 'on_play_step_begin' , agent_context ) def on_play_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): self . log ( 'on_play_step_end' , agent_context ) def on_train_begin ( self , agent_context : core . AgentContext ): self . log ( 'on_train_begin' , agent_context ) def on_train_end ( self , agent_context : core . AgentContext ): self . log ( 'on_train_end' , agent_context ) def on_train_iteration_begin ( self , agent_context : core . AgentContext ): self . log ( 'on_train_iteration_begin' , agent_context ) def on_train_iteration_end ( self , agent_context : core . AgentContext ): self . log ( 'on_train_iteration_end' , agent_context ) class _AgentContext ( _LogCallbackBase ): \"\"\"Logs the agent context and its subcontexts after every training iteration / episode played \"\"\" def on_play_episode_end ( self , agent_context : core . AgentContext ): self . log ( str ( agent_context )) def on_train_iteration_end ( self , agent_context : core . AgentContext ): self . log ( str ( agent_context )) class _CallbackCounts ( core . AgentCallback ): def __init__ ( self ): self . gym_init_begin_count = 0 self . gym_init_end_count = 0 self . gym_reset_begin_count = 0 self . gym_reset_end_count = 0 self . gym_step_begin_count = 0 self . gym_step_end_count = 0 self . api_log_count = 0 self . log_count = 0 self . train_begin_count = 0 self . train_end_count = 0 self . train_iteration_begin_count = 0 self . train_iteration_end_count = 0 def __str__ ( self ): return f 'gym_init={self.gym_init_begin_count}:{self.gym_init_end_count} ' + \\ f 'gym_reset={self.gym_reset_begin_count}:{self.gym_reset_end_count} ' + \\ f 'gym_step={self.gym_step_begin_count}:{self.gym_step_end_count}' + \\ f 'train={self.train_begin_count}:{self.train_end_count} ' + \\ f 'train_iteration={self.train_iteration_begin_count}:{self.train_iteration_end_count}' + \\ f 'api_log={self.api_log_count} log={self.log_count} ' def on_api_log ( self , agent_context : core . AgentContext , api_target : str , log_msg : str ): self . api_log_count += 1 def on_log ( self , agent_context : core . AgentContext , log_msg : str ): self . log_count += 1 def on_gym_init_begin ( self , agent_context : core . AgentContext ): self . gym_init_begin_count += 1 def on_gym_init_end ( self , agent_context : core . AgentContext ): self . gym_init_end_count += 1 def on_gym_reset_begin ( self , agent_context : core . AgentContext , ** kwargs ): self . gym_reset_begin_count += 1 def on_gym_reset_end ( self , agent_context : core . AgentContext , reset_result : Tuple , ** kwargs ): self . gym_reset_end_count += 1 def on_gym_step_begin ( self , agent_context : core . AgentContext , action ): self . gym_step_begin_count += 1 def on_gym_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): self . gym_step_end_count += 1 def on_train_begin ( self , agent_context : core . AgentContext ): \"\"\"Called once at the entry of an agent.train() call. \"\"\" self . train_begin_count += 1 def on_train_end ( self , agent_context : core . AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\" self . train_end_count += 1 def on_train_iteration_begin ( self , agent_context : core . AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\" self . train_iteration_begin_count += 1 def on_train_iteration_end ( self , agent_context : core . AgentContext ): \"\"\"Called once after the current iteration is completed\"\"\" self . train_iteration_end_count += 1 class Agent ( _LogCallbackBase ): \"\"\"Logs agent activities to a python logger.\"\"\" def on_api_log ( self , agent_context : core . AgentContext , api_target : str , log_msg : str ): self . log ( api_target , log_msg ) def on_log ( self , agent_context : core . AgentContext , log_msg : str ): self . log ( log_msg ) class Duration ( _LogCallbackBase ): \"\"\"Logs training / play duration definition summary to a logger.\"\"\" def log_duration ( self , agent_context : core . AgentContext ): tc = agent_context . train msg = f 'num_iterations={tc.num_iterations} ' if isinstance ( tc , core . EpisodesTrainContext ): ec : core . EpisodesTrainContext = tc msg = msg + f 'num_episodes_per_iteration={ec.num_episodes_per_iteration} ' if isinstance ( tc , core . StepsTrainContext ): sc : core . StepsTrainContext = tc msg = msg + f 'num_steps_per_iteration={sc.num_steps_per_iteration} ' msg = msg + f 'num_max_steps_per_episode={tc.max_steps_per_episode} ' msg = msg + f 'num_iterations_between_plot={tc.num_iterations_between_plot} ' msg = msg + f 'num_iterations_between_eval={tc.num_iterations_between_eval} ' msg = msg + f 'num_episodes_per_eval={tc.num_episodes_per_eval} ' self . log ( f '{\"duration\":<25}{msg}' ) def on_train_begin ( self , agent_context : core . AgentContext ): self . log_duration ( agent_context ) class Iteration ( _LogCallbackBase ): \"\"\"Logs training iteration summaries to a python logger.\"\"\" def __init__ ( self , eval_only : bool = False , logger : logging . Logger = None , prefix : str = None ): \"\"\"Logs the completion of each training iteration. On an iteration with policy evaluation the current average reward/episode and steps/episode is logged as well. Args: eval_only: if set a log is only created if the policy was re-evaluated in the current iteration. logger: the logger to log (if None a new logger with level debug is created) prefix: a string written in front of each log msg \"\"\" self . _eval_only : bool = eval_only super () . __init__ ( logger = logger , prefix = prefix ) def log_iteration ( self , agent_context : core . AgentContext ): tc = agent_context . train e = tc . episodes_done_in_training if not self . _eval_only or ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ): msg = f 'episodes_done={e:<3} steps_done={tc.steps_done_in_training:<5} ' if e in tc . loss : loss = tc . loss [ e ] if not ( isinstance ( loss , float ) and math . isnan ( loss )): msg = msg + f 'loss={tc.loss[e]:<7.1f} ' if isinstance ( tc , core . PpoTrainContext ): msg = msg + f '[actor={tc.actor_loss[e]:<7.1f} ' msg = msg + f 'critic={tc.critic_loss[e]:<7.1f}] ' if e in tc . eval_rewards : r = tc . eval_rewards [ e ] msg = msg + f 'rewards=({r[0]:.1f},{r[1]:.1f},{r[2]:.1f}) ' if e in tc . eval_steps : s = tc . eval_steps [ e ] msg = msg + f 'steps=({s[0]:.1f},{s[1]:.1f},{s[2]:.1f}) ' prefix = f 'iteration {tc.iterations_done_in_training:<2} of {tc.num_iterations} ' self . log ( f '{prefix:<25}{msg}' ) def on_train_iteration_begin ( self , agent_context : core . AgentContext ): tc : core . TrainContext = agent_context . train # log the results of a pre-train evaluation (if existing) if ( 0 in tc . eval_rewards ) and \\ ( tc . episodes_done_in_training == 0 ) and \\ ( tc . iterations_done_in_training == 0 ): self . log_iteration ( agent_context ) def on_train_iteration_end ( self , agent_context : core . AgentContext ): self . log_iteration ( agent_context ) class Step ( _LogCallbackBase ): \"\"\"Logs each environment step to a python logger.\"\"\" def on_gym_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): prefix = '' monitor = agent_context . gym . _monitor_env if monitor : prefix = f '[{monitor.gym_env_name} {monitor.instance_id}:{monitor.episodes_done:<3}:' + \\ f '{monitor.steps_done_in_episode:<3}] ' tc : core . TrainContext = agent_context . train if tc : prefix += f 'train iteration={tc.iterations_done_in_training:<2} step={tc.steps_done_in_iteration:<4}' pc : core . PlayContext = agent_context . play if pc : prefix += f 'play episode={pc.episodes_done:<2} step={pc.steps_done_in_episode:<5} ' + \\ f 'sum_of_rewards={pc.sum_of_rewards[pc.episodes_done + 1]:<7.1f}' ( observation , reward , done , info ) = step_result msg = '' if info : msg = f ' info={msg}' self . log ( f '{prefix} reward={reward:<5.1f} done={str(done):5} action={action} observation={observation}{msg}' ) Classes Agent class Agent ( logger : logging . Logger = None , prefix : str = None ) Logs agent activities to a python logger. View Source class Agent ( _LogCallbackBase ): \"\"\"Logs agent activities to a python logger.\"\"\" def on_api_log ( self , agent_context: core . AgentContext , api_target: str , log_msg: str ): self . log ( api_target , log_msg ) def on_log ( self , agent_context: core . AgentContext , log_msg: str ): self . log ( log_msg ) Ancestors (in MRO) easyagents.callbacks.log._LogCallbackBase easyagents.core.AgentCallback abc.ABC Methods log def log ( self , msg_id : str , * args ) View Source def log ( self , msg_id : str , * args ): msg = self . _prefix + f '{msg_id:<25}' for arg in args : if arg is not None : msg += str ( arg ) + ' ' self . _logger . warning ( msg ) on_api_log def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : core . AgentContext , api_target : str , log_msg : str ): self . log ( api_target , log_msg ) on_gym_init_begin def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" on_gym_init_end def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass on_gym_reset_begin def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\" on_gym_reset_end def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass on_gym_step_begin def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass on_gym_step_end def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass on_log def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : core . AgentContext , log_msg : str ): self . log ( log_msg ) on_play_begin def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.play() call (during play or eval, but not during train). \"\"\" on_play_end def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.play() call (during play or eval, but not during train)\"\"\" on_play_episode_begin def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\" on_play_episode_end def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : AgentContext ): \"\"\"Called once after an episode is done or stopped (during play or eval, but not during train).\"\"\" on_play_step_begin def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\" on_play_step_end def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"Called once after a step is completed in the current episode (during play or eval, but not during train).\"\"\" on_train_begin def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.train() call. \"\"\" on_train_end def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\" on_train_iteration_begin def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\" on_train_iteration_end def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : AgentContext ): \"\"\"Called once after the current iteration is completed\"\"\" Duration class Duration ( logger : logging . Logger = None , prefix : str = None ) Logs training / play duration definition summary to a logger. View Source class Duration ( _LogCallbackBase ): \"\"\"Logs training / play duration definition summary to a logger.\"\"\" def log_duration ( self , agent_context: core . AgentContext ): tc = agent_context . train msg = f'num_iterations ={ tc . num_iterations } ' if isinstance(tc, core.EpisodesTrainContext): ec: core.EpisodesTrainContext = tc msg = msg + f' num_episodes_per_iteration ={ ec . num_episodes_per_iteration } ' if isinstance(tc, core.StepsTrainContext): sc: core.StepsTrainContext = tc msg = msg + f' num_steps_per_iteration ={ sc . num_steps_per_iteration } ' msg = msg + f' num_max_steps_per_episode ={ tc . max_steps_per_episode } ' msg = msg + f' num_iterations_between_plot ={ tc . num_iterations_between_plot } ' msg = msg + f' num_iterations_between_eval ={ tc . num_iterations_between_eval } ' msg = msg + f' num_episodes_per_eval ={ tc . num_episodes_per_eval } ' self.log(f' { \"duration\" :< 25 }{ msg }') def on_train_begin ( self , agent_context: core . AgentContext ): self . log_duration ( agent_context ) Ancestors (in MRO) easyagents.callbacks.log._LogCallbackBase easyagents.core.AgentCallback abc.ABC Methods log def log ( self , msg_id : str , * args ) View Source def log ( self , msg_id : str , * args ): msg = self . _prefix + f '{msg_id:<25}' for arg in args : if arg is not None : msg += str ( arg ) + ' ' self . _logger . warning ( msg ) log_duration def log_duration ( self , agent_context : easyagents . core . AgentContext ) View Source def log_duration ( self , agent_context : core . AgentContext ): tc = agent_context . train msg = f 'num_iterations={tc.num_iterations} ' if isinstance ( tc , core . EpisodesTrainContext ): ec : core . EpisodesTrainContext = tc msg = msg + f 'num_episodes_per_iteration={ec.num_episodes_per_iteration} ' if isinstance ( tc , core . StepsTrainContext ): sc : core . StepsTrainContext = tc msg = msg + f 'num_steps_per_iteration={sc.num_steps_per_iteration} ' msg = msg + f 'num_max_steps_per_episode={tc.max_steps_per_episode} ' msg = msg + f 'num_iterations_between_plot={tc.num_iterations_between_plot} ' msg = msg + f 'num_iterations_between_eval={tc.num_iterations_between_eval} ' msg = msg + f 'num_episodes_per_eval={tc.num_episodes_per_eval} ' self . log ( f '{\"duration\":<25}{msg}' ) on_api_log def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass on_gym_init_begin def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" on_gym_init_end def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass on_gym_reset_begin def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\" on_gym_reset_end def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass on_gym_step_begin def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass on_gym_step_end def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass on_log def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass on_play_begin def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.play() call (during play or eval, but not during train). \"\"\" on_play_end def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.play() call (during play or eval, but not during train)\"\"\" on_play_episode_begin def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\" on_play_episode_end def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : AgentContext ): \"\"\"Called once after an episode is done or stopped (during play or eval, but not during train).\"\"\" on_play_step_begin def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\" on_play_step_end def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"Called once after a step is completed in the current episode (during play or eval, but not during train).\"\"\" on_train_begin def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : core . AgentContext ): self . log_duration ( agent_context ) on_train_end def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\" on_train_iteration_begin def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\" on_train_iteration_end def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : AgentContext ): \"\"\"Called once after the current iteration is completed\"\"\" Iteration class Iteration ( eval_only : bool = False , logger : logging . Logger = None , prefix : str = None ) Logs training iteration summaries to a python logger. View Source class Iteration ( _LogCallbackBase ) : \"\"\"Logs training iteration summaries to a python logger.\"\"\" def __init__ ( self , eval_only : bool = False , logger : logging . Logger = None , prefix : str = None ) : \"\"\"Logs the completion of each training iteration. On an iteration with policy evaluation the current average reward/episode and steps/episode is logged as well. Args: eval_only: if set a log is only created if the policy was re-evaluated in the current iteration. logger: the logger to log (if None a new logger with level debug is created) prefix: a string written in front of each log msg \"\"\" self . _eval_only : bool = eval_only super (). __init__ ( logger = logger , prefix = prefix ) def log_iteration ( self , agent_context : core . AgentContext ) : tc = agent_context . train e = tc . episodes_done_in_training if not self . _eval_only or ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : msg = f 'episodes_done={e:<3} steps_done={tc.steps_done_in_training:<5} ' if e in tc . loss : loss = tc . loss [ e ] if not ( isinstance ( loss , float ) and math . isnan ( loss )) : msg = msg + f 'loss={tc.loss[e]:<7.1f} ' if isinstance ( tc , core . PpoTrainContext ) : msg = msg + f '[actor={tc.actor_loss[e]:<7.1f} ' msg = msg + f 'critic={tc.critic_loss[e]:<7.1f}] ' if e in tc . eval_rewards : r = tc . eval_rewards [ e ] msg = msg + f 'rewards=({r[0]:.1f},{r[1]:.1f},{r[2]:.1f}) ' if e in tc . eval_steps : s = tc . eval_steps [ e ] msg = msg + f 'steps=({s[0]:.1f},{s[1]:.1f},{s[2]:.1f}) ' prefix = f 'iteration {tc.iterations_done_in_training:<2} of {tc.num_iterations} ' self . log ( f '{prefix:<25}{msg}' ) def on_train_iteration_begin ( self , agent_context : core . AgentContext ) : tc : core . TrainContext = agent_context . train # log the results of a pre - train evaluation ( if existing ) if ( 0 in tc . eval_rewards ) and \\ ( tc . episodes_done_in_training == 0 ) and \\ ( tc . iterations_done_in_training == 0 ) : self . log_iteration ( agent_context ) def on_train_iteration_end ( self , agent_context : core . AgentContext ) : self . log_iteration ( agent_context ) Ancestors (in MRO) easyagents.callbacks.log._LogCallbackBase easyagents.core.AgentCallback abc.ABC Methods log def log ( self , msg_id : str , * args ) View Source def log ( self , msg_id : str , * args ): msg = self . _prefix + f '{msg_id:<25}' for arg in args : if arg is not None : msg += str ( arg ) + ' ' self . _logger . warning ( msg ) log_iteration def log_iteration ( self , agent_context : easyagents . core . AgentContext ) View Source def log_iteration ( self , agent_context : core . AgentContext ) : tc = agent_context . train e = tc . episodes_done_in_training if not self . _eval_only or ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : msg = f 'episodes_done={e:<3} steps_done={tc.steps_done_in_training:<5} ' if e in tc . loss : loss = tc . loss [ e ] if not ( isinstance ( loss , float ) and math . isnan ( loss )) : msg = msg + f 'loss={tc.loss[e]:<7.1f} ' if isinstance ( tc , core . PpoTrainContext ) : msg = msg + f '[actor={tc.actor_loss[e]:<7.1f} ' msg = msg + f 'critic={tc.critic_loss[e]:<7.1f}] ' if e in tc . eval_rewards : r = tc . eval_rewards [ e ] msg = msg + f 'rewards=({r[0]:.1f},{r[1]:.1f},{r[2]:.1f}) ' if e in tc . eval_steps : s = tc . eval_steps [ e ] msg = msg + f 'steps=({s[0]:.1f},{s[1]:.1f},{s[2]:.1f}) ' prefix = f 'iteration {tc.iterations_done_in_training:<2} of {tc.num_iterations} ' self . log ( f '{prefix:<25}{msg}' ) on_api_log def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass on_gym_init_begin def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" on_gym_init_end def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass on_gym_reset_begin def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\" on_gym_reset_end def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass on_gym_step_begin def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass on_gym_step_end def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass on_log def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass on_play_begin def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.play() call (during play or eval, but not during train). \"\"\" on_play_end def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.play() call (during play or eval, but not during train)\"\"\" on_play_episode_begin def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\" on_play_episode_end def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : AgentContext ): \"\"\"Called once after an episode is done or stopped (during play or eval, but not during train).\"\"\" on_play_step_begin def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\" on_play_step_end def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"Called once after a step is completed in the current episode (during play or eval, but not during train).\"\"\" on_train_begin def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.train() call. \"\"\" on_train_end def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\" on_train_iteration_begin def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : core . AgentContext ): tc : core . TrainContext = agent_context . train # log the results of a pre - train evaluation ( if existing ) if ( 0 in tc . eval_rewards ) and \\ ( tc . episodes_done_in_training == 0 ) and \\ ( tc . iterations_done_in_training == 0 ): self . log_iteration ( agent_context ) on_train_iteration_end def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : core . AgentContext ): self . log_iteration ( agent_context ) Step class Step ( logger : logging . Logger = None , prefix : str = None ) Logs each environment step to a python logger. View Source class Step ( _LogCallbackBase ): \"\"\"Logs each environment step to a python logger.\"\"\" def on_gym_step_end ( self , agent_context: core . AgentContext , action , step_result: Tuple ): prefix = '' monitor = agent_context . gym . _monitor_env if monitor: prefix = f' [{ monitor . gym_env_name } { monitor . instance_id }:{ monitor . episodes_done: < 3 }: ' + \\ f' { monitor . steps_done_in_episode: < 3 }] ' tc: core.TrainContext = agent_context.train if tc: prefix += f' train iteration ={ tc . iterations_done_in_training: < 2 } step ={ tc . steps_done_in_iteration: < 4 } ' pc: core.PlayContext = agent_context.play if pc: prefix += f' play episode ={ pc . episodes_done: < 2 } step ={ pc . steps_done_in_episode: < 5 } ' + \\ f' sum_of_rewards ={ pc . sum_of_rewards [ pc . episodes_done + 1 ]:< 7.1 f } ' (observation, reward, done, info) = step_result msg = '' if info: msg = f' info ={ msg } ' self.log(f' { prefix } reward ={ reward: < 5.1 f } done ={ str ( done ): 5 } action ={ action } observation ={ observation }{ msg }') Ancestors (in MRO) easyagents.callbacks.log._LogCallbackBase easyagents.core.AgentCallback abc.ABC Methods log def log ( self , msg_id : str , * args ) View Source def log ( self , msg_id : str , * args ): msg = self . _prefix + f '{msg_id:<25}' for arg in args : if arg is not None : msg += str ( arg ) + ' ' self . _logger . warning ( msg ) on_api_log def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass on_gym_init_begin def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" on_gym_init_end def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass on_gym_reset_begin def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\" on_gym_reset_end def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass on_gym_step_begin def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass on_gym_step_end def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): prefix = '' monitor = agent_context . gym . _monitor_env if monitor : prefix = f '[{monitor.gym_env_name} {monitor.instance_id}:{monitor.episodes_done:<3}:' + \\ f '{monitor.steps_done_in_episode:<3}] ' tc : core . TrainContext = agent_context . train if tc : prefix += f 'train iteration={tc.iterations_done_in_training:<2} step={tc.steps_done_in_iteration:<4}' pc : core . PlayContext = agent_context . play if pc : prefix += f 'play episode={pc.episodes_done:<2} step={pc.steps_done_in_episode:<5} ' + \\ f 'sum_of_rewards={pc.sum_of_rewards[pc.episodes_done + 1]:<7.1f}' ( observation , reward , done , info ) = step_result msg = '' if info : msg = f ' info={msg}' self . log ( f '{prefix} reward={reward:<5.1f} done={str(done):5} action={action} observation={observation}{msg}' ) on_log def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass on_play_begin def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.play() call (during play or eval, but not during train). \"\"\" on_play_end def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.play() call (during play or eval, but not during train)\"\"\" on_play_episode_begin def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\" on_play_episode_end def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : AgentContext ): \"\"\"Called once after an episode is done or stopped (during play or eval, but not during train).\"\"\" on_play_step_begin def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\" on_play_step_end def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"Called once after a step is completed in the current episode (during play or eval, but not during train).\"\"\" on_train_begin def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.train() call. \"\"\" on_train_end def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\" on_train_iteration_begin def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\" on_train_iteration_end def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : AgentContext ): \"\"\"Called once after the current iteration is completed\"\"\"","title":"Log"},{"location":"reference/easyagents/callbacks/log/#module-easyagentscallbackslog","text":"View Source from typing import Tuple import logging import math from easyagents import core class _LogCallbackBase ( core . AgentCallback ): \"\"\"Base class for Callback loggers\"\"\" def __init__ ( self , logger : logging . Logger = None , prefix : str = None ): \"\"\"Writes all calls to logger with the given prefix. Args: logger: the logger to log (if None a new logger with level debug is created) prefix: a string written in front of each log msg \"\"\" self . _logger = logger if self . _logger is None : self . _logger = logging . getLogger () self . _prefix = prefix if self . _prefix is None : self . _prefix = '' def log ( self , msg_id : str , * args ): msg = self . _prefix + f '{msg_id:<25}' for arg in args : if arg is not None : msg += str ( arg ) + ' ' self . _logger . warning ( msg ) class _Callbacks ( _LogCallbackBase ): \"\"\"Logs all AgentCallback calls to a Logger\"\"\" def __init__ ( self , logger : logging . Logger = None , prefix : str = None ): \"\"\"Writes all calls to a callback function to logger with the given prefix. Args: logger: the logger to log (if None a new logger with level debug is created) prefix: a string written in front of each log msg \"\"\" super () . __init__ ( logger = logger , prefix = prefix ) def on_api_log ( self , agent_context : core . AgentContext , api_target : str , log_msg : str ): msg = f '{api_target:<30}' if log_msg : msg += ' ' + log_msg self . log ( 'on_api_log' , msg ) def on_log ( self , agent_context : core . AgentContext , log_msg : str ): self . log ( 'on_log' , log_msg ) def on_gym_init_begin ( self , agent_context : core . AgentContext ): self . log ( 'on_gym_init_begin' , agent_context ) def on_gym_init_end ( self , agent_context : core . AgentContext ): self . log ( 'on_gym_init_end' , agent_context ) def on_gym_reset_begin ( self , agent_context : core . AgentContext , ** kwargs ): self . log ( 'on_gym_reset_begin' , agent_context ) def on_gym_reset_end ( self , agent_context : core . AgentContext , reset_result : Tuple , ** kwargs ): self . log ( 'on_gym_reset_end' , agent_context ) def on_gym_step_begin ( self , agent_context : core . AgentContext , action ): self . log ( 'on_gym_step_begin' , agent_context ) def on_gym_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): self . log ( 'on_gym_step_end' , agent_context ) def on_play_begin ( self , agent_context : core . AgentContext ): self . log ( 'on_play_begin' , agent_context ) def on_play_end ( self , agent_context : core . AgentContext ): self . log ( 'on_play_end' , agent_context ) def on_play_episode_begin ( self , agent_context : core . AgentContext ): self . log ( 'on_play_episode_begin' , agent_context ) def on_play_episode_end ( self , agent_context : core . AgentContext ): self . log ( 'on_play_episode_end' , agent_context ) def on_play_step_begin ( self , agent_context : core . AgentContext , action ): self . log ( 'on_play_step_begin' , agent_context ) def on_play_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): self . log ( 'on_play_step_end' , agent_context ) def on_train_begin ( self , agent_context : core . AgentContext ): self . log ( 'on_train_begin' , agent_context ) def on_train_end ( self , agent_context : core . AgentContext ): self . log ( 'on_train_end' , agent_context ) def on_train_iteration_begin ( self , agent_context : core . AgentContext ): self . log ( 'on_train_iteration_begin' , agent_context ) def on_train_iteration_end ( self , agent_context : core . AgentContext ): self . log ( 'on_train_iteration_end' , agent_context ) class _AgentContext ( _LogCallbackBase ): \"\"\"Logs the agent context and its subcontexts after every training iteration / episode played \"\"\" def on_play_episode_end ( self , agent_context : core . AgentContext ): self . log ( str ( agent_context )) def on_train_iteration_end ( self , agent_context : core . AgentContext ): self . log ( str ( agent_context )) class _CallbackCounts ( core . AgentCallback ): def __init__ ( self ): self . gym_init_begin_count = 0 self . gym_init_end_count = 0 self . gym_reset_begin_count = 0 self . gym_reset_end_count = 0 self . gym_step_begin_count = 0 self . gym_step_end_count = 0 self . api_log_count = 0 self . log_count = 0 self . train_begin_count = 0 self . train_end_count = 0 self . train_iteration_begin_count = 0 self . train_iteration_end_count = 0 def __str__ ( self ): return f 'gym_init={self.gym_init_begin_count}:{self.gym_init_end_count} ' + \\ f 'gym_reset={self.gym_reset_begin_count}:{self.gym_reset_end_count} ' + \\ f 'gym_step={self.gym_step_begin_count}:{self.gym_step_end_count}' + \\ f 'train={self.train_begin_count}:{self.train_end_count} ' + \\ f 'train_iteration={self.train_iteration_begin_count}:{self.train_iteration_end_count}' + \\ f 'api_log={self.api_log_count} log={self.log_count} ' def on_api_log ( self , agent_context : core . AgentContext , api_target : str , log_msg : str ): self . api_log_count += 1 def on_log ( self , agent_context : core . AgentContext , log_msg : str ): self . log_count += 1 def on_gym_init_begin ( self , agent_context : core . AgentContext ): self . gym_init_begin_count += 1 def on_gym_init_end ( self , agent_context : core . AgentContext ): self . gym_init_end_count += 1 def on_gym_reset_begin ( self , agent_context : core . AgentContext , ** kwargs ): self . gym_reset_begin_count += 1 def on_gym_reset_end ( self , agent_context : core . AgentContext , reset_result : Tuple , ** kwargs ): self . gym_reset_end_count += 1 def on_gym_step_begin ( self , agent_context : core . AgentContext , action ): self . gym_step_begin_count += 1 def on_gym_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): self . gym_step_end_count += 1 def on_train_begin ( self , agent_context : core . AgentContext ): \"\"\"Called once at the entry of an agent.train() call. \"\"\" self . train_begin_count += 1 def on_train_end ( self , agent_context : core . AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\" self . train_end_count += 1 def on_train_iteration_begin ( self , agent_context : core . AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\" self . train_iteration_begin_count += 1 def on_train_iteration_end ( self , agent_context : core . AgentContext ): \"\"\"Called once after the current iteration is completed\"\"\" self . train_iteration_end_count += 1 class Agent ( _LogCallbackBase ): \"\"\"Logs agent activities to a python logger.\"\"\" def on_api_log ( self , agent_context : core . AgentContext , api_target : str , log_msg : str ): self . log ( api_target , log_msg ) def on_log ( self , agent_context : core . AgentContext , log_msg : str ): self . log ( log_msg ) class Duration ( _LogCallbackBase ): \"\"\"Logs training / play duration definition summary to a logger.\"\"\" def log_duration ( self , agent_context : core . AgentContext ): tc = agent_context . train msg = f 'num_iterations={tc.num_iterations} ' if isinstance ( tc , core . EpisodesTrainContext ): ec : core . EpisodesTrainContext = tc msg = msg + f 'num_episodes_per_iteration={ec.num_episodes_per_iteration} ' if isinstance ( tc , core . StepsTrainContext ): sc : core . StepsTrainContext = tc msg = msg + f 'num_steps_per_iteration={sc.num_steps_per_iteration} ' msg = msg + f 'num_max_steps_per_episode={tc.max_steps_per_episode} ' msg = msg + f 'num_iterations_between_plot={tc.num_iterations_between_plot} ' msg = msg + f 'num_iterations_between_eval={tc.num_iterations_between_eval} ' msg = msg + f 'num_episodes_per_eval={tc.num_episodes_per_eval} ' self . log ( f '{\"duration\":<25}{msg}' ) def on_train_begin ( self , agent_context : core . AgentContext ): self . log_duration ( agent_context ) class Iteration ( _LogCallbackBase ): \"\"\"Logs training iteration summaries to a python logger.\"\"\" def __init__ ( self , eval_only : bool = False , logger : logging . Logger = None , prefix : str = None ): \"\"\"Logs the completion of each training iteration. On an iteration with policy evaluation the current average reward/episode and steps/episode is logged as well. Args: eval_only: if set a log is only created if the policy was re-evaluated in the current iteration. logger: the logger to log (if None a new logger with level debug is created) prefix: a string written in front of each log msg \"\"\" self . _eval_only : bool = eval_only super () . __init__ ( logger = logger , prefix = prefix ) def log_iteration ( self , agent_context : core . AgentContext ): tc = agent_context . train e = tc . episodes_done_in_training if not self . _eval_only or ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ): msg = f 'episodes_done={e:<3} steps_done={tc.steps_done_in_training:<5} ' if e in tc . loss : loss = tc . loss [ e ] if not ( isinstance ( loss , float ) and math . isnan ( loss )): msg = msg + f 'loss={tc.loss[e]:<7.1f} ' if isinstance ( tc , core . PpoTrainContext ): msg = msg + f '[actor={tc.actor_loss[e]:<7.1f} ' msg = msg + f 'critic={tc.critic_loss[e]:<7.1f}] ' if e in tc . eval_rewards : r = tc . eval_rewards [ e ] msg = msg + f 'rewards=({r[0]:.1f},{r[1]:.1f},{r[2]:.1f}) ' if e in tc . eval_steps : s = tc . eval_steps [ e ] msg = msg + f 'steps=({s[0]:.1f},{s[1]:.1f},{s[2]:.1f}) ' prefix = f 'iteration {tc.iterations_done_in_training:<2} of {tc.num_iterations} ' self . log ( f '{prefix:<25}{msg}' ) def on_train_iteration_begin ( self , agent_context : core . AgentContext ): tc : core . TrainContext = agent_context . train # log the results of a pre-train evaluation (if existing) if ( 0 in tc . eval_rewards ) and \\ ( tc . episodes_done_in_training == 0 ) and \\ ( tc . iterations_done_in_training == 0 ): self . log_iteration ( agent_context ) def on_train_iteration_end ( self , agent_context : core . AgentContext ): self . log_iteration ( agent_context ) class Step ( _LogCallbackBase ): \"\"\"Logs each environment step to a python logger.\"\"\" def on_gym_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): prefix = '' monitor = agent_context . gym . _monitor_env if monitor : prefix = f '[{monitor.gym_env_name} {monitor.instance_id}:{monitor.episodes_done:<3}:' + \\ f '{monitor.steps_done_in_episode:<3}] ' tc : core . TrainContext = agent_context . train if tc : prefix += f 'train iteration={tc.iterations_done_in_training:<2} step={tc.steps_done_in_iteration:<4}' pc : core . PlayContext = agent_context . play if pc : prefix += f 'play episode={pc.episodes_done:<2} step={pc.steps_done_in_episode:<5} ' + \\ f 'sum_of_rewards={pc.sum_of_rewards[pc.episodes_done + 1]:<7.1f}' ( observation , reward , done , info ) = step_result msg = '' if info : msg = f ' info={msg}' self . log ( f '{prefix} reward={reward:<5.1f} done={str(done):5} action={action} observation={observation}{msg}' )","title":"Module easyagents.callbacks.log"},{"location":"reference/easyagents/callbacks/log/#classes","text":"","title":"Classes"},{"location":"reference/easyagents/callbacks/log/#agent","text":"class Agent ( logger : logging . Logger = None , prefix : str = None ) Logs agent activities to a python logger. View Source class Agent ( _LogCallbackBase ): \"\"\"Logs agent activities to a python logger.\"\"\" def on_api_log ( self , agent_context: core . AgentContext , api_target: str , log_msg: str ): self . log ( api_target , log_msg ) def on_log ( self , agent_context: core . AgentContext , log_msg: str ): self . log ( log_msg )","title":"Agent"},{"location":"reference/easyagents/callbacks/log/#ancestors-in-mro","text":"easyagents.callbacks.log._LogCallbackBase easyagents.core.AgentCallback abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/callbacks/log/#methods","text":"","title":"Methods"},{"location":"reference/easyagents/callbacks/log/#log","text":"def log ( self , msg_id : str , * args ) View Source def log ( self , msg_id : str , * args ): msg = self . _prefix + f '{msg_id:<25}' for arg in args : if arg is not None : msg += str ( arg ) + ' ' self . _logger . warning ( msg )","title":"log"},{"location":"reference/easyagents/callbacks/log/#on_api_log","text":"def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : core . AgentContext , api_target : str , log_msg : str ): self . log ( api_target , log_msg )","title":"on_api_log"},{"location":"reference/easyagents/callbacks/log/#on_gym_init_begin","text":"def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\"","title":"on_gym_init_begin"},{"location":"reference/easyagents/callbacks/log/#on_gym_init_end","text":"def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass","title":"on_gym_init_end"},{"location":"reference/easyagents/callbacks/log/#on_gym_reset_begin","text":"def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\"","title":"on_gym_reset_begin"},{"location":"reference/easyagents/callbacks/log/#on_gym_reset_end","text":"def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass","title":"on_gym_reset_end"},{"location":"reference/easyagents/callbacks/log/#on_gym_step_begin","text":"def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass","title":"on_gym_step_begin"},{"location":"reference/easyagents/callbacks/log/#on_gym_step_end","text":"def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass","title":"on_gym_step_end"},{"location":"reference/easyagents/callbacks/log/#on_log","text":"def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : core . AgentContext , log_msg : str ): self . log ( log_msg )","title":"on_log"},{"location":"reference/easyagents/callbacks/log/#on_play_begin","text":"def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.play() call (during play or eval, but not during train). \"\"\"","title":"on_play_begin"},{"location":"reference/easyagents/callbacks/log/#on_play_end","text":"def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.play() call (during play or eval, but not during train)\"\"\"","title":"on_play_end"},{"location":"reference/easyagents/callbacks/log/#on_play_episode_begin","text":"def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\"","title":"on_play_episode_begin"},{"location":"reference/easyagents/callbacks/log/#on_play_episode_end","text":"def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : AgentContext ): \"\"\"Called once after an episode is done or stopped (during play or eval, but not during train).\"\"\"","title":"on_play_episode_end"},{"location":"reference/easyagents/callbacks/log/#on_play_step_begin","text":"def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\"","title":"on_play_step_begin"},{"location":"reference/easyagents/callbacks/log/#on_play_step_end","text":"def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"Called once after a step is completed in the current episode (during play or eval, but not during train).\"\"\"","title":"on_play_step_end"},{"location":"reference/easyagents/callbacks/log/#on_train_begin","text":"def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.train() call. \"\"\"","title":"on_train_begin"},{"location":"reference/easyagents/callbacks/log/#on_train_end","text":"def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\"","title":"on_train_end"},{"location":"reference/easyagents/callbacks/log/#on_train_iteration_begin","text":"def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\"","title":"on_train_iteration_begin"},{"location":"reference/easyagents/callbacks/log/#on_train_iteration_end","text":"def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : AgentContext ): \"\"\"Called once after the current iteration is completed\"\"\"","title":"on_train_iteration_end"},{"location":"reference/easyagents/callbacks/log/#duration","text":"class Duration ( logger : logging . Logger = None , prefix : str = None ) Logs training / play duration definition summary to a logger. View Source class Duration ( _LogCallbackBase ): \"\"\"Logs training / play duration definition summary to a logger.\"\"\" def log_duration ( self , agent_context: core . AgentContext ): tc = agent_context . train msg = f'num_iterations ={ tc . num_iterations } ' if isinstance(tc, core.EpisodesTrainContext): ec: core.EpisodesTrainContext = tc msg = msg + f' num_episodes_per_iteration ={ ec . num_episodes_per_iteration } ' if isinstance(tc, core.StepsTrainContext): sc: core.StepsTrainContext = tc msg = msg + f' num_steps_per_iteration ={ sc . num_steps_per_iteration } ' msg = msg + f' num_max_steps_per_episode ={ tc . max_steps_per_episode } ' msg = msg + f' num_iterations_between_plot ={ tc . num_iterations_between_plot } ' msg = msg + f' num_iterations_between_eval ={ tc . num_iterations_between_eval } ' msg = msg + f' num_episodes_per_eval ={ tc . num_episodes_per_eval } ' self.log(f' { \"duration\" :< 25 }{ msg }') def on_train_begin ( self , agent_context: core . AgentContext ): self . log_duration ( agent_context )","title":"Duration"},{"location":"reference/easyagents/callbacks/log/#ancestors-in-mro_1","text":"easyagents.callbacks.log._LogCallbackBase easyagents.core.AgentCallback abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/callbacks/log/#methods_1","text":"","title":"Methods"},{"location":"reference/easyagents/callbacks/log/#log_1","text":"def log ( self , msg_id : str , * args ) View Source def log ( self , msg_id : str , * args ): msg = self . _prefix + f '{msg_id:<25}' for arg in args : if arg is not None : msg += str ( arg ) + ' ' self . _logger . warning ( msg )","title":"log"},{"location":"reference/easyagents/callbacks/log/#log_duration","text":"def log_duration ( self , agent_context : easyagents . core . AgentContext ) View Source def log_duration ( self , agent_context : core . AgentContext ): tc = agent_context . train msg = f 'num_iterations={tc.num_iterations} ' if isinstance ( tc , core . EpisodesTrainContext ): ec : core . EpisodesTrainContext = tc msg = msg + f 'num_episodes_per_iteration={ec.num_episodes_per_iteration} ' if isinstance ( tc , core . StepsTrainContext ): sc : core . StepsTrainContext = tc msg = msg + f 'num_steps_per_iteration={sc.num_steps_per_iteration} ' msg = msg + f 'num_max_steps_per_episode={tc.max_steps_per_episode} ' msg = msg + f 'num_iterations_between_plot={tc.num_iterations_between_plot} ' msg = msg + f 'num_iterations_between_eval={tc.num_iterations_between_eval} ' msg = msg + f 'num_episodes_per_eval={tc.num_episodes_per_eval} ' self . log ( f '{\"duration\":<25}{msg}' )","title":"log_duration"},{"location":"reference/easyagents/callbacks/log/#on_api_log_1","text":"def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass","title":"on_api_log"},{"location":"reference/easyagents/callbacks/log/#on_gym_init_begin_1","text":"def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\"","title":"on_gym_init_begin"},{"location":"reference/easyagents/callbacks/log/#on_gym_init_end_1","text":"def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass","title":"on_gym_init_end"},{"location":"reference/easyagents/callbacks/log/#on_gym_reset_begin_1","text":"def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\"","title":"on_gym_reset_begin"},{"location":"reference/easyagents/callbacks/log/#on_gym_reset_end_1","text":"def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass","title":"on_gym_reset_end"},{"location":"reference/easyagents/callbacks/log/#on_gym_step_begin_1","text":"def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass","title":"on_gym_step_begin"},{"location":"reference/easyagents/callbacks/log/#on_gym_step_end_1","text":"def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass","title":"on_gym_step_end"},{"location":"reference/easyagents/callbacks/log/#on_log_1","text":"def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass","title":"on_log"},{"location":"reference/easyagents/callbacks/log/#on_play_begin_1","text":"def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.play() call (during play or eval, but not during train). \"\"\"","title":"on_play_begin"},{"location":"reference/easyagents/callbacks/log/#on_play_end_1","text":"def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.play() call (during play or eval, but not during train)\"\"\"","title":"on_play_end"},{"location":"reference/easyagents/callbacks/log/#on_play_episode_begin_1","text":"def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\"","title":"on_play_episode_begin"},{"location":"reference/easyagents/callbacks/log/#on_play_episode_end_1","text":"def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : AgentContext ): \"\"\"Called once after an episode is done or stopped (during play or eval, but not during train).\"\"\"","title":"on_play_episode_end"},{"location":"reference/easyagents/callbacks/log/#on_play_step_begin_1","text":"def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\"","title":"on_play_step_begin"},{"location":"reference/easyagents/callbacks/log/#on_play_step_end_1","text":"def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"Called once after a step is completed in the current episode (during play or eval, but not during train).\"\"\"","title":"on_play_step_end"},{"location":"reference/easyagents/callbacks/log/#on_train_begin_1","text":"def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : core . AgentContext ): self . log_duration ( agent_context )","title":"on_train_begin"},{"location":"reference/easyagents/callbacks/log/#on_train_end_1","text":"def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\"","title":"on_train_end"},{"location":"reference/easyagents/callbacks/log/#on_train_iteration_begin_1","text":"def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\"","title":"on_train_iteration_begin"},{"location":"reference/easyagents/callbacks/log/#on_train_iteration_end_1","text":"def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : AgentContext ): \"\"\"Called once after the current iteration is completed\"\"\"","title":"on_train_iteration_end"},{"location":"reference/easyagents/callbacks/log/#iteration","text":"class Iteration ( eval_only : bool = False , logger : logging . Logger = None , prefix : str = None ) Logs training iteration summaries to a python logger. View Source class Iteration ( _LogCallbackBase ) : \"\"\"Logs training iteration summaries to a python logger.\"\"\" def __init__ ( self , eval_only : bool = False , logger : logging . Logger = None , prefix : str = None ) : \"\"\"Logs the completion of each training iteration. On an iteration with policy evaluation the current average reward/episode and steps/episode is logged as well. Args: eval_only: if set a log is only created if the policy was re-evaluated in the current iteration. logger: the logger to log (if None a new logger with level debug is created) prefix: a string written in front of each log msg \"\"\" self . _eval_only : bool = eval_only super (). __init__ ( logger = logger , prefix = prefix ) def log_iteration ( self , agent_context : core . AgentContext ) : tc = agent_context . train e = tc . episodes_done_in_training if not self . _eval_only or ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : msg = f 'episodes_done={e:<3} steps_done={tc.steps_done_in_training:<5} ' if e in tc . loss : loss = tc . loss [ e ] if not ( isinstance ( loss , float ) and math . isnan ( loss )) : msg = msg + f 'loss={tc.loss[e]:<7.1f} ' if isinstance ( tc , core . PpoTrainContext ) : msg = msg + f '[actor={tc.actor_loss[e]:<7.1f} ' msg = msg + f 'critic={tc.critic_loss[e]:<7.1f}] ' if e in tc . eval_rewards : r = tc . eval_rewards [ e ] msg = msg + f 'rewards=({r[0]:.1f},{r[1]:.1f},{r[2]:.1f}) ' if e in tc . eval_steps : s = tc . eval_steps [ e ] msg = msg + f 'steps=({s[0]:.1f},{s[1]:.1f},{s[2]:.1f}) ' prefix = f 'iteration {tc.iterations_done_in_training:<2} of {tc.num_iterations} ' self . log ( f '{prefix:<25}{msg}' ) def on_train_iteration_begin ( self , agent_context : core . AgentContext ) : tc : core . TrainContext = agent_context . train # log the results of a pre - train evaluation ( if existing ) if ( 0 in tc . eval_rewards ) and \\ ( tc . episodes_done_in_training == 0 ) and \\ ( tc . iterations_done_in_training == 0 ) : self . log_iteration ( agent_context ) def on_train_iteration_end ( self , agent_context : core . AgentContext ) : self . log_iteration ( agent_context )","title":"Iteration"},{"location":"reference/easyagents/callbacks/log/#ancestors-in-mro_2","text":"easyagents.callbacks.log._LogCallbackBase easyagents.core.AgentCallback abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/callbacks/log/#methods_2","text":"","title":"Methods"},{"location":"reference/easyagents/callbacks/log/#log_2","text":"def log ( self , msg_id : str , * args ) View Source def log ( self , msg_id : str , * args ): msg = self . _prefix + f '{msg_id:<25}' for arg in args : if arg is not None : msg += str ( arg ) + ' ' self . _logger . warning ( msg )","title":"log"},{"location":"reference/easyagents/callbacks/log/#log_iteration","text":"def log_iteration ( self , agent_context : easyagents . core . AgentContext ) View Source def log_iteration ( self , agent_context : core . AgentContext ) : tc = agent_context . train e = tc . episodes_done_in_training if not self . _eval_only or ( tc . iterations_done_in_training % tc . num_iterations_between_eval == 0 ) : msg = f 'episodes_done={e:<3} steps_done={tc.steps_done_in_training:<5} ' if e in tc . loss : loss = tc . loss [ e ] if not ( isinstance ( loss , float ) and math . isnan ( loss )) : msg = msg + f 'loss={tc.loss[e]:<7.1f} ' if isinstance ( tc , core . PpoTrainContext ) : msg = msg + f '[actor={tc.actor_loss[e]:<7.1f} ' msg = msg + f 'critic={tc.critic_loss[e]:<7.1f}] ' if e in tc . eval_rewards : r = tc . eval_rewards [ e ] msg = msg + f 'rewards=({r[0]:.1f},{r[1]:.1f},{r[2]:.1f}) ' if e in tc . eval_steps : s = tc . eval_steps [ e ] msg = msg + f 'steps=({s[0]:.1f},{s[1]:.1f},{s[2]:.1f}) ' prefix = f 'iteration {tc.iterations_done_in_training:<2} of {tc.num_iterations} ' self . log ( f '{prefix:<25}{msg}' )","title":"log_iteration"},{"location":"reference/easyagents/callbacks/log/#on_api_log_2","text":"def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass","title":"on_api_log"},{"location":"reference/easyagents/callbacks/log/#on_gym_init_begin_2","text":"def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\"","title":"on_gym_init_begin"},{"location":"reference/easyagents/callbacks/log/#on_gym_init_end_2","text":"def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass","title":"on_gym_init_end"},{"location":"reference/easyagents/callbacks/log/#on_gym_reset_begin_2","text":"def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\"","title":"on_gym_reset_begin"},{"location":"reference/easyagents/callbacks/log/#on_gym_reset_end_2","text":"def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass","title":"on_gym_reset_end"},{"location":"reference/easyagents/callbacks/log/#on_gym_step_begin_2","text":"def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass","title":"on_gym_step_begin"},{"location":"reference/easyagents/callbacks/log/#on_gym_step_end_2","text":"def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass","title":"on_gym_step_end"},{"location":"reference/easyagents/callbacks/log/#on_log_2","text":"def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass","title":"on_log"},{"location":"reference/easyagents/callbacks/log/#on_play_begin_2","text":"def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.play() call (during play or eval, but not during train). \"\"\"","title":"on_play_begin"},{"location":"reference/easyagents/callbacks/log/#on_play_end_2","text":"def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.play() call (during play or eval, but not during train)\"\"\"","title":"on_play_end"},{"location":"reference/easyagents/callbacks/log/#on_play_episode_begin_2","text":"def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\"","title":"on_play_episode_begin"},{"location":"reference/easyagents/callbacks/log/#on_play_episode_end_2","text":"def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : AgentContext ): \"\"\"Called once after an episode is done or stopped (during play or eval, but not during train).\"\"\"","title":"on_play_episode_end"},{"location":"reference/easyagents/callbacks/log/#on_play_step_begin_2","text":"def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\"","title":"on_play_step_begin"},{"location":"reference/easyagents/callbacks/log/#on_play_step_end_2","text":"def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"Called once after a step is completed in the current episode (during play or eval, but not during train).\"\"\"","title":"on_play_step_end"},{"location":"reference/easyagents/callbacks/log/#on_train_begin_2","text":"def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.train() call. \"\"\"","title":"on_train_begin"},{"location":"reference/easyagents/callbacks/log/#on_train_end_2","text":"def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\"","title":"on_train_end"},{"location":"reference/easyagents/callbacks/log/#on_train_iteration_begin_2","text":"def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : core . AgentContext ): tc : core . TrainContext = agent_context . train # log the results of a pre - train evaluation ( if existing ) if ( 0 in tc . eval_rewards ) and \\ ( tc . episodes_done_in_training == 0 ) and \\ ( tc . iterations_done_in_training == 0 ): self . log_iteration ( agent_context )","title":"on_train_iteration_begin"},{"location":"reference/easyagents/callbacks/log/#on_train_iteration_end_2","text":"def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : core . AgentContext ): self . log_iteration ( agent_context )","title":"on_train_iteration_end"},{"location":"reference/easyagents/callbacks/log/#step","text":"class Step ( logger : logging . Logger = None , prefix : str = None ) Logs each environment step to a python logger. View Source class Step ( _LogCallbackBase ): \"\"\"Logs each environment step to a python logger.\"\"\" def on_gym_step_end ( self , agent_context: core . AgentContext , action , step_result: Tuple ): prefix = '' monitor = agent_context . gym . _monitor_env if monitor: prefix = f' [{ monitor . gym_env_name } { monitor . instance_id }:{ monitor . episodes_done: < 3 }: ' + \\ f' { monitor . steps_done_in_episode: < 3 }] ' tc: core.TrainContext = agent_context.train if tc: prefix += f' train iteration ={ tc . iterations_done_in_training: < 2 } step ={ tc . steps_done_in_iteration: < 4 } ' pc: core.PlayContext = agent_context.play if pc: prefix += f' play episode ={ pc . episodes_done: < 2 } step ={ pc . steps_done_in_episode: < 5 } ' + \\ f' sum_of_rewards ={ pc . sum_of_rewards [ pc . episodes_done + 1 ]:< 7.1 f } ' (observation, reward, done, info) = step_result msg = '' if info: msg = f' info ={ msg } ' self.log(f' { prefix } reward ={ reward: < 5.1 f } done ={ str ( done ): 5 } action ={ action } observation ={ observation }{ msg }')","title":"Step"},{"location":"reference/easyagents/callbacks/log/#ancestors-in-mro_3","text":"easyagents.callbacks.log._LogCallbackBase easyagents.core.AgentCallback abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/callbacks/log/#methods_3","text":"","title":"Methods"},{"location":"reference/easyagents/callbacks/log/#log_3","text":"def log ( self , msg_id : str , * args ) View Source def log ( self , msg_id : str , * args ): msg = self . _prefix + f '{msg_id:<25}' for arg in args : if arg is not None : msg += str ( arg ) + ' ' self . _logger . warning ( msg )","title":"log"},{"location":"reference/easyagents/callbacks/log/#on_api_log_3","text":"def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass","title":"on_api_log"},{"location":"reference/easyagents/callbacks/log/#on_gym_init_begin_3","text":"def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\"","title":"on_gym_init_begin"},{"location":"reference/easyagents/callbacks/log/#on_gym_init_end_3","text":"def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass","title":"on_gym_init_end"},{"location":"reference/easyagents/callbacks/log/#on_gym_reset_begin_3","text":"def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\"","title":"on_gym_reset_begin"},{"location":"reference/easyagents/callbacks/log/#on_gym_reset_end_3","text":"def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass","title":"on_gym_reset_end"},{"location":"reference/easyagents/callbacks/log/#on_gym_step_begin_3","text":"def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass","title":"on_gym_step_begin"},{"location":"reference/easyagents/callbacks/log/#on_gym_step_end_3","text":"def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): prefix = '' monitor = agent_context . gym . _monitor_env if monitor : prefix = f '[{monitor.gym_env_name} {monitor.instance_id}:{monitor.episodes_done:<3}:' + \\ f '{monitor.steps_done_in_episode:<3}] ' tc : core . TrainContext = agent_context . train if tc : prefix += f 'train iteration={tc.iterations_done_in_training:<2} step={tc.steps_done_in_iteration:<4}' pc : core . PlayContext = agent_context . play if pc : prefix += f 'play episode={pc.episodes_done:<2} step={pc.steps_done_in_episode:<5} ' + \\ f 'sum_of_rewards={pc.sum_of_rewards[pc.episodes_done + 1]:<7.1f}' ( observation , reward , done , info ) = step_result msg = '' if info : msg = f ' info={msg}' self . log ( f '{prefix} reward={reward:<5.1f} done={str(done):5} action={action} observation={observation}{msg}' )","title":"on_gym_step_end"},{"location":"reference/easyagents/callbacks/log/#on_log_3","text":"def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass","title":"on_log"},{"location":"reference/easyagents/callbacks/log/#on_play_begin_3","text":"def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.play() call (during play or eval, but not during train). \"\"\"","title":"on_play_begin"},{"location":"reference/easyagents/callbacks/log/#on_play_end_3","text":"def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.play() call (during play or eval, but not during train)\"\"\"","title":"on_play_end"},{"location":"reference/easyagents/callbacks/log/#on_play_episode_begin_3","text":"def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\"","title":"on_play_episode_begin"},{"location":"reference/easyagents/callbacks/log/#on_play_episode_end_3","text":"def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : AgentContext ): \"\"\"Called once after an episode is done or stopped (during play or eval, but not during train).\"\"\"","title":"on_play_episode_end"},{"location":"reference/easyagents/callbacks/log/#on_play_step_begin_3","text":"def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\"","title":"on_play_step_begin"},{"location":"reference/easyagents/callbacks/log/#on_play_step_end_3","text":"def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"Called once after a step is completed in the current episode (during play or eval, but not during train).\"\"\"","title":"on_play_step_end"},{"location":"reference/easyagents/callbacks/log/#on_train_begin_3","text":"def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.train() call. \"\"\"","title":"on_train_begin"},{"location":"reference/easyagents/callbacks/log/#on_train_end_3","text":"def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\"","title":"on_train_end"},{"location":"reference/easyagents/callbacks/log/#on_train_iteration_begin_3","text":"def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\"","title":"on_train_iteration_begin"},{"location":"reference/easyagents/callbacks/log/#on_train_iteration_end_3","text":"def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : AgentContext ): \"\"\"Called once after the current iteration is completed\"\"\"","title":"on_train_iteration_end"},{"location":"reference/easyagents/callbacks/plot/","text":"Module easyagents.callbacks.plot View Source from typing import Optional , List , Tuple , Union , Dict import easyagents.core as core import easyagents.backends.core import base64 import matplotlib.pyplot as plt import numpy as np import imageio import math import gym import os.path # avoid \"double rendering\" of the final jupyter output on_play_end_clear_jupyter_display : bool = True on_train_end_clear_jupyter_display : bool = True # check if we are running in Jupyter, if so interactive plotting must be handled differently # (in order to get plot updates during training) _is_jupyter_active = False try : # noinspection PyUnresolvedReferences from IPython import get_ipython # noinspection PyUnresolvedReferences from IPython.display import display , clear_output # noinspection PyUnresolvedReferences from IPython.display import HTML shell = get_ipython () . __class__ . __name__ if shell == 'ZMQInteractiveShell' : _is_jupyter_active = True else : # noinspection PyPackageRequirements import google.colab _is_jupyter_active = True except ImportError : pass class _PreProcess ( core . _PreProcessCallback ): \"\"\"Initializes the matplotlib agent_context.pyplot.figure\"\"\" def _setup ( self , agent_context : core . AgentContext ): # create figure / remove all existing axes from previous calls to train/play pyc = agent_context . pyplot pyc . is_jupyter_active = _is_jupyter_active if pyc . figure is None : pyc . figure = plt . figure ( \"_EasyAgents\" , figsize = pyc . figsize ) for ax in pyc . figure . axes : pyc . figure . delaxes ( ax ) def on_play_begin ( self , agent_context : core . AgentContext ): # play_begin is also called at the start of a policy evaluation if agent_context . is_play : self . _setup ( agent_context = agent_context ) def on_train_begin ( self , agent_context : core . AgentContext ): self . _setup ( agent_context = agent_context ) class _PostProcess ( core . _PostProcessCallback ): \"\"\"Redraws the plots int the matplotlib agent_context.figure. In jupyter the plots are only refreshed once per second. \"\"\" def __init__ ( self ): self . _call_jupyter_display : bool self . _reset () def _clear_jupyter_plots ( self , agent_context : core . AgentContext , wait = True ): \"\"\"Clears the content in the current jupyter output cell. NoOp if not in jupyter or no plot output. Args: wait: Wait to clear the output until new output is available to replace it. \"\"\" # don't clear the jupyter output if no plot is present, may clear the log output otherwise if agent_context . pyplot . is_jupyter_active and self . _plot_exists ( agent_context ): clear_output ( wait = wait ) def _display_plots ( self , agent_context : core . AgentContext ): \"\"\"Fixes the layout of multiple subplots and refreshs the display.\"\"\" pyc = agent_context . pyplot if self . _plot_exists ( agent_context ): count = len ( pyc . figure . axes ) rows = math . ceil ( count / pyc . max_columns ) columns = math . ceil ( count / rows ) for i in range ( count ): pyc . figure . axes [ i ] . change_geometry ( rows , columns , i + 1 ) pyc . figure . tight_layout () if pyc . is_jupyter_active : self . _clear_jupyter_plots ( agent_context ) if self . _call_jupyter_display : # noinspection PyTypeChecker display ( pyc . figure ) self . _call_jupyter_display = True else : plt . pause ( 0.01 ) def _plot_exists ( self , agent_context : core . AgentContext ): \"\"\"Yields true if at least 1 jupyter plot exists.\"\"\" pyc = agent_context . pyplot count = len ( pyc . figure . axes ) result = count > 0 return result def _reset ( self ): self . _call_jupyter_display = False def on_play_begin ( self , agent_context : core . AgentContext ): if agent_context . is_play : self . _reset () def on_train_begin ( self , agent_context : core . AgentContext ): self . _reset () def on_play_episode_end ( self , agent_context : core . AgentContext ): if agent_context . _is_plot_ready ( core . PlotType . PLAY_EPISODE ): self . _display_plots ( agent_context ) def on_play_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): if agent_context . _is_plot_ready ( core . PlotType . PLAY_STEP ): self . _display_plots ( agent_context ) def on_play_end ( self , agent_context : core . AgentContext ): if agent_context . _is_plot_ready ( core . PlotType . TRAIN_EVAL ): self . _display_plots ( agent_context ) if agent_context . is_play : self . _display_plots ( agent_context ) if on_play_end_clear_jupyter_display : self . _clear_jupyter_plots ( agent_context , wait = False ) def on_train_end ( self , agent_context : core . AgentContext ): self . _display_plots ( agent_context ) if on_train_end_clear_jupyter_display : self . _clear_jupyter_plots ( agent_context , wait = False ) def on_train_iteration_begin ( self , agent_context : core . AgentContext ): # display initial evaluation before training starts. if agent_context . train . iterations_done_in_training == 0 and \\ agent_context . _is_plot_ready ( core . PlotType . TRAIN_EVAL ): self . _display_plots ( agent_context ) def on_train_iteration_end ( self , agent_context : core . AgentContext ): if agent_context . _is_plot_ready ( core . PlotType . TRAIN_ITERATION ): self . _display_plots ( agent_context ) # noinspection DuplicatedCode class _PlotCallback ( core . AgentCallback ): \"\"\"Base class of plyplot callbacks generating a plot after a trained iteration or an episode played. Attributes: axes: the subplot to plot onto \"\"\" def __init__ ( self , plot_type : core . PlotType ): \"\"\"Base class of plyplot callbacks generating a plot after a trained iteration or an episode played. Args: plot_type: point in time when the plot is updated \"\"\" self . axes = None self . axes_color : str = 'grey' self . _plot_type : core . PlotType = plot_type def _create_subplot ( self , agent_context : core . AgentContext ): if self . axes is None : pyc = agent_context . pyplot pyc . _created_subplots = pyc . _created_subplots | self . _plot_type count = len ( pyc . figure . axes ) + 1 rows = math . ceil ( count / pyc . max_columns ) columns = math . ceil ( count / rows ) self . axes = pyc . figure . add_subplot ( rows , columns , count ) self . plot_axes ( xlim = ( 0 , 1 ), ylabel = '' , xlabel = '' ) def _is_nan ( self , values : Optional [ List [ float ]]): \"\"\"yields true if all values are equal to nan. yields false if values is None or empty.\"\"\" result = False if values and all ( isinstance ( v , float ) for v in values ): result = all ( math . isnan ( v ) for v in values ) return result def _refresh_subplot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ): \"\"\"Sets this axes active and calls plot if this plot callback is registered on at least 1 plot out of plot_type.\"\"\" assert self . axes is not None plot_type = plot_type & self . _plot_type if agent_context . _is_plot_ready ( plot_type ): pyc = agent_context . pyplot if not pyc . is_jupyter_active : plt . figure ( pyc . figure . number ) if plt . gcf () is pyc . figure : plt . sca ( self . axes ) self . plot ( agent_context , plot_type ) def clear_plot ( self , agent_context : core . AgentContext ): \"\"\"Clears the axes for this plot. Should be called by self.plot before replotting an axes.\"\"\" assert self . axes is not None pyc = agent_context . pyplot if pyc . is_jupyter_active : self . axes . cla () else : plt . cla () def on_play_begin ( self , agent_context : core . AgentContext ): if ( self . _plot_type & ( core . PlotType . PLAY_EPISODE | core . PlotType . PLAY_STEP )) != core . PlotType . NONE : self . _create_subplot ( agent_context ) def on_play_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_EVAL ) def on_play_episode_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_EPISODE ) def on_play_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_STEP ) def on_train_begin ( self , agent_context : core . AgentContext ): if ( self . _plot_type & ( core . PlotType . TRAIN_EVAL | core . PlotType . TRAIN_ITERATION )) != core . PlotType . NONE : self . _create_subplot ( agent_context ) def on_train_iteration_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_ITERATION ) def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ): \"\"\"Plots a graph on the self.axes object. Args: agent_context: the context containing the data to plot plot_type: the PlotType requesting this plot call. \"\"\" pass def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = 'linear' , ylim : Optional [ Tuple [ float , float ]] = None ): \"\"\"Draws the x- and y-axes. Attributes: xlim: None or (min,max) for the x-axes xlabel: label of the x-axes ylim: None or (min,max) for the y-axes (or None) ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log', ...) \"\"\" assert xlim is None or xlim [ 0 ] <= xlim [ 1 ] assert ylim is None or ylim [ 0 ] <= ylim [ 1 ] assert xlabel is not None assert ylabel is not None # setup subplot (axes, labels, colors) axes_color = self . axes_color self . axes . set_xlabel ( xlabel ) self . axes . set_ylabel ( ylabel ) self . axes . spines [ 'top' ] . set_visible ( False ) self . axes . spines [ 'right' ] . set_visible ( False ) self . axes . spines [ 'bottom' ] . set_color ( axes_color ) self . axes . spines [ 'left' ] . set_color ( axes_color ) self . axes . grid ( color = axes_color , linestyle = '-' , linewidth = 0.25 , alpha = 0.5 ) if xlim is not None : self . axes . set_xlim ( xlim ) if ylim is not None : self . axes . set_ylim ( ylim ) self . axes . set_yscale ( yscale ) def plot_text ( self , text : str ): if text : ax = self . axes ax . text ( 0.5 , 0.5 , text , horizontalalignment = 'center' , verticalalignment = 'center' , color = 'blue' , wrap = True ) ax . set_xlabel ( '' ) ax . get_xaxis () . set_ticks ([]) ax . get_yaxis () . set_ticks ([]) axes_color = self . axes_color for spin in ax . spines : ax . spines [ spin ] . set_visible ( True ) ax . spines [ spin ] . set_color ( axes_color ) def plot_subplot ( self , agent_context : core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], ylabel : str , xlabel : str = None , xlim : Optional [ Tuple [ float , float ]] = None , yscale : str = 'linear' , ylim : Optional [ Tuple [ float , float ]] = None , color : str = 'blue' ): \"\"\"Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) ylim: (min,max) for the x-axes xlabel: label of the x-axes ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log',...) ylim: (min,max) for the y-axes (or None) color: the graphs color (must be the name of a matplotlib color) \"\"\" if not xlim : xmin = 0 xmax = 1 if agent_context . is_play : xmin = 1 xmax = agent_context . play . episodes_done if agent_context . is_train or agent_context . is_eval : xmin = 0 xmax = agent_context . train . episodes_done_in_training xlim = ( xmin , xmax ) if xlabel is None : xlabel = 'episodes' if agent_context . is_play : xlabel = 'episodes played' if agent_context . is_eval or agent_context . is_train : xlabel = 'episodes trained' self . clear_plot ( agent_context ) self . plot_axes ( xlim = xlim , xlabel = xlabel , ylabel = ylabel , yscale = yscale , ylim = ylim ) self . plot_values ( agent_context = agent_context , xvalues = xvalues , yvalues = yvalues , color = color ) def plot_values ( self , agent_context : core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], color : Optional [ str ] = 'blue' , marker : str = None , pause : bool = True ): \"\"\"Draws the graph given by xvalues, yvalues. Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) color: the graphs color (must be the name of a matplotlib color) or None marker: if None a marker is used if the graph has just a few points. pause: pause to redraw the plot. \"\"\" assert xvalues is not None assert yvalues is not None assert len ( xvalues ) == len ( yvalues ), \"xvalues do not match yvalues\" pyc : core . PyPlotContext = agent_context . pyplot # extract min / max and y values if yvalues is of the form [(min,y,max),...) yminvalues = None ymaxvalues = None if len ( yvalues ) > 0 and isinstance ( yvalues [ 0 ], tuple ): ymaxvalues = [ t [ 2 ] for t in yvalues ] yminvalues = [ t [ 0 ] for t in yvalues ] yvalues = [ t [ 1 ] for t in yvalues ] # plot values marker = marker if marker is None and len ( xvalues ) < 10 : marker = 'o' fill_alpha = 0.1 if pyc . is_jupyter_active : if yminvalues is not None : self . axes . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) self . axes . plot ( xvalues , yvalues , color = color , marker = marker ) else : if yminvalues is not None : plt . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) plt . plot ( xvalues , yvalues , color = color , marker = marker ) if pause : plt . pause ( 0.01 ) # noinspection DuplicatedCode class Actions ( _PlotCallback ): def __init__ ( self , num_steps_between_plot = 100 ): \"\"\"Plots a histogram of the actions taken during play or during the last evaluation period. Args: num_steps_between_plot: num of steps to play before plot is updated. \"\"\" super () . __init__ ( plot_type = core . PlotType . PLAY_STEP | core . PlotType . PLAY_EPISODE | core . PlotType . TRAIN_EVAL ) assert num_steps_between_plot > 0 self . _actions : List [ float ] = [] self . _num_steps_between_plot = num_steps_between_plot def _reset ( self ): self . _actions : List [ float ] = [] def on_play_begin ( self , agent_context : core . AgentContext ): super () . on_play_begin ( agent_context ) self . _reset () def on_play_end ( self , agent_context : core . AgentContext ): super () . on_play_end ( agent_context ) self . _reset () def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ): try : pc = agent_context . play is_plot = True xlabel = 'actions' if plot_type & core . PlotType . TRAIN_EVAL != core . PlotType . NONE : xlabel = 'actions taken during last evaluation period' if plot_type & core . PlotType . PLAY_STEP != core . PlotType . NONE : is_plot = ( pc . steps_done_in_episode % self . _num_steps_between_plot == 0 ) if is_plot : self . clear_plot ( agent_context ) self . plot_axes ( xlabel = xlabel , ylabel = 'count' ) self . axes . hist ( pc . actions . values ()) except : self . plot_text ( f 'Failed to create the actions histogram. \\n ' ) class Clear ( core . AgentCallback ): \"\"\"Configures the clearing of plots in the jupyter output cell after calls to train or play.\"\"\" def __init__ ( self , on_play : bool = True , on_train : bool = True ): \"\"\"Define the cell clearing behaviour after agent.train and agent.play. Args: on_play: if set the output cell is cleared after agent.play if a plot exists on_train: if set the output cell is cleared after agent.train if a plot exists \"\"\" self . _on_play : bool = on_play self . _on_train : bool = on_train on_play_end_clear_jupyter_display = self . _on_play on_train_end_clear_jupyter_display = self . _on_train def on_play_begin ( self , agent_context : core . AgentContext ): global on_play_end_clear_jupyter_display on_play_end_clear_jupyter_display = self . _on_play def on_train_begin ( self , agent_context : core . AgentContext ): global on_train_end_clear_jupyter_display on_train_end_clear_jupyter_display = self . _on_train class Loss ( _PlotCallback ): def __init__ ( self , yscale : str = 'symlog' , ylim : Optional [ Tuple [ float , float ]] = None ): \"\"\"Plots the loss resulting from each iterations policy training. Hints: o for actro-critic agents the loss from training the actor- and critic-networks are plotted along with the total loss. Args: yscale: scale of the y-axes ('linear', 'symlog',...) ylim: (min,max) for the y-axes \"\"\" super () . __init__ ( plot_type = core . PlotType . TRAIN_ITERATION ) self . ylim = ylim self . yscale = yscale def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ): ac = agent_context tc = ac . train xvalues = list ( tc . loss . keys ()) self . clear_plot ( agent_context ) lossvalues = list ( tc . loss . values ()) if self . _is_nan ( lossvalues ): self . plot_text ( 'no loss data available' ) else : self . plot_axes ( xlim = ( 0 , tc . episodes_done_in_training ), xlabel = 'episodes trained' , ylim = self . ylim , ylabel = 'loss' , yscale = self . yscale ) if isinstance ( tc , core . PpoTrainContext ): acc : core . PpoTrainContext = tc self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = lossvalues , color = 'indigo' , pause = False ) self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = list ( acc . actor_loss . values ()), color = 'g' , pause = False ) self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = list ( acc . critic_loss . values ()), color = 'b' ) self . axes . legend (( 'total' , 'actor' , 'critic' )) else : self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = lossvalues , color = 'indigo' ) class Rewards ( _PlotCallback ): def __init__ ( self , yscale : str = 'linear' , ylim : Optional [ Tuple [ float , float ]] = None ): \"\"\"Plots the sum of rewards observed during policy evaluation. Args: yscale: scale of the y-axes ('linear', 'symlog',...) ylim: (min,max) for the y-axes \"\"\" super () . __init__ ( core . PlotType . TRAIN_ITERATION | core . PlotType . TRAIN_EVAL | core . PlotType . PLAY_EPISODE ) self . ylim = ylim self . yscale = yscale def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ): xvalues = yvalues = [] ylabel = 'sum of rewards' if agent_context . is_train or agent_context . is_eval : tc = agent_context . train xvalues = list ( tc . eval_rewards . keys ()) yvalues = list ( tc . eval_rewards . values ()) ylabel = '\u00d8 sum of rewards' if agent_context . is_play : pc = agent_context . play xvalues = list ( pc . sum_of_rewards . keys ()) yvalues = list ( pc . sum_of_rewards . values ()) if xvalues : self . plot_subplot ( agent_context , color = 'green' , ylim = self . ylim , yscale = self . yscale , xvalues = xvalues , yvalues = yvalues , ylabel = ylabel ) class State ( _PlotCallback ): \"\"\"Renders the gym state as a plot to the pyplot figure using gym.render('rgb_array'). During training only the last state of the last game evaluation is plotted. During play all states are plotted. \"\"\" def __init__ ( self , mode = 'rgb_array' ): \"\"\" Args: mode: the render mode passed to gym.render(), yielding an rgb_array \"\"\" super () . __init__ ( plot_type = core . PlotType . PLAY_STEP | core . PlotType . TRAIN_EVAL ) self . _render_mode = mode def _plot_rgb_array ( self , agent_context : core . AgentContext , rgb_array : np . ndarray ): \"\"\"Renders rgb_array to the current subplot.\"\"\" assert rgb_array is not None ax = self . axes xlabel = '' if agent_context . is_eval : xlabel = \"'done state' of last evaluation episode\" ax . imshow ( rgb_array ) ax . set_xlabel ( xlabel ) ax . get_xaxis () . set_ticks ([]) ax . get_yaxis () . set_ticks ([]) axes_color = self . axes_color for spin in ax . spines : ax . spines [ spin ] . set_visible ( True ) ax . spines [ spin ] . set_color ( axes_color ) # noinspection PyArgumentList,DuplicatedCode def _render_to_rgb_array ( self , gym_env : gym . Env , mode : str ) -> np . ndarray : \"\"\" calls gym_env.render(mode) and validates the return value to be a numpy rgb array throws an exception if not an rgb array Returns: numpy rgb array \"\"\" result = gym_env . render ( mode = mode ) assert result is not None , f 'gym_env.render(mode= {mode} ) yielded None' assert isinstance ( result , np . ndarray ), f 'gym_env.render(mode= {mode} ) did not yield a numpy.ndarray.' assert result . min () >= 0 , f 'gym_env.render(mode= {mode} ) contains negative values => not an rgb array' assert result . max () <= 255 , f 'gym_env.render(mode= {mode} ) contains values > 255 => not an rgb array' assert len ( result . shape ) == 3 , f 'gym_env.render(mode= {mode} ) shape is not of the form (x,y,n)' assert result . shape [ 2 ] == 3 or result . shape [ 2 ] == 4 , \\ f 'gym_env.render(mode= {mode} ) shape is not of the form (x,y,3|4)' return result def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ): self . clear_plot ( agent_context ) try : rgb_array : np . ndarray = self . _render_to_rgb_array ( agent_context . play . gym_env , self . _render_mode ) self . _plot_rgb_array ( agent_context , rgb_array ) except : self . plot_text ( f 'gym.Env.render(mode=\" {self._render_mode} \") failed: \\n ' ) class Steps ( _PlotCallback ): def __init__ ( self , yscale : str = 'linear' , ylim : Optional [ Tuple [ float , float ]] = None ): \"\"\"Plots the step counts observed during policy evaluation. Args: yscale: scale of the y-axes ('linear','log') ylim: (min,max) for the y-axes \"\"\" super () . __init__ ( core . PlotType . TRAIN_ITERATION | core . PlotType . TRAIN_EVAL | core . PlotType . PLAY_EPISODE ) self . ylim = ylim self . yscale = yscale def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ): xvalues = yvalues = [] ylabel = 'steps' if agent_context . is_train or agent_context . is_eval : tc = agent_context . train xvalues = list ( tc . eval_steps . keys ()) yvalues = list ( tc . eval_steps . values ()) ylabel = '\u00d8 steps' if agent_context . is_play : pc = agent_context . play xvalues = list ( pc . actions . keys ()) yvalues = [ len ( pc . actions [ episode ]) for episode in pc . actions . keys ()] self . plot_subplot ( agent_context , color = 'blue' , ylim = self . ylim , yscale = self . yscale , xvalues = xvalues , yvalues = yvalues , ylabel = ylabel ) class StepRewards ( _PlotCallback ): def __init__ ( self , num_steps_between_plot = 100 ): \"\"\"Plots the sum of rewards up to the current step during play or at the end of an evaluation period. Args: num_steps_between_plot: num of steps to play before plot is updated. \"\"\" super () . __init__ ( core . PlotType . PLAY_STEP | core . PlotType . PLAY_EPISODE | core . PlotType . TRAIN_EVAL ) assert num_steps_between_plot > 0 self . _xy_values : Dict [ int , Tuple [ List [ int ], List [ float ]]] = dict () self . _xmax : int = 0 self . _num_steps_between_plot = num_steps_between_plot def _replot ( self , agent_context : core . AgentContext ): if self . _xmax >= 1 : self . clear_plot ( agent_context ) xlabel = 'steps played' if agent_context . is_eval : xlabel = 'steps taken during last evaluation period' self . plot_axes ( xlim = ( 1 , self . _xmax ), ylabel = 'sum of rewards' , xlabel = xlabel ) xy_values = list ( self . _xy_values . values ()) xlast , _ = xy_values [ - 1 ] for xvalues , yvalues in xy_values : pause = ( xvalues == xlast ) self . plot_values ( agent_context = agent_context , xvalues = xvalues , yvalues = yvalues , color = None , marker = '' , pause = pause ) def _reset ( self ): self . _xy_values = dict () self . _xmax = 0 def on_play_begin ( self , agent_context : core . AgentContext ): super () . on_play_begin ( agent_context ) self . _reset () def on_play_end ( self , agent_context : core . AgentContext ): super () . on_play_end ( agent_context ) self . _reset () def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ): pc = agent_context . play if plot_type & core . PlotType . PLAY_STEP != core . PlotType . NONE : episode = pc . episodes_done + 1 if episode not in self . _xy_values : self . _xy_values [ episode ] = ([], []) xvalues , yvalues = self . _xy_values [ episode ] xvalues . append ( pc . steps_done_in_episode ) yvalues . append ( pc . sum_of_rewards [ episode ]) if pc . steps_done_in_episode > self . _xmax : self . _xmax = pc . steps_done_in_episode if ( episode == 1 and pc . steps_done == 1 ) or ( pc . steps_done % self . _num_steps_between_plot ) == 0 : self . _replot ( agent_context ) if plot_type & core . PlotType . PLAY_EPISODE != core . PlotType . NONE : self . _replot ( agent_context ) if plot_type & core . PlotType . TRAIN_EVAL != core . PlotType . NONE : self . _xmax = max ([ len ( step_rewards ) for step_rewards in pc . rewards . values ()]) for episode in pc . rewards . keys (): step_rewards = pc . rewards [ episode ] xvalues = list ( range ( 1 , len ( step_rewards ) + 1 )) yvalues = [] for reward in step_rewards : old_sum = yvalues [ - 1 ] if len ( yvalues ) > 0 else 0 yvalues . append ( old_sum + reward ) self . _xy_values [ episode ] = ( xvalues , yvalues ) self . _replot ( agent_context ) class ToMovie ( core . _PostProcessCallback ): \"\"\"Plots the pyplot figure to an mp4 file Attributes: fps: frame per seconds filepath: the filepath of the mp4 file. \"\"\" def __init__ ( self , fps : Optional [ int ] = None , filepath : str = None ): \"\"\"Writes the ploted graphs and images to the mp4 / gif file given by filepath. if filepath ends in '.gif' an animated gif is created. Args: fps: frames per second filepath: the filepath of the mp4 or gif file file. If None the file is written to a temp file. \"\"\" super () . __init__ () self . fps = fps self . _is_filepath_set = filepath is not None self . filepath = filepath if not self . _is_filepath_set : self . filepath = easyagents . backends . core . _get_temp_path () if ( not self . _is_animated_gif ()) and ( not self . filepath . lower () . endswith ( '.mp4' )): self . filepath = self . filepath + '.mp4' self . _video = imageio . get_writer ( self . filepath , fps = fps ) if fps else imageio . get_writer ( self . filepath ) def _close ( self , agent_context : core . AgentContext ): \"\"\"closes the mp4 file and displays it in jupyter cell (if in a jupyter notebook)\"\"\" self . _video . close () self . _video = None if agent_context . pyplot . is_jupyter_active : with open ( self . filepath , 'rb' ) as f : video = f . read () b64 = base64 . b64encode ( video ) if not self . _is_filepath_set : os . remove ( self . filepath ) width = 640 height = 480 if self . _is_animated_gif (): result = ''' <img src=\"data:image/gif;base64, {2} \" alt=\"easyagents.plot\" width= {0} /> ''' . format ( width , height , b64 . decode ()) else : result = ''' <video width=\" {0} \" height=\" {1} \" controls> <source src=\"data:video/mp4;base64, {2} \" type=\"video/mp4\"> Your browser does not support the video tag. </video>''' . format ( width , height , b64 . decode ()) result = HTML ( result ) # noinspection PyTypeChecker clear_output ( wait = True ) # noinspection PyTypeChecker display ( result ) def _get_rgb_array ( self , agent_context : core . AgentContext ) -> np . ndarray : \"\"\"Yields an rgb array representing the current content of the subplots.\"\"\" pyc = agent_context . pyplot pyc . figure . canvas . draw () result = np . frombuffer ( pyc . figure . canvas . tostring_rgb (), dtype = 'uint8' ) result = result . reshape ( pyc . figure . canvas . get_width_height ()[:: - 1 ] + ( 3 ,)) return result def _is_animated_gif ( self ): return self . filepath . lower () . endswith ( '.gif' ) def _write_figure_to_video ( self , agent_context : core . AgentContext ): \"\"\"Appends the current pyplot figure to the video. if an exception occures no frame is added. \"\"\" try : rgb_array = self . _get_rgb_array ( agent_context ) self . _video . append_data ( rgb_array ) except : pass def on_play_episode_end ( self , agent_context : core . AgentContext ): if agent_context . _is_plot_ready ( core . PlotType . PLAY_EPISODE | core . PlotType . TRAIN_EVAL ): self . _write_figure_to_video ( agent_context ) def on_play_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): if agent_context . _is_plot_ready ( core . PlotType . PLAY_STEP ): self . _write_figure_to_video ( agent_context ) def on_train_iteration_end ( self , agent_context : core . AgentContext ): if agent_context . _is_plot_ready ( core . PlotType . TRAIN_ITERATION ): self . _write_figure_to_video ( agent_context ) def on_play_end ( self , agent_context : core . AgentContext ): if agent_context . _is_plot_ready ( core . PlotType . PLAY_EPISODE ): self . _close ( agent_context ) def on_train_end ( self , agent_context : core . AgentContext ): self . _close ( agent_context ) Variables on_play_end_clear_jupyter_display on_train_end_clear_jupyter_display shell Classes Actions class Actions ( num_steps_between_plot = 100 ) Base class of plyplot callbacks generating a plot after a trained iteration or an episode played. Attributes: axes: the subplot to plot onto View Source class Actions ( _PlotCallback ) : def __init__ ( self , num_steps_between_plot = 100 ) : \"\"\"Plots a histogram of the actions taken during play or during the last evaluation period. Args: num_steps_between_plot: num of steps to play before plot is updated. \"\"\" super (). __init__ ( plot_type = core . PlotType . PLAY_STEP | core . PlotType . PLAY_EPISODE | core . PlotType . TRAIN_EVAL ) assert num_steps_between_plot > 0 self . _actions : List [ float ] = [] self . _num_steps_between_plot = num_steps_between_plot def _reset ( self ) : self . _actions : List [ float ] = [] def on_play_begin ( self , agent_context : core . AgentContext ) : super (). on_play_begin ( agent_context ) self . _reset () def on_play_end ( self , agent_context : core . AgentContext ) : super (). on_play_end ( agent_context ) self . _reset () def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ) : try : pc = agent_context . play is_plot = True xlabel = 'actions' if plot_type & core . PlotType . TRAIN_EVAL != core . PlotType . NONE : xlabel = 'actions taken during last evaluation period' if plot_type & core . PlotType . PLAY_STEP != core . PlotType . NONE : is_plot = ( pc . steps_done_in_episode % self . _num_steps_between_plot == 0 ) if is_plot : self . clear_plot ( agent_context ) self . plot_axes ( xlabel = xlabel , ylabel = 'count' ) self . axes . hist ( pc . actions . values ()) except : self . plot_text ( f 'Failed to create the actions histogram.\\n' ) Ancestors (in MRO) easyagents.callbacks.plot._PlotCallback easyagents.core.AgentCallback abc.ABC Methods clear_plot def clear_plot ( self , agent_context : easyagents . core . AgentContext ) Clears the axes for this plot. Should be called by self.plot before replotting an axes. View Source def clear_plot ( self , agent_context : core . AgentContext ): \"\"\"Clears the axes for this plot. Should be called by self.plot before replotting an axes.\"\"\" assert self . axes is not None pyc = agent_context . pyplot if pyc . is_jupyter_active : self . axes . cla () else : plt . cla () on_api_log def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass on_gym_init_begin def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" on_gym_init_end def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass on_gym_reset_begin def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\" on_gym_reset_end def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass on_gym_step_begin def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass on_gym_step_end def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass on_log def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass on_play_begin def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : core . AgentContext ): super (). on_play_begin ( agent_context ) self . _reset () on_play_end def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : core . AgentContext ): super (). on_play_end ( agent_context ) self . _reset () on_play_episode_begin def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\" on_play_episode_end def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_EPISODE ) on_play_step_begin def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\" on_play_step_end def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_STEP ) on_train_begin def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : core . AgentContext ): if ( self . _plot_type & ( core . PlotType . TRAIN_EVAL | core . PlotType . TRAIN_ITERATION )) != core . PlotType . NONE : self . _create_subplot ( agent_context ) on_train_end def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\" on_train_iteration_begin def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\" on_train_iteration_end def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_ITERATION ) plot def plot ( self , agent_context : easyagents . core . AgentContext , plot_type : easyagents . core . PlotType ) Plots a graph on the self.axes object. Args: agent_context: the context containing the data to plot plot_type: the PlotType requesting this plot call. View Source def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ): try : pc = agent_context . play is_plot = True xlabel = 'actions' if plot_type & core . PlotType . TRAIN_EVAL != core . PlotType . NONE : xlabel = 'actions taken during last evaluation period' if plot_type & core . PlotType . PLAY_STEP != core . PlotType . NONE : is_plot = ( pc . steps_done_in_episode % self . _num_steps_between_plot == 0 ) if is_plot : self . clear_plot ( agent_context ) self . plot_axes ( xlabel = xlabel , ylabel = 'count' ) self . axes . hist ( pc . actions . values ()) except : self . plot_text ( f 'Failed to create the actions histogram.\\n' ) plot_axes def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None ) Draws the x- and y-axes. Attributes: xlim: None or (min,max) for the x-axes xlabel: label of the x-axes ylim: None or (min,max) for the y-axes (or None) ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log', ...) View Source def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = 'linear' , ylim : Optional [ Tuple [ float , float ]] = None ): \"\"\"Draws the x- and y-axes. Attributes: xlim: None or (min,max) for the x-axes xlabel: label of the x-axes ylim: None or (min,max) for the y-axes (or None) ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log', ...) \"\"\" assert xlim is None or xlim [ 0 ] <= xlim [ 1 ] assert ylim is None or ylim [ 0 ] <= ylim [ 1 ] assert xlabel is not None assert ylabel is not None # setup subplot ( axes , labels , colors ) axes_color = self . axes_color self . axes . set_xlabel ( xlabel ) self . axes . set_ylabel ( ylabel ) self . axes . spines [ 'top' ]. set_visible ( False ) self . axes . spines [ 'right' ]. set_visible ( False ) self . axes . spines [ 'bottom' ]. set_color ( axes_color ) self . axes . spines [ 'left' ]. set_color ( axes_color ) self . axes . grid ( color = axes_color , linestyle = '-' , linewidth = 0 . 25 , alpha = 0 . 5 ) if xlim is not None : self . axes . set_xlim ( xlim ) if ylim is not None : self . axes . set_ylim ( ylim ) self . axes . set_yscale ( yscale ) plot_subplot def plot_subplot ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], ylabel : str , xlabel : str = None , xlim : Union [ Tuple [ float , float ], NoneType ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None , color : str = 'blue' ) Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) ylim: (min,max) for the x-axes xlabel: label of the x-axes ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log',...) ylim: (min,max) for the y-axes (or None) color: the graphs color (must be the name of a matplotlib color) View Source def plot_subplot ( self , agent_context : core . AgentContext , xvalues : List [ int ] , yvalues : List [ Union[float, Tuple[float, float, float ] ]] , ylabel : str , xlabel : str = None , xlim : Optional [ Tuple[float, float ] ] = None , yscale : str = 'linear' , ylim : Optional [ Tuple[float, float ] ] = None , color : str = 'blue' ) : \"\"\"Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) ylim: (min,max) for the x-axes xlabel: label of the x-axes ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log',...) ylim: (min,max) for the y-axes (or None) color: the graphs color (must be the name of a matplotlib color) \"\"\" if not xlim : xmin = 0 xmax = 1 if agent_context . is_play : xmin = 1 xmax = agent_context . play . episodes_done if agent_context . is_train or agent_context . is_eval : xmin = 0 xmax = agent_context . train . episodes_done_in_training xlim = ( xmin , xmax ) if xlabel is None : xlabel = 'episodes' if agent_context . is_play : xlabel = 'episodes played' if agent_context . is_eval or agent_context . is_train : xlabel = 'episodes trained' self . clear_plot ( agent_context ) self . plot_axes ( xlim = xlim , xlabel = xlabel , ylabel = ylabel , yscale = yscale , ylim = ylim ) self . plot_values ( agent_context = agent_context , xvalues = xvalues , yvalues = yvalues , color = color ) plot_text def plot_text ( self , text : str ) View Source def plot_text ( self , text : str ) : if text : ax = self . axes ax . text ( 0.5 , 0.5 , text , horizontalalignment = 'center' , verticalalignment = 'center' , color = 'blue' , wrap = True ) ax . set_xlabel ( '' ) ax . get_xaxis (). set_ticks ( [] ) ax . get_yaxis (). set_ticks ( [] ) axes_color = self . axes_color for spin in ax . spines : ax . spines [ spin ] . set_visible ( True ) ax . spines [ spin ] . set_color ( axes_color ) plot_values def plot_values ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], color : Union [ str , NoneType ] = 'blue' , marker : str = None , pause : bool = True ) Draws the graph given by xvalues, yvalues. Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) color: the graphs color (must be the name of a matplotlib color) or None marker: if None a marker is used if the graph has just a few points. pause: pause to redraw the plot. View Source def plot_values ( self , agent_context : core . AgentContext , xvalues : List [ int ] , yvalues : List [ Union[float, Tuple[float, float, float ] ]] , color : Optional [ str ] = 'blue' , marker : str = None , pause : bool = True ) : \"\"\"Draws the graph given by xvalues, yvalues. Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) color: the graphs color (must be the name of a matplotlib color) or None marker: if None a marker is used if the graph has just a few points. pause: pause to redraw the plot. \"\"\" assert xvalues is not None assert yvalues is not None assert len ( xvalues ) == len ( yvalues ), \"xvalues do not match yvalues\" pyc : core . PyPlotContext = agent_context . pyplot # extract min / max and y values if yvalues is of the form [ (min,y,max),...) yminvalues = None ymaxvalues = None if len(yvalues) > 0 and isinstance(yvalues[0 ] , tuple ) : ymaxvalues = [ t[2 ] for t in yvalues ] yminvalues = [ t[0 ] for t in yvalues ] yvalues = [ t[1 ] for t in yvalues ] # plot values marker = marker if marker is None and len ( xvalues ) < 10 : marker = 'o' fill_alpha = 0.1 if pyc . is_jupyter_active : if yminvalues is not None : self . axes . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) self . axes . plot ( xvalues , yvalues , color = color , marker = marker ) else : if yminvalues is not None : plt . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) plt . plot ( xvalues , yvalues , color = color , marker = marker ) if pause : plt . pause ( 0.01 ) Clear class Clear ( on_play : bool = True , on_train : bool = True ) Configures the clearing of plots in the jupyter output cell after calls to train or play. View Source class Clear ( core . AgentCallback ): \"\"\"Configures the clearing of plots in the jupyter output cell after calls to train or play.\"\"\" def __init__ ( self , on_play: bool = True , on_train: bool = True ): \"\"\"Define the cell clearing behaviour after agent.train and agent.play. Args: on_play: if set the output cell is cleared after agent.play if a plot exists on_train: if set the output cell is cleared after agent.train if a plot exists \"\"\" self . _on_play: bool = on_play self . _on_train: bool = on_train on_play_end_clear_jupyter_display = self . _on_play on_train_end_clear_jupyter_display = self . _on_train def on_play_begin ( self , agent_context: core . AgentContext ): global on_play_end_clear_jupyter_display on_play_end_clear_jupyter_display = self . _on_play def on_train_begin ( self , agent_context: core . AgentContext ): global on_train_end_clear_jupyter_display on_train_end_clear_jupyter_display = self . _on_train Ancestors (in MRO) easyagents.core.AgentCallback abc.ABC Methods on_api_log def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass on_gym_init_begin def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" on_gym_init_end def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass on_gym_reset_begin def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\" on_gym_reset_end def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass on_gym_step_begin def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass on_gym_step_end def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass on_log def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass on_play_begin def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : core . AgentContext ): global on_play_end_clear_jupyter_display on_play_end_clear_jupyter_display = self . _on_play on_play_end def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.play() call (during play or eval, but not during train)\"\"\" on_play_episode_begin def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\" on_play_episode_end def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : AgentContext ): \"\"\"Called once after an episode is done or stopped (during play or eval, but not during train).\"\"\" on_play_step_begin def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\" on_play_step_end def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"Called once after a step is completed in the current episode (during play or eval, but not during train).\"\"\" on_train_begin def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : core . AgentContext ): global on_train_end_clear_jupyter_display on_train_end_clear_jupyter_display = self . _on_train on_train_end def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\" on_train_iteration_begin def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\" on_train_iteration_end def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : AgentContext ): \"\"\"Called once after the current iteration is completed\"\"\" Loss class Loss ( yscale : str = 'symlog' , ylim : Union [ Tuple [ float , float ], NoneType ] = None ) Base class of plyplot callbacks generating a plot after a trained iteration or an episode played. Attributes: axes: the subplot to plot onto View Source class Loss ( _PlotCallback ): def __init__ ( self , yscale: str = 'symlog' , ylim: Optional [ Tuple [ float , float ]] = None ): \"\"\"Plots the loss resulting from each iterations policy training. Hints: o for actro-critic agents the loss from training the actor- and critic-networks are plotted along with the total loss. Args: yscale: scale of the y-axes ('linear', 'symlog',...) ylim: (min,max) for the y-axes \"\"\" super (). __init__ ( plot_type = core . PlotType . TRAIN_ITERATION ) self . ylim = ylim self . yscale = yscale def plot ( self , agent_context: core . AgentContext , plot_type: core . PlotType ): ac = agent_context tc = ac . train xvalues = list ( tc . loss . keys ()) self . clear_plot ( agent_context ) lossvalues = list ( tc . loss . values ()) if self . _is_nan ( lossvalues ): self . plot_text ( 'no loss data available' ) else: self . plot_axes ( xlim =( 0 , tc . episodes_done_in_training ), xlabel = 'episodes trained' , ylim = self . ylim , ylabel = 'loss' , yscale = self . yscale ) if isinstance ( tc , core . PpoTrainContext ): acc: core . PpoTrainContext = tc self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = lossvalues , color = 'indigo' , pause = False ) self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = list ( acc . actor_loss . values ()), color = 'g' , pause = False ) self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = list ( acc . critic_loss . values ()), color = 'b' ) self . axes . legend (( 'total' , 'actor' , 'critic' )) else: self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = lossvalues , color = 'indigo' ) Ancestors (in MRO) easyagents.callbacks.plot._PlotCallback easyagents.core.AgentCallback abc.ABC Methods clear_plot def clear_plot ( self , agent_context : easyagents . core . AgentContext ) Clears the axes for this plot. Should be called by self.plot before replotting an axes. View Source def clear_plot ( self , agent_context : core . AgentContext ): \"\"\"Clears the axes for this plot. Should be called by self.plot before replotting an axes.\"\"\" assert self . axes is not None pyc = agent_context . pyplot if pyc . is_jupyter_active : self . axes . cla () else : plt . cla () on_api_log def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass on_gym_init_begin def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" on_gym_init_end def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass on_gym_reset_begin def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\" on_gym_reset_end def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass on_gym_step_begin def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass on_gym_step_end def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass on_log def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass on_play_begin def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : core . AgentContext ): if ( self . _plot_type & ( core . PlotType . PLAY_EPISODE | core . PlotType . PLAY_STEP )) != core . PlotType . NONE : self . _create_subplot ( agent_context ) on_play_end def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_EVAL ) on_play_episode_begin def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\" on_play_episode_end def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_EPISODE ) on_play_step_begin def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\" on_play_step_end def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_STEP ) on_train_begin def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : core . AgentContext ): if ( self . _plot_type & ( core . PlotType . TRAIN_EVAL | core . PlotType . TRAIN_ITERATION )) != core . PlotType . NONE : self . _create_subplot ( agent_context ) on_train_end def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\" on_train_iteration_begin def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\" on_train_iteration_end def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_ITERATION ) plot def plot ( self , agent_context : easyagents . core . AgentContext , plot_type : easyagents . core . PlotType ) Plots a graph on the self.axes object. Args: agent_context: the context containing the data to plot plot_type: the PlotType requesting this plot call. View Source def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ): ac = agent_context tc = ac . train xvalues = list ( tc . loss . keys ()) self . clear_plot ( agent_context ) lossvalues = list ( tc . loss . values ()) if self . _is_nan ( lossvalues ): self . plot_text ( 'no loss data available' ) else : self . plot_axes ( xlim = ( 0 , tc . episodes_done_in_training ), xlabel = 'episodes trained' , ylim = self . ylim , ylabel = 'loss' , yscale = self . yscale ) if isinstance ( tc , core . PpoTrainContext ): acc : core . PpoTrainContext = tc self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = lossvalues , color = 'indigo' , pause = False ) self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = list ( acc . actor_loss . values ()), color = 'g' , pause = False ) self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = list ( acc . critic_loss . values ()), color = 'b' ) self . axes . legend (( 'total' , 'actor' , 'critic' )) else : self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = lossvalues , color = 'indigo' ) plot_axes def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None ) Draws the x- and y-axes. Attributes: xlim: None or (min,max) for the x-axes xlabel: label of the x-axes ylim: None or (min,max) for the y-axes (or None) ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log', ...) View Source def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = 'linear' , ylim : Optional [ Tuple [ float , float ]] = None ): \"\"\"Draws the x- and y-axes. Attributes: xlim: None or (min,max) for the x-axes xlabel: label of the x-axes ylim: None or (min,max) for the y-axes (or None) ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log', ...) \"\"\" assert xlim is None or xlim [ 0 ] <= xlim [ 1 ] assert ylim is None or ylim [ 0 ] <= ylim [ 1 ] assert xlabel is not None assert ylabel is not None # setup subplot ( axes , labels , colors ) axes_color = self . axes_color self . axes . set_xlabel ( xlabel ) self . axes . set_ylabel ( ylabel ) self . axes . spines [ 'top' ]. set_visible ( False ) self . axes . spines [ 'right' ]. set_visible ( False ) self . axes . spines [ 'bottom' ]. set_color ( axes_color ) self . axes . spines [ 'left' ]. set_color ( axes_color ) self . axes . grid ( color = axes_color , linestyle = '-' , linewidth = 0 . 25 , alpha = 0 . 5 ) if xlim is not None : self . axes . set_xlim ( xlim ) if ylim is not None : self . axes . set_ylim ( ylim ) self . axes . set_yscale ( yscale ) plot_subplot def plot_subplot ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], ylabel : str , xlabel : str = None , xlim : Union [ Tuple [ float , float ], NoneType ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None , color : str = 'blue' ) Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) ylim: (min,max) for the x-axes xlabel: label of the x-axes ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log',...) ylim: (min,max) for the y-axes (or None) color: the graphs color (must be the name of a matplotlib color) View Source def plot_subplot ( self , agent_context : core . AgentContext , xvalues : List [ int ] , yvalues : List [ Union[float, Tuple[float, float, float ] ]] , ylabel : str , xlabel : str = None , xlim : Optional [ Tuple[float, float ] ] = None , yscale : str = 'linear' , ylim : Optional [ Tuple[float, float ] ] = None , color : str = 'blue' ) : \"\"\"Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) ylim: (min,max) for the x-axes xlabel: label of the x-axes ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log',...) ylim: (min,max) for the y-axes (or None) color: the graphs color (must be the name of a matplotlib color) \"\"\" if not xlim : xmin = 0 xmax = 1 if agent_context . is_play : xmin = 1 xmax = agent_context . play . episodes_done if agent_context . is_train or agent_context . is_eval : xmin = 0 xmax = agent_context . train . episodes_done_in_training xlim = ( xmin , xmax ) if xlabel is None : xlabel = 'episodes' if agent_context . is_play : xlabel = 'episodes played' if agent_context . is_eval or agent_context . is_train : xlabel = 'episodes trained' self . clear_plot ( agent_context ) self . plot_axes ( xlim = xlim , xlabel = xlabel , ylabel = ylabel , yscale = yscale , ylim = ylim ) self . plot_values ( agent_context = agent_context , xvalues = xvalues , yvalues = yvalues , color = color ) plot_text def plot_text ( self , text : str ) View Source def plot_text ( self , text : str ) : if text : ax = self . axes ax . text ( 0.5 , 0.5 , text , horizontalalignment = 'center' , verticalalignment = 'center' , color = 'blue' , wrap = True ) ax . set_xlabel ( '' ) ax . get_xaxis (). set_ticks ( [] ) ax . get_yaxis (). set_ticks ( [] ) axes_color = self . axes_color for spin in ax . spines : ax . spines [ spin ] . set_visible ( True ) ax . spines [ spin ] . set_color ( axes_color ) plot_values def plot_values ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], color : Union [ str , NoneType ] = 'blue' , marker : str = None , pause : bool = True ) Draws the graph given by xvalues, yvalues. Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) color: the graphs color (must be the name of a matplotlib color) or None marker: if None a marker is used if the graph has just a few points. pause: pause to redraw the plot. View Source def plot_values ( self , agent_context : core . AgentContext , xvalues : List [ int ] , yvalues : List [ Union[float, Tuple[float, float, float ] ]] , color : Optional [ str ] = 'blue' , marker : str = None , pause : bool = True ) : \"\"\"Draws the graph given by xvalues, yvalues. Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) color: the graphs color (must be the name of a matplotlib color) or None marker: if None a marker is used if the graph has just a few points. pause: pause to redraw the plot. \"\"\" assert xvalues is not None assert yvalues is not None assert len ( xvalues ) == len ( yvalues ), \"xvalues do not match yvalues\" pyc : core . PyPlotContext = agent_context . pyplot # extract min / max and y values if yvalues is of the form [ (min,y,max),...) yminvalues = None ymaxvalues = None if len(yvalues) > 0 and isinstance(yvalues[0 ] , tuple ) : ymaxvalues = [ t[2 ] for t in yvalues ] yminvalues = [ t[0 ] for t in yvalues ] yvalues = [ t[1 ] for t in yvalues ] # plot values marker = marker if marker is None and len ( xvalues ) < 10 : marker = 'o' fill_alpha = 0.1 if pyc . is_jupyter_active : if yminvalues is not None : self . axes . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) self . axes . plot ( xvalues , yvalues , color = color , marker = marker ) else : if yminvalues is not None : plt . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) plt . plot ( xvalues , yvalues , color = color , marker = marker ) if pause : plt . pause ( 0.01 ) Rewards class Rewards ( yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None ) Base class of plyplot callbacks generating a plot after a trained iteration or an episode played. Attributes: axes: the subplot to plot onto View Source class Rewards ( _PlotCallback ): def __init__ ( self , yscale: str = 'linear' , ylim: Optional [ Tuple [ float , float ]] = None ): \"\"\"Plots the sum of rewards observed during policy evaluation. Args: yscale: scale of the y-axes ('linear', 'symlog',...) ylim: (min,max) for the y-axes \"\"\" super (). __init__ ( core . PlotType . TRAIN_ITERATION | core . PlotType . TRAIN_EVAL | core . PlotType . PLAY_EPISODE ) self . ylim = ylim self . yscale = yscale def plot ( self , agent_context: core . AgentContext , plot_type: core . PlotType ): xvalues = yvalues = [] ylabel = 'sum of rewards' if agent_context . is_train or agent_context . is_eval: tc = agent_context . train xvalues = list ( tc . eval_rewards . keys ()) yvalues = list ( tc . eval_rewards . values ()) ylabel = '\u00d8 sum of rewards' if agent_context . is_play: pc = agent_context . play xvalues = list ( pc . sum_of_rewards . keys ()) yvalues = list ( pc . sum_of_rewards . values ()) if xvalues: self . plot_subplot ( agent_context , color = 'green' , ylim = self . ylim , yscale = self . yscale , xvalues = xvalues , yvalues = yvalues , ylabel = ylabel ) Ancestors (in MRO) easyagents.callbacks.plot._PlotCallback easyagents.core.AgentCallback abc.ABC Methods clear_plot def clear_plot ( self , agent_context : easyagents . core . AgentContext ) Clears the axes for this plot. Should be called by self.plot before replotting an axes. View Source def clear_plot ( self , agent_context : core . AgentContext ): \"\"\"Clears the axes for this plot. Should be called by self.plot before replotting an axes.\"\"\" assert self . axes is not None pyc = agent_context . pyplot if pyc . is_jupyter_active : self . axes . cla () else : plt . cla () on_api_log def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass on_gym_init_begin def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" on_gym_init_end def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass on_gym_reset_begin def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\" on_gym_reset_end def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass on_gym_step_begin def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass on_gym_step_end def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass on_log def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass on_play_begin def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : core . AgentContext ): if ( self . _plot_type & ( core . PlotType . PLAY_EPISODE | core . PlotType . PLAY_STEP )) != core . PlotType . NONE : self . _create_subplot ( agent_context ) on_play_end def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_EVAL ) on_play_episode_begin def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\" on_play_episode_end def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_EPISODE ) on_play_step_begin def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\" on_play_step_end def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_STEP ) on_train_begin def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : core . AgentContext ): if ( self . _plot_type & ( core . PlotType . TRAIN_EVAL | core . PlotType . TRAIN_ITERATION )) != core . PlotType . NONE : self . _create_subplot ( agent_context ) on_train_end def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\" on_train_iteration_begin def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\" on_train_iteration_end def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_ITERATION ) plot def plot ( self , agent_context : easyagents . core . AgentContext , plot_type : easyagents . core . PlotType ) Plots a graph on the self.axes object. Args: agent_context: the context containing the data to plot plot_type: the PlotType requesting this plot call. View Source def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ): xvalues = yvalues = [] ylabel = 'sum of rewards' if agent_context . is_train or agent_context . is_eval : tc = agent_context . train xvalues = list ( tc . eval_rewards . keys ()) yvalues = list ( tc . eval_rewards . values ()) ylabel = '\u00d8 sum of rewards' if agent_context . is_play : pc = agent_context . play xvalues = list ( pc . sum_of_rewards . keys ()) yvalues = list ( pc . sum_of_rewards . values ()) if xvalues : self . plot_subplot ( agent_context , color = 'green' , ylim = self . ylim , yscale = self . yscale , xvalues = xvalues , yvalues = yvalues , ylabel = ylabel ) plot_axes def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None ) Draws the x- and y-axes. Attributes: xlim: None or (min,max) for the x-axes xlabel: label of the x-axes ylim: None or (min,max) for the y-axes (or None) ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log', ...) View Source def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = 'linear' , ylim : Optional [ Tuple [ float , float ]] = None ): \"\"\"Draws the x- and y-axes. Attributes: xlim: None or (min,max) for the x-axes xlabel: label of the x-axes ylim: None or (min,max) for the y-axes (or None) ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log', ...) \"\"\" assert xlim is None or xlim [ 0 ] <= xlim [ 1 ] assert ylim is None or ylim [ 0 ] <= ylim [ 1 ] assert xlabel is not None assert ylabel is not None # setup subplot ( axes , labels , colors ) axes_color = self . axes_color self . axes . set_xlabel ( xlabel ) self . axes . set_ylabel ( ylabel ) self . axes . spines [ 'top' ]. set_visible ( False ) self . axes . spines [ 'right' ]. set_visible ( False ) self . axes . spines [ 'bottom' ]. set_color ( axes_color ) self . axes . spines [ 'left' ]. set_color ( axes_color ) self . axes . grid ( color = axes_color , linestyle = '-' , linewidth = 0 . 25 , alpha = 0 . 5 ) if xlim is not None : self . axes . set_xlim ( xlim ) if ylim is not None : self . axes . set_ylim ( ylim ) self . axes . set_yscale ( yscale ) plot_subplot def plot_subplot ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], ylabel : str , xlabel : str = None , xlim : Union [ Tuple [ float , float ], NoneType ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None , color : str = 'blue' ) Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) ylim: (min,max) for the x-axes xlabel: label of the x-axes ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log',...) ylim: (min,max) for the y-axes (or None) color: the graphs color (must be the name of a matplotlib color) View Source def plot_subplot ( self , agent_context : core . AgentContext , xvalues : List [ int ] , yvalues : List [ Union[float, Tuple[float, float, float ] ]] , ylabel : str , xlabel : str = None , xlim : Optional [ Tuple[float, float ] ] = None , yscale : str = 'linear' , ylim : Optional [ Tuple[float, float ] ] = None , color : str = 'blue' ) : \"\"\"Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) ylim: (min,max) for the x-axes xlabel: label of the x-axes ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log',...) ylim: (min,max) for the y-axes (or None) color: the graphs color (must be the name of a matplotlib color) \"\"\" if not xlim : xmin = 0 xmax = 1 if agent_context . is_play : xmin = 1 xmax = agent_context . play . episodes_done if agent_context . is_train or agent_context . is_eval : xmin = 0 xmax = agent_context . train . episodes_done_in_training xlim = ( xmin , xmax ) if xlabel is None : xlabel = 'episodes' if agent_context . is_play : xlabel = 'episodes played' if agent_context . is_eval or agent_context . is_train : xlabel = 'episodes trained' self . clear_plot ( agent_context ) self . plot_axes ( xlim = xlim , xlabel = xlabel , ylabel = ylabel , yscale = yscale , ylim = ylim ) self . plot_values ( agent_context = agent_context , xvalues = xvalues , yvalues = yvalues , color = color ) plot_text def plot_text ( self , text : str ) View Source def plot_text ( self , text : str ) : if text : ax = self . axes ax . text ( 0.5 , 0.5 , text , horizontalalignment = 'center' , verticalalignment = 'center' , color = 'blue' , wrap = True ) ax . set_xlabel ( '' ) ax . get_xaxis (). set_ticks ( [] ) ax . get_yaxis (). set_ticks ( [] ) axes_color = self . axes_color for spin in ax . spines : ax . spines [ spin ] . set_visible ( True ) ax . spines [ spin ] . set_color ( axes_color ) plot_values def plot_values ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], color : Union [ str , NoneType ] = 'blue' , marker : str = None , pause : bool = True ) Draws the graph given by xvalues, yvalues. Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) color: the graphs color (must be the name of a matplotlib color) or None marker: if None a marker is used if the graph has just a few points. pause: pause to redraw the plot. View Source def plot_values ( self , agent_context : core . AgentContext , xvalues : List [ int ] , yvalues : List [ Union[float, Tuple[float, float, float ] ]] , color : Optional [ str ] = 'blue' , marker : str = None , pause : bool = True ) : \"\"\"Draws the graph given by xvalues, yvalues. Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) color: the graphs color (must be the name of a matplotlib color) or None marker: if None a marker is used if the graph has just a few points. pause: pause to redraw the plot. \"\"\" assert xvalues is not None assert yvalues is not None assert len ( xvalues ) == len ( yvalues ), \"xvalues do not match yvalues\" pyc : core . PyPlotContext = agent_context . pyplot # extract min / max and y values if yvalues is of the form [ (min,y,max),...) yminvalues = None ymaxvalues = None if len(yvalues) > 0 and isinstance(yvalues[0 ] , tuple ) : ymaxvalues = [ t[2 ] for t in yvalues ] yminvalues = [ t[0 ] for t in yvalues ] yvalues = [ t[1 ] for t in yvalues ] # plot values marker = marker if marker is None and len ( xvalues ) < 10 : marker = 'o' fill_alpha = 0.1 if pyc . is_jupyter_active : if yminvalues is not None : self . axes . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) self . axes . plot ( xvalues , yvalues , color = color , marker = marker ) else : if yminvalues is not None : plt . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) plt . plot ( xvalues , yvalues , color = color , marker = marker ) if pause : plt . pause ( 0.01 ) State class State ( mode = 'rgb_array' ) Renders the gym state as a plot to the pyplot figure using gym.render('rgb_array'). During training only the last state of the last game evaluation is plotted. During play all states are plotted. View Source class State ( _PlotCallback ) : \"\"\"Renders the gym state as a plot to the pyplot figure using gym.render('rgb_array'). During training only the last state of the last game evaluation is plotted. During play all states are plotted. \"\"\" def __init__ ( self , mode = 'rgb_array' ) : \"\"\" Args: mode: the render mode passed to gym.render(), yielding an rgb_array \"\"\" super (). __init__ ( plot_type = core . PlotType . PLAY_STEP | core . PlotType . TRAIN_EVAL ) self . _render_mode = mode def _plot_rgb_array ( self , agent_context : core . AgentContext , rgb_array : np . ndarray ) : \"\"\"Renders rgb_array to the current subplot.\"\"\" assert rgb_array is not None ax = self . axes xlabel = '' if agent_context . is_eval : xlabel = \"'done state' of last evaluation episode\" ax . imshow ( rgb_array ) ax . set_xlabel ( xlabel ) ax . get_xaxis (). set_ticks ( [] ) ax . get_yaxis (). set_ticks ( [] ) axes_color = self . axes_color for spin in ax . spines : ax . spines [ spin ] . set_visible ( True ) ax . spines [ spin ] . set_color ( axes_color ) # noinspection PyArgumentList , DuplicatedCode def _render_to_rgb_array ( self , gym_env : gym . Env , mode : str ) -> np . ndarray : \"\"\" calls gym_env.render(mode) and validates the return value to be a numpy rgb array throws an exception if not an rgb array Returns: numpy rgb array \"\"\" result = gym_env . render ( mode = mode ) assert result is not None , f 'gym_env.render(mode={mode}) yielded None' assert isinstance ( result , np . ndarray ), f 'gym_env.render(mode={mode}) did not yield a numpy.ndarray.' assert result . min () >= 0 , f 'gym_env.render(mode={mode}) contains negative values => not an rgb array' assert result . max () <= 255 , f 'gym_env.render(mode={mode}) contains values > 255 => not an rgb array' assert len ( result . shape ) == 3 , f 'gym_env.render(mode={mode}) shape is not of the form (x,y,n)' assert result . shape [ 2 ] == 3 or result . shape [ 2 ] == 4 , \\ f 'gym_env.render(mode={mode}) shape is not of the form (x,y,3|4)' return result def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ) : self . clear_plot ( agent_context ) try : rgb_array : np . ndarray = self . _render_to_rgb_array ( agent_context . play . gym_env , self . _render_mode ) self . _plot_rgb_array ( agent_context , rgb_array ) except : self . plot_text ( f 'gym.Env.render(mode=\"{self._render_mode}\") failed:\\n' ) Ancestors (in MRO) easyagents.callbacks.plot._PlotCallback easyagents.core.AgentCallback abc.ABC Methods clear_plot def clear_plot ( self , agent_context : easyagents . core . AgentContext ) Clears the axes for this plot. Should be called by self.plot before replotting an axes. View Source def clear_plot ( self , agent_context : core . AgentContext ): \"\"\"Clears the axes for this plot. Should be called by self.plot before replotting an axes.\"\"\" assert self . axes is not None pyc = agent_context . pyplot if pyc . is_jupyter_active : self . axes . cla () else : plt . cla () on_api_log def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass on_gym_init_begin def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" on_gym_init_end def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass on_gym_reset_begin def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\" on_gym_reset_end def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass on_gym_step_begin def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass on_gym_step_end def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass on_log def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass on_play_begin def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : core . AgentContext ): if ( self . _plot_type & ( core . PlotType . PLAY_EPISODE | core . PlotType . PLAY_STEP )) != core . PlotType . NONE : self . _create_subplot ( agent_context ) on_play_end def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_EVAL ) on_play_episode_begin def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\" on_play_episode_end def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_EPISODE ) on_play_step_begin def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\" on_play_step_end def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_STEP ) on_train_begin def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : core . AgentContext ): if ( self . _plot_type & ( core . PlotType . TRAIN_EVAL | core . PlotType . TRAIN_ITERATION )) != core . PlotType . NONE : self . _create_subplot ( agent_context ) on_train_end def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\" on_train_iteration_begin def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\" on_train_iteration_end def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_ITERATION ) plot def plot ( self , agent_context : easyagents . core . AgentContext , plot_type : easyagents . core . PlotType ) Plots a graph on the self.axes object. Args: agent_context: the context containing the data to plot plot_type: the PlotType requesting this plot call. View Source def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ): self . clear_plot ( agent_context ) try : rgb_array : np . ndarray = self . _render_to_rgb_array ( agent_context . play . gym_env , self . _render_mode ) self . _plot_rgb_array ( agent_context , rgb_array ) except : self . plot_text ( f 'gym.Env.render(mode=\"{self._render_mode}\") failed:\\n' ) plot_axes def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None ) Draws the x- and y-axes. Attributes: xlim: None or (min,max) for the x-axes xlabel: label of the x-axes ylim: None or (min,max) for the y-axes (or None) ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log', ...) View Source def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = 'linear' , ylim : Optional [ Tuple [ float , float ]] = None ): \"\"\"Draws the x- and y-axes. Attributes: xlim: None or (min,max) for the x-axes xlabel: label of the x-axes ylim: None or (min,max) for the y-axes (or None) ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log', ...) \"\"\" assert xlim is None or xlim [ 0 ] <= xlim [ 1 ] assert ylim is None or ylim [ 0 ] <= ylim [ 1 ] assert xlabel is not None assert ylabel is not None # setup subplot ( axes , labels , colors ) axes_color = self . axes_color self . axes . set_xlabel ( xlabel ) self . axes . set_ylabel ( ylabel ) self . axes . spines [ 'top' ]. set_visible ( False ) self . axes . spines [ 'right' ]. set_visible ( False ) self . axes . spines [ 'bottom' ]. set_color ( axes_color ) self . axes . spines [ 'left' ]. set_color ( axes_color ) self . axes . grid ( color = axes_color , linestyle = '-' , linewidth = 0 . 25 , alpha = 0 . 5 ) if xlim is not None : self . axes . set_xlim ( xlim ) if ylim is not None : self . axes . set_ylim ( ylim ) self . axes . set_yscale ( yscale ) plot_subplot def plot_subplot ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], ylabel : str , xlabel : str = None , xlim : Union [ Tuple [ float , float ], NoneType ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None , color : str = 'blue' ) Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) ylim: (min,max) for the x-axes xlabel: label of the x-axes ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log',...) ylim: (min,max) for the y-axes (or None) color: the graphs color (must be the name of a matplotlib color) View Source def plot_subplot ( self , agent_context : core . AgentContext , xvalues : List [ int ] , yvalues : List [ Union[float, Tuple[float, float, float ] ]] , ylabel : str , xlabel : str = None , xlim : Optional [ Tuple[float, float ] ] = None , yscale : str = 'linear' , ylim : Optional [ Tuple[float, float ] ] = None , color : str = 'blue' ) : \"\"\"Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) ylim: (min,max) for the x-axes xlabel: label of the x-axes ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log',...) ylim: (min,max) for the y-axes (or None) color: the graphs color (must be the name of a matplotlib color) \"\"\" if not xlim : xmin = 0 xmax = 1 if agent_context . is_play : xmin = 1 xmax = agent_context . play . episodes_done if agent_context . is_train or agent_context . is_eval : xmin = 0 xmax = agent_context . train . episodes_done_in_training xlim = ( xmin , xmax ) if xlabel is None : xlabel = 'episodes' if agent_context . is_play : xlabel = 'episodes played' if agent_context . is_eval or agent_context . is_train : xlabel = 'episodes trained' self . clear_plot ( agent_context ) self . plot_axes ( xlim = xlim , xlabel = xlabel , ylabel = ylabel , yscale = yscale , ylim = ylim ) self . plot_values ( agent_context = agent_context , xvalues = xvalues , yvalues = yvalues , color = color ) plot_text def plot_text ( self , text : str ) View Source def plot_text ( self , text : str ) : if text : ax = self . axes ax . text ( 0.5 , 0.5 , text , horizontalalignment = 'center' , verticalalignment = 'center' , color = 'blue' , wrap = True ) ax . set_xlabel ( '' ) ax . get_xaxis (). set_ticks ( [] ) ax . get_yaxis (). set_ticks ( [] ) axes_color = self . axes_color for spin in ax . spines : ax . spines [ spin ] . set_visible ( True ) ax . spines [ spin ] . set_color ( axes_color ) plot_values def plot_values ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], color : Union [ str , NoneType ] = 'blue' , marker : str = None , pause : bool = True ) Draws the graph given by xvalues, yvalues. Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) color: the graphs color (must be the name of a matplotlib color) or None marker: if None a marker is used if the graph has just a few points. pause: pause to redraw the plot. View Source def plot_values ( self , agent_context : core . AgentContext , xvalues : List [ int ] , yvalues : List [ Union[float, Tuple[float, float, float ] ]] , color : Optional [ str ] = 'blue' , marker : str = None , pause : bool = True ) : \"\"\"Draws the graph given by xvalues, yvalues. Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) color: the graphs color (must be the name of a matplotlib color) or None marker: if None a marker is used if the graph has just a few points. pause: pause to redraw the plot. \"\"\" assert xvalues is not None assert yvalues is not None assert len ( xvalues ) == len ( yvalues ), \"xvalues do not match yvalues\" pyc : core . PyPlotContext = agent_context . pyplot # extract min / max and y values if yvalues is of the form [ (min,y,max),...) yminvalues = None ymaxvalues = None if len(yvalues) > 0 and isinstance(yvalues[0 ] , tuple ) : ymaxvalues = [ t[2 ] for t in yvalues ] yminvalues = [ t[0 ] for t in yvalues ] yvalues = [ t[1 ] for t in yvalues ] # plot values marker = marker if marker is None and len ( xvalues ) < 10 : marker = 'o' fill_alpha = 0.1 if pyc . is_jupyter_active : if yminvalues is not None : self . axes . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) self . axes . plot ( xvalues , yvalues , color = color , marker = marker ) else : if yminvalues is not None : plt . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) plt . plot ( xvalues , yvalues , color = color , marker = marker ) if pause : plt . pause ( 0.01 ) StepRewards class StepRewards ( num_steps_between_plot = 100 ) Base class of plyplot callbacks generating a plot after a trained iteration or an episode played. Attributes: axes: the subplot to plot onto View Source class StepRewards ( _PlotCallback ) : def __init__ ( self , num_steps_between_plot = 100 ) : \"\"\"Plots the sum of rewards up to the current step during play or at the end of an evaluation period. Args: num_steps_between_plot: num of steps to play before plot is updated. \"\"\" super (). __init__ ( core . PlotType . PLAY_STEP | core . PlotType . PLAY_EPISODE | core . PlotType . TRAIN_EVAL ) assert num_steps_between_plot > 0 self . _xy_values : Dict [ int, Tuple[List[int ] , List [ float ] ]] = dict () self . _xmax : int = 0 self . _num_steps_between_plot = num_steps_between_plot def _replot ( self , agent_context : core . AgentContext ) : if self . _xmax >= 1 : self . clear_plot ( agent_context ) xlabel = 'steps played' if agent_context . is_eval : xlabel = 'steps taken during last evaluation period' self . plot_axes ( xlim = ( 1 , self . _xmax ), ylabel = 'sum of rewards' , xlabel = xlabel ) xy_values = list ( self . _xy_values . values ()) xlast , _ = xy_values [ -1 ] for xvalues , yvalues in xy_values : pause = ( xvalues == xlast ) self . plot_values ( agent_context = agent_context , xvalues = xvalues , yvalues = yvalues , color = None , marker = '' , pause = pause ) def _reset ( self ) : self . _xy_values = dict () self . _xmax = 0 def on_play_begin ( self , agent_context : core . AgentContext ) : super (). on_play_begin ( agent_context ) self . _reset () def on_play_end ( self , agent_context : core . AgentContext ) : super (). on_play_end ( agent_context ) self . _reset () def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ) : pc = agent_context . play if plot_type & core . PlotType . PLAY_STEP != core . PlotType . NONE : episode = pc . episodes_done + 1 if episode not in self . _xy_values : self . _xy_values [ episode ] = ( [] , [] ) xvalues , yvalues = self . _xy_values [ episode ] xvalues . append ( pc . steps_done_in_episode ) yvalues . append ( pc . sum_of_rewards [ episode ] ) if pc . steps_done_in_episode > self . _xmax : self . _xmax = pc . steps_done_in_episode if ( episode == 1 and pc . steps_done == 1 ) or ( pc . steps_done % self . _num_steps_between_plot ) == 0 : self . _replot ( agent_context ) if plot_type & core . PlotType . PLAY_EPISODE != core . PlotType . NONE : self . _replot ( agent_context ) if plot_type & core . PlotType . TRAIN_EVAL != core . PlotType . NONE : self . _xmax = max ( [ len(step_rewards) for step_rewards in pc.rewards.values() ] ) for episode in pc . rewards . keys () : step_rewards = pc . rewards [ episode ] xvalues = list ( range ( 1 , len ( step_rewards ) + 1 )) yvalues = [] for reward in step_rewards : old_sum = yvalues [ -1 ] if len ( yvalues ) > 0 else 0 yvalues . append ( old_sum + reward ) self . _xy_values [ episode ] = ( xvalues , yvalues ) self . _replot ( agent_context ) Ancestors (in MRO) easyagents.callbacks.plot._PlotCallback easyagents.core.AgentCallback abc.ABC Methods clear_plot def clear_plot ( self , agent_context : easyagents . core . AgentContext ) Clears the axes for this plot. Should be called by self.plot before replotting an axes. View Source def clear_plot ( self , agent_context : core . AgentContext ): \"\"\"Clears the axes for this plot. Should be called by self.plot before replotting an axes.\"\"\" assert self . axes is not None pyc = agent_context . pyplot if pyc . is_jupyter_active : self . axes . cla () else : plt . cla () on_api_log def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass on_gym_init_begin def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" on_gym_init_end def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass on_gym_reset_begin def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\" on_gym_reset_end def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass on_gym_step_begin def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass on_gym_step_end def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass on_log def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass on_play_begin def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : core . AgentContext ): super (). on_play_begin ( agent_context ) self . _reset () on_play_end def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : core . AgentContext ): super (). on_play_end ( agent_context ) self . _reset () on_play_episode_begin def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\" on_play_episode_end def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_EPISODE ) on_play_step_begin def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\" on_play_step_end def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_STEP ) on_train_begin def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : core . AgentContext ): if ( self . _plot_type & ( core . PlotType . TRAIN_EVAL | core . PlotType . TRAIN_ITERATION )) != core . PlotType . NONE : self . _create_subplot ( agent_context ) on_train_end def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\" on_train_iteration_begin def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\" on_train_iteration_end def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_ITERATION ) plot def plot ( self , agent_context : easyagents . core . AgentContext , plot_type : easyagents . core . PlotType ) Plots a graph on the self.axes object. Args: agent_context: the context containing the data to plot plot_type: the PlotType requesting this plot call. View Source def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ) : pc = agent_context . play if plot_type & core . PlotType . PLAY_STEP != core . PlotType . NONE : episode = pc . episodes_done + 1 if episode not in self . _xy_values : self . _xy_values [ episode ] = ( [] , [] ) xvalues , yvalues = self . _xy_values [ episode ] xvalues . append ( pc . steps_done_in_episode ) yvalues . append ( pc . sum_of_rewards [ episode ] ) if pc . steps_done_in_episode > self . _xmax : self . _xmax = pc . steps_done_in_episode if ( episode == 1 and pc . steps_done == 1 ) or ( pc . steps_done % self . _num_steps_between_plot ) == 0 : self . _replot ( agent_context ) if plot_type & core . PlotType . PLAY_EPISODE != core . PlotType . NONE : self . _replot ( agent_context ) if plot_type & core . PlotType . TRAIN_EVAL != core . PlotType . NONE : self . _xmax = max ( [ len(step_rewards) for step_rewards in pc.rewards.values() ] ) for episode in pc . rewards . keys () : step_rewards = pc . rewards [ episode ] xvalues = list ( range ( 1 , len ( step_rewards ) + 1 )) yvalues = [] for reward in step_rewards : old_sum = yvalues [ -1 ] if len ( yvalues ) > 0 else 0 yvalues . append ( old_sum + reward ) self . _xy_values [ episode ] = ( xvalues , yvalues ) self . _replot ( agent_context ) plot_axes def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None ) Draws the x- and y-axes. Attributes: xlim: None or (min,max) for the x-axes xlabel: label of the x-axes ylim: None or (min,max) for the y-axes (or None) ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log', ...) View Source def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = 'linear' , ylim : Optional [ Tuple [ float , float ]] = None ): \"\"\"Draws the x- and y-axes. Attributes: xlim: None or (min,max) for the x-axes xlabel: label of the x-axes ylim: None or (min,max) for the y-axes (or None) ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log', ...) \"\"\" assert xlim is None or xlim [ 0 ] <= xlim [ 1 ] assert ylim is None or ylim [ 0 ] <= ylim [ 1 ] assert xlabel is not None assert ylabel is not None # setup subplot ( axes , labels , colors ) axes_color = self . axes_color self . axes . set_xlabel ( xlabel ) self . axes . set_ylabel ( ylabel ) self . axes . spines [ 'top' ]. set_visible ( False ) self . axes . spines [ 'right' ]. set_visible ( False ) self . axes . spines [ 'bottom' ]. set_color ( axes_color ) self . axes . spines [ 'left' ]. set_color ( axes_color ) self . axes . grid ( color = axes_color , linestyle = '-' , linewidth = 0 . 25 , alpha = 0 . 5 ) if xlim is not None : self . axes . set_xlim ( xlim ) if ylim is not None : self . axes . set_ylim ( ylim ) self . axes . set_yscale ( yscale ) plot_subplot def plot_subplot ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], ylabel : str , xlabel : str = None , xlim : Union [ Tuple [ float , float ], NoneType ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None , color : str = 'blue' ) Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) ylim: (min,max) for the x-axes xlabel: label of the x-axes ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log',...) ylim: (min,max) for the y-axes (or None) color: the graphs color (must be the name of a matplotlib color) View Source def plot_subplot ( self , agent_context : core . AgentContext , xvalues : List [ int ] , yvalues : List [ Union[float, Tuple[float, float, float ] ]] , ylabel : str , xlabel : str = None , xlim : Optional [ Tuple[float, float ] ] = None , yscale : str = 'linear' , ylim : Optional [ Tuple[float, float ] ] = None , color : str = 'blue' ) : \"\"\"Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) ylim: (min,max) for the x-axes xlabel: label of the x-axes ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log',...) ylim: (min,max) for the y-axes (or None) color: the graphs color (must be the name of a matplotlib color) \"\"\" if not xlim : xmin = 0 xmax = 1 if agent_context . is_play : xmin = 1 xmax = agent_context . play . episodes_done if agent_context . is_train or agent_context . is_eval : xmin = 0 xmax = agent_context . train . episodes_done_in_training xlim = ( xmin , xmax ) if xlabel is None : xlabel = 'episodes' if agent_context . is_play : xlabel = 'episodes played' if agent_context . is_eval or agent_context . is_train : xlabel = 'episodes trained' self . clear_plot ( agent_context ) self . plot_axes ( xlim = xlim , xlabel = xlabel , ylabel = ylabel , yscale = yscale , ylim = ylim ) self . plot_values ( agent_context = agent_context , xvalues = xvalues , yvalues = yvalues , color = color ) plot_text def plot_text ( self , text : str ) View Source def plot_text ( self , text : str ) : if text : ax = self . axes ax . text ( 0.5 , 0.5 , text , horizontalalignment = 'center' , verticalalignment = 'center' , color = 'blue' , wrap = True ) ax . set_xlabel ( '' ) ax . get_xaxis (). set_ticks ( [] ) ax . get_yaxis (). set_ticks ( [] ) axes_color = self . axes_color for spin in ax . spines : ax . spines [ spin ] . set_visible ( True ) ax . spines [ spin ] . set_color ( axes_color ) plot_values def plot_values ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], color : Union [ str , NoneType ] = 'blue' , marker : str = None , pause : bool = True ) Draws the graph given by xvalues, yvalues. Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) color: the graphs color (must be the name of a matplotlib color) or None marker: if None a marker is used if the graph has just a few points. pause: pause to redraw the plot. View Source def plot_values ( self , agent_context : core . AgentContext , xvalues : List [ int ] , yvalues : List [ Union[float, Tuple[float, float, float ] ]] , color : Optional [ str ] = 'blue' , marker : str = None , pause : bool = True ) : \"\"\"Draws the graph given by xvalues, yvalues. Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) color: the graphs color (must be the name of a matplotlib color) or None marker: if None a marker is used if the graph has just a few points. pause: pause to redraw the plot. \"\"\" assert xvalues is not None assert yvalues is not None assert len ( xvalues ) == len ( yvalues ), \"xvalues do not match yvalues\" pyc : core . PyPlotContext = agent_context . pyplot # extract min / max and y values if yvalues is of the form [ (min,y,max),...) yminvalues = None ymaxvalues = None if len(yvalues) > 0 and isinstance(yvalues[0 ] , tuple ) : ymaxvalues = [ t[2 ] for t in yvalues ] yminvalues = [ t[0 ] for t in yvalues ] yvalues = [ t[1 ] for t in yvalues ] # plot values marker = marker if marker is None and len ( xvalues ) < 10 : marker = 'o' fill_alpha = 0.1 if pyc . is_jupyter_active : if yminvalues is not None : self . axes . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) self . axes . plot ( xvalues , yvalues , color = color , marker = marker ) else : if yminvalues is not None : plt . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) plt . plot ( xvalues , yvalues , color = color , marker = marker ) if pause : plt . pause ( 0.01 ) Steps class Steps ( yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None ) Base class of plyplot callbacks generating a plot after a trained iteration or an episode played. Attributes: axes: the subplot to plot onto View Source class Steps ( _PlotCallback ) : def __init__ ( self , yscale : str = 'linear' , ylim : Optional [ Tuple[float, float ] ] = None ) : \"\"\"Plots the step counts observed during policy evaluation. Args: yscale: scale of the y-axes ('linear','log') ylim: (min,max) for the y-axes \"\"\" super (). __init__ ( core . PlotType . TRAIN_ITERATION | core . PlotType . TRAIN_EVAL | core . PlotType . PLAY_EPISODE ) self . ylim = ylim self . yscale = yscale def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ) : xvalues = yvalues = [] ylabel = 'steps' if agent_context . is_train or agent_context . is_eval : tc = agent_context . train xvalues = list ( tc . eval_steps . keys ()) yvalues = list ( tc . eval_steps . values ()) ylabel = '\u00d8 steps' if agent_context . is_play : pc = agent_context . play xvalues = list ( pc . actions . keys ()) yvalues = [ len(pc.actions[episode ] ) for episode in pc . actions . keys () ] self . plot_subplot ( agent_context , color = 'blue' , ylim = self . ylim , yscale = self . yscale , xvalues = xvalues , yvalues = yvalues , ylabel = ylabel ) Ancestors (in MRO) easyagents.callbacks.plot._PlotCallback easyagents.core.AgentCallback abc.ABC Methods clear_plot def clear_plot ( self , agent_context : easyagents . core . AgentContext ) Clears the axes for this plot. Should be called by self.plot before replotting an axes. View Source def clear_plot ( self , agent_context : core . AgentContext ): \"\"\"Clears the axes for this plot. Should be called by self.plot before replotting an axes.\"\"\" assert self . axes is not None pyc = agent_context . pyplot if pyc . is_jupyter_active : self . axes . cla () else : plt . cla () on_api_log def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass on_gym_init_begin def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" on_gym_init_end def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass on_gym_reset_begin def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\" on_gym_reset_end def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass on_gym_step_begin def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass on_gym_step_end def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass on_log def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass on_play_begin def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : core . AgentContext ): if ( self . _plot_type & ( core . PlotType . PLAY_EPISODE | core . PlotType . PLAY_STEP )) != core . PlotType . NONE : self . _create_subplot ( agent_context ) on_play_end def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_EVAL ) on_play_episode_begin def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\" on_play_episode_end def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_EPISODE ) on_play_step_begin def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\" on_play_step_end def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_STEP ) on_train_begin def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : core . AgentContext ): if ( self . _plot_type & ( core . PlotType . TRAIN_EVAL | core . PlotType . TRAIN_ITERATION )) != core . PlotType . NONE : self . _create_subplot ( agent_context ) on_train_end def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\" on_train_iteration_begin def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\" on_train_iteration_end def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_ITERATION ) plot def plot ( self , agent_context : easyagents . core . AgentContext , plot_type : easyagents . core . PlotType ) Plots a graph on the self.axes object. Args: agent_context: the context containing the data to plot plot_type: the PlotType requesting this plot call. View Source def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ) : xvalues = yvalues = [] ylabel = 'steps' if agent_context . is_train or agent_context . is_eval : tc = agent_context . train xvalues = list ( tc . eval_steps . keys ()) yvalues = list ( tc . eval_steps . values ()) ylabel = '\u00d8 steps' if agent_context . is_play : pc = agent_context . play xvalues = list ( pc . actions . keys ()) yvalues = [ len(pc.actions[episode ] ) for episode in pc . actions . keys () ] self . plot_subplot ( agent_context , color = 'blue' , ylim = self . ylim , yscale = self . yscale , xvalues = xvalues , yvalues = yvalues , ylabel = ylabel ) plot_axes def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None ) Draws the x- and y-axes. Attributes: xlim: None or (min,max) for the x-axes xlabel: label of the x-axes ylim: None or (min,max) for the y-axes (or None) ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log', ...) View Source def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = 'linear' , ylim : Optional [ Tuple [ float , float ]] = None ): \"\"\"Draws the x- and y-axes. Attributes: xlim: None or (min,max) for the x-axes xlabel: label of the x-axes ylim: None or (min,max) for the y-axes (or None) ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log', ...) \"\"\" assert xlim is None or xlim [ 0 ] <= xlim [ 1 ] assert ylim is None or ylim [ 0 ] <= ylim [ 1 ] assert xlabel is not None assert ylabel is not None # setup subplot ( axes , labels , colors ) axes_color = self . axes_color self . axes . set_xlabel ( xlabel ) self . axes . set_ylabel ( ylabel ) self . axes . spines [ 'top' ]. set_visible ( False ) self . axes . spines [ 'right' ]. set_visible ( False ) self . axes . spines [ 'bottom' ]. set_color ( axes_color ) self . axes . spines [ 'left' ]. set_color ( axes_color ) self . axes . grid ( color = axes_color , linestyle = '-' , linewidth = 0 . 25 , alpha = 0 . 5 ) if xlim is not None : self . axes . set_xlim ( xlim ) if ylim is not None : self . axes . set_ylim ( ylim ) self . axes . set_yscale ( yscale ) plot_subplot def plot_subplot ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], ylabel : str , xlabel : str = None , xlim : Union [ Tuple [ float , float ], NoneType ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None , color : str = 'blue' ) Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) ylim: (min,max) for the x-axes xlabel: label of the x-axes ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log',...) ylim: (min,max) for the y-axes (or None) color: the graphs color (must be the name of a matplotlib color) View Source def plot_subplot ( self , agent_context : core . AgentContext , xvalues : List [ int ] , yvalues : List [ Union[float, Tuple[float, float, float ] ]] , ylabel : str , xlabel : str = None , xlim : Optional [ Tuple[float, float ] ] = None , yscale : str = 'linear' , ylim : Optional [ Tuple[float, float ] ] = None , color : str = 'blue' ) : \"\"\"Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) ylim: (min,max) for the x-axes xlabel: label of the x-axes ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log',...) ylim: (min,max) for the y-axes (or None) color: the graphs color (must be the name of a matplotlib color) \"\"\" if not xlim : xmin = 0 xmax = 1 if agent_context . is_play : xmin = 1 xmax = agent_context . play . episodes_done if agent_context . is_train or agent_context . is_eval : xmin = 0 xmax = agent_context . train . episodes_done_in_training xlim = ( xmin , xmax ) if xlabel is None : xlabel = 'episodes' if agent_context . is_play : xlabel = 'episodes played' if agent_context . is_eval or agent_context . is_train : xlabel = 'episodes trained' self . clear_plot ( agent_context ) self . plot_axes ( xlim = xlim , xlabel = xlabel , ylabel = ylabel , yscale = yscale , ylim = ylim ) self . plot_values ( agent_context = agent_context , xvalues = xvalues , yvalues = yvalues , color = color ) plot_text def plot_text ( self , text : str ) View Source def plot_text ( self , text : str ) : if text : ax = self . axes ax . text ( 0.5 , 0.5 , text , horizontalalignment = 'center' , verticalalignment = 'center' , color = 'blue' , wrap = True ) ax . set_xlabel ( '' ) ax . get_xaxis (). set_ticks ( [] ) ax . get_yaxis (). set_ticks ( [] ) axes_color = self . axes_color for spin in ax . spines : ax . spines [ spin ] . set_visible ( True ) ax . spines [ spin ] . set_color ( axes_color ) plot_values def plot_values ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], color : Union [ str , NoneType ] = 'blue' , marker : str = None , pause : bool = True ) Draws the graph given by xvalues, yvalues. Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) color: the graphs color (must be the name of a matplotlib color) or None marker: if None a marker is used if the graph has just a few points. pause: pause to redraw the plot. View Source def plot_values ( self , agent_context : core . AgentContext , xvalues : List [ int ] , yvalues : List [ Union[float, Tuple[float, float, float ] ]] , color : Optional [ str ] = 'blue' , marker : str = None , pause : bool = True ) : \"\"\"Draws the graph given by xvalues, yvalues. Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) color: the graphs color (must be the name of a matplotlib color) or None marker: if None a marker is used if the graph has just a few points. pause: pause to redraw the plot. \"\"\" assert xvalues is not None assert yvalues is not None assert len ( xvalues ) == len ( yvalues ), \"xvalues do not match yvalues\" pyc : core . PyPlotContext = agent_context . pyplot # extract min / max and y values if yvalues is of the form [ (min,y,max),...) yminvalues = None ymaxvalues = None if len(yvalues) > 0 and isinstance(yvalues[0 ] , tuple ) : ymaxvalues = [ t[2 ] for t in yvalues ] yminvalues = [ t[0 ] for t in yvalues ] yvalues = [ t[1 ] for t in yvalues ] # plot values marker = marker if marker is None and len ( xvalues ) < 10 : marker = 'o' fill_alpha = 0.1 if pyc . is_jupyter_active : if yminvalues is not None : self . axes . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) self . axes . plot ( xvalues , yvalues , color = color , marker = marker ) else : if yminvalues is not None : plt . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) plt . plot ( xvalues , yvalues , color = color , marker = marker ) if pause : plt . pause ( 0.01 ) ToMovie class ToMovie ( fps : Union [ int , NoneType ] = None , filepath : str = None ) Plots the pyplot figure to an mp4 file Attributes: fps: frame per seconds filepath: the filepath of the mp4 file. View Source class ToMovie ( core . _ PostProcessCallback ) : \"\"\"Plots the pyplot figure to an mp4 file Attributes: fps: frame per seconds filepath: the filepath of the mp4 file. \"\"\" def __ init__ ( self , fps : Optional [ int ] = None , filepath : str = None ) : \"\"\"Writes the ploted graphs and images to the mp4 / gif file given by filepath. if filepath ends in '.gif' an animated gif is created. Args: fps: frames per second filepath: the filepath of the mp4 or gif file file. If None the file is written to a temp file. \"\"\" super (). __ init__ () self . fps = fps self . _ is_filepath_set = filepath is not None self . filepath = filepath if not self . _ is_filepath_set: self . filepath = easyagents . backends . core . _ get_temp_path () if ( not self . _ is_animated_gif ()) and ( not self . filepath . lower (). endswith ( '.mp4' )) : self . filepath = self . filepath + '.mp4' self . _ video = imageio . get_writer ( self . filepath , fps = fps ) if fps else imageio . get_writer ( self . filepath ) def _ close ( self , agent_context: core . AgentContext ) : \"\"\"closes the mp4 file and displays it in jupyter cell (if in a jupyter notebook)\"\"\" self . _ video . close () self . _ video = None if agent_context . pyplot . is_jupyter_active: with open ( self . filepath , 'rb' ) as f : video = f . read () b64 = base64 . b64encode ( video ) if not self . _ is_filepath_set: os . remove ( self . filepath ) width = 640 height = 480 if self . _ is_animated_gif () : result = ''' <img src=\"data:image/gif;base64,{2}\" alt=\"easyagents.plot\" width={0}/> ''' . format ( width , height , b64 . decode ()) else : result = ''' <video width=\"{0}\" height=\"{1}\" controls> <source src=\"data:video/mp4;base64,{2}\" type=\"video/mp4\"> Your browser does not support the video tag. </video>''' . format ( width , height , b64 . decode ()) result = HTML ( result ) # noinspection PyTypeChecker clear_output ( wait = True ) # noinspection PyTypeChecker display ( result ) def _ get_rgb_array ( self , agent_context: core . AgentContext ) -> np . ndarray : \"\"\"Yields an rgb array representing the current content of the subplots.\"\"\" pyc = agent_context . pyplot pyc . figure . canvas . draw () result = np . frombuffer ( pyc . figure . canvas . tostring_rgb (), dtype='uint8' ) result = result . reshape ( pyc . figure . canvas . get_width_height ()[ ::- 1 ] + ( 3 ,)) return result def _ is_animated_gif ( self ) : return self . filepath . lower (). endswith ( '.gif' ) def _ write_figure_to_video ( self , agent_context: core . AgentContext ) : \"\"\"Appends the current pyplot figure to the video. if an exception occures no frame is added. \"\"\" try : rgb_array = self . _ get_rgb_array ( agent_context ) self . _ video . append_data ( rgb_array ) except : pass def on_play_episode_end ( self , agent_context: core . AgentContext ) : if agent_context . _ is_plot_ready ( core . PlotType . PLAY_EPISODE | core . PlotType . TRAIN_EVAL ) : self . _ write_figure_to_video ( agent_context ) def on_play_step_end ( self , agent_context: core . AgentContext , action , step_result: Tuple ) : if agent_context . _ is_plot_ready ( core . PlotType . PLAY_STEP ) : self . _ write_figure_to_video ( agent_context ) def on_train_iteration_end ( self , agent_context: core . AgentContext ) : if agent_context . _ is_plot_ready ( core . PlotType . TRAIN_ITERATION ) : self . _ write_figure_to_video ( agent_context ) def on_play_end ( self , agent_context: core . AgentContext ) : if agent_context . _ is_plot_ready ( core . PlotType . PLAY_EPISODE ) : self . _ close ( agent_context ) def on_train_end ( self , agent_context: core . AgentContext ) : self . _ close ( agent_context ) Ancestors (in MRO) easyagents.core._PostProcessCallback easyagents.core.AgentCallback abc.ABC Methods on_api_log def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass on_gym_init_begin def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" on_gym_init_end def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass on_gym_reset_begin def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\" on_gym_reset_end def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass on_gym_step_begin def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass on_gym_step_end def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass on_log def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass on_play_begin def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.play() call (during play or eval, but not during train). \"\"\" on_play_end def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : core . AgentContext ): if agent_context . _is_plot_ready ( core . PlotType . PLAY_EPISODE ): self . _close ( agent_context ) on_play_episode_begin def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\" on_play_episode_end def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : core . AgentContext ): if agent_context . _is_plot_ready ( core . PlotType . PLAY_EPISODE | core . PlotType . TRAIN_EVAL ): self . _write_figure_to_video ( agent_context ) on_play_step_begin def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\" on_play_step_end def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): if agent_context . _is_plot_ready ( core . PlotType . PLAY_STEP ): self . _write_figure_to_video ( agent_context ) on_train_begin def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.train() call. \"\"\" on_train_end def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : core . AgentContext ): self . _close ( agent_context ) on_train_iteration_begin def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\" on_train_iteration_end def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : core . AgentContext ): if agent_context . _is_plot_ready ( core . PlotType . TRAIN_ITERATION ): self . _write_figure_to_video ( agent_context )","title":"Plot"},{"location":"reference/easyagents/callbacks/plot/#module-easyagentscallbacksplot","text":"View Source from typing import Optional , List , Tuple , Union , Dict import easyagents.core as core import easyagents.backends.core import base64 import matplotlib.pyplot as plt import numpy as np import imageio import math import gym import os.path # avoid \"double rendering\" of the final jupyter output on_play_end_clear_jupyter_display : bool = True on_train_end_clear_jupyter_display : bool = True # check if we are running in Jupyter, if so interactive plotting must be handled differently # (in order to get plot updates during training) _is_jupyter_active = False try : # noinspection PyUnresolvedReferences from IPython import get_ipython # noinspection PyUnresolvedReferences from IPython.display import display , clear_output # noinspection PyUnresolvedReferences from IPython.display import HTML shell = get_ipython () . __class__ . __name__ if shell == 'ZMQInteractiveShell' : _is_jupyter_active = True else : # noinspection PyPackageRequirements import google.colab _is_jupyter_active = True except ImportError : pass class _PreProcess ( core . _PreProcessCallback ): \"\"\"Initializes the matplotlib agent_context.pyplot.figure\"\"\" def _setup ( self , agent_context : core . AgentContext ): # create figure / remove all existing axes from previous calls to train/play pyc = agent_context . pyplot pyc . is_jupyter_active = _is_jupyter_active if pyc . figure is None : pyc . figure = plt . figure ( \"_EasyAgents\" , figsize = pyc . figsize ) for ax in pyc . figure . axes : pyc . figure . delaxes ( ax ) def on_play_begin ( self , agent_context : core . AgentContext ): # play_begin is also called at the start of a policy evaluation if agent_context . is_play : self . _setup ( agent_context = agent_context ) def on_train_begin ( self , agent_context : core . AgentContext ): self . _setup ( agent_context = agent_context ) class _PostProcess ( core . _PostProcessCallback ): \"\"\"Redraws the plots int the matplotlib agent_context.figure. In jupyter the plots are only refreshed once per second. \"\"\" def __init__ ( self ): self . _call_jupyter_display : bool self . _reset () def _clear_jupyter_plots ( self , agent_context : core . AgentContext , wait = True ): \"\"\"Clears the content in the current jupyter output cell. NoOp if not in jupyter or no plot output. Args: wait: Wait to clear the output until new output is available to replace it. \"\"\" # don't clear the jupyter output if no plot is present, may clear the log output otherwise if agent_context . pyplot . is_jupyter_active and self . _plot_exists ( agent_context ): clear_output ( wait = wait ) def _display_plots ( self , agent_context : core . AgentContext ): \"\"\"Fixes the layout of multiple subplots and refreshs the display.\"\"\" pyc = agent_context . pyplot if self . _plot_exists ( agent_context ): count = len ( pyc . figure . axes ) rows = math . ceil ( count / pyc . max_columns ) columns = math . ceil ( count / rows ) for i in range ( count ): pyc . figure . axes [ i ] . change_geometry ( rows , columns , i + 1 ) pyc . figure . tight_layout () if pyc . is_jupyter_active : self . _clear_jupyter_plots ( agent_context ) if self . _call_jupyter_display : # noinspection PyTypeChecker display ( pyc . figure ) self . _call_jupyter_display = True else : plt . pause ( 0.01 ) def _plot_exists ( self , agent_context : core . AgentContext ): \"\"\"Yields true if at least 1 jupyter plot exists.\"\"\" pyc = agent_context . pyplot count = len ( pyc . figure . axes ) result = count > 0 return result def _reset ( self ): self . _call_jupyter_display = False def on_play_begin ( self , agent_context : core . AgentContext ): if agent_context . is_play : self . _reset () def on_train_begin ( self , agent_context : core . AgentContext ): self . _reset () def on_play_episode_end ( self , agent_context : core . AgentContext ): if agent_context . _is_plot_ready ( core . PlotType . PLAY_EPISODE ): self . _display_plots ( agent_context ) def on_play_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): if agent_context . _is_plot_ready ( core . PlotType . PLAY_STEP ): self . _display_plots ( agent_context ) def on_play_end ( self , agent_context : core . AgentContext ): if agent_context . _is_plot_ready ( core . PlotType . TRAIN_EVAL ): self . _display_plots ( agent_context ) if agent_context . is_play : self . _display_plots ( agent_context ) if on_play_end_clear_jupyter_display : self . _clear_jupyter_plots ( agent_context , wait = False ) def on_train_end ( self , agent_context : core . AgentContext ): self . _display_plots ( agent_context ) if on_train_end_clear_jupyter_display : self . _clear_jupyter_plots ( agent_context , wait = False ) def on_train_iteration_begin ( self , agent_context : core . AgentContext ): # display initial evaluation before training starts. if agent_context . train . iterations_done_in_training == 0 and \\ agent_context . _is_plot_ready ( core . PlotType . TRAIN_EVAL ): self . _display_plots ( agent_context ) def on_train_iteration_end ( self , agent_context : core . AgentContext ): if agent_context . _is_plot_ready ( core . PlotType . TRAIN_ITERATION ): self . _display_plots ( agent_context ) # noinspection DuplicatedCode class _PlotCallback ( core . AgentCallback ): \"\"\"Base class of plyplot callbacks generating a plot after a trained iteration or an episode played. Attributes: axes: the subplot to plot onto \"\"\" def __init__ ( self , plot_type : core . PlotType ): \"\"\"Base class of plyplot callbacks generating a plot after a trained iteration or an episode played. Args: plot_type: point in time when the plot is updated \"\"\" self . axes = None self . axes_color : str = 'grey' self . _plot_type : core . PlotType = plot_type def _create_subplot ( self , agent_context : core . AgentContext ): if self . axes is None : pyc = agent_context . pyplot pyc . _created_subplots = pyc . _created_subplots | self . _plot_type count = len ( pyc . figure . axes ) + 1 rows = math . ceil ( count / pyc . max_columns ) columns = math . ceil ( count / rows ) self . axes = pyc . figure . add_subplot ( rows , columns , count ) self . plot_axes ( xlim = ( 0 , 1 ), ylabel = '' , xlabel = '' ) def _is_nan ( self , values : Optional [ List [ float ]]): \"\"\"yields true if all values are equal to nan. yields false if values is None or empty.\"\"\" result = False if values and all ( isinstance ( v , float ) for v in values ): result = all ( math . isnan ( v ) for v in values ) return result def _refresh_subplot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ): \"\"\"Sets this axes active and calls plot if this plot callback is registered on at least 1 plot out of plot_type.\"\"\" assert self . axes is not None plot_type = plot_type & self . _plot_type if agent_context . _is_plot_ready ( plot_type ): pyc = agent_context . pyplot if not pyc . is_jupyter_active : plt . figure ( pyc . figure . number ) if plt . gcf () is pyc . figure : plt . sca ( self . axes ) self . plot ( agent_context , plot_type ) def clear_plot ( self , agent_context : core . AgentContext ): \"\"\"Clears the axes for this plot. Should be called by self.plot before replotting an axes.\"\"\" assert self . axes is not None pyc = agent_context . pyplot if pyc . is_jupyter_active : self . axes . cla () else : plt . cla () def on_play_begin ( self , agent_context : core . AgentContext ): if ( self . _plot_type & ( core . PlotType . PLAY_EPISODE | core . PlotType . PLAY_STEP )) != core . PlotType . NONE : self . _create_subplot ( agent_context ) def on_play_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_EVAL ) def on_play_episode_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_EPISODE ) def on_play_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_STEP ) def on_train_begin ( self , agent_context : core . AgentContext ): if ( self . _plot_type & ( core . PlotType . TRAIN_EVAL | core . PlotType . TRAIN_ITERATION )) != core . PlotType . NONE : self . _create_subplot ( agent_context ) def on_train_iteration_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_ITERATION ) def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ): \"\"\"Plots a graph on the self.axes object. Args: agent_context: the context containing the data to plot plot_type: the PlotType requesting this plot call. \"\"\" pass def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = 'linear' , ylim : Optional [ Tuple [ float , float ]] = None ): \"\"\"Draws the x- and y-axes. Attributes: xlim: None or (min,max) for the x-axes xlabel: label of the x-axes ylim: None or (min,max) for the y-axes (or None) ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log', ...) \"\"\" assert xlim is None or xlim [ 0 ] <= xlim [ 1 ] assert ylim is None or ylim [ 0 ] <= ylim [ 1 ] assert xlabel is not None assert ylabel is not None # setup subplot (axes, labels, colors) axes_color = self . axes_color self . axes . set_xlabel ( xlabel ) self . axes . set_ylabel ( ylabel ) self . axes . spines [ 'top' ] . set_visible ( False ) self . axes . spines [ 'right' ] . set_visible ( False ) self . axes . spines [ 'bottom' ] . set_color ( axes_color ) self . axes . spines [ 'left' ] . set_color ( axes_color ) self . axes . grid ( color = axes_color , linestyle = '-' , linewidth = 0.25 , alpha = 0.5 ) if xlim is not None : self . axes . set_xlim ( xlim ) if ylim is not None : self . axes . set_ylim ( ylim ) self . axes . set_yscale ( yscale ) def plot_text ( self , text : str ): if text : ax = self . axes ax . text ( 0.5 , 0.5 , text , horizontalalignment = 'center' , verticalalignment = 'center' , color = 'blue' , wrap = True ) ax . set_xlabel ( '' ) ax . get_xaxis () . set_ticks ([]) ax . get_yaxis () . set_ticks ([]) axes_color = self . axes_color for spin in ax . spines : ax . spines [ spin ] . set_visible ( True ) ax . spines [ spin ] . set_color ( axes_color ) def plot_subplot ( self , agent_context : core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], ylabel : str , xlabel : str = None , xlim : Optional [ Tuple [ float , float ]] = None , yscale : str = 'linear' , ylim : Optional [ Tuple [ float , float ]] = None , color : str = 'blue' ): \"\"\"Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) ylim: (min,max) for the x-axes xlabel: label of the x-axes ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log',...) ylim: (min,max) for the y-axes (or None) color: the graphs color (must be the name of a matplotlib color) \"\"\" if not xlim : xmin = 0 xmax = 1 if agent_context . is_play : xmin = 1 xmax = agent_context . play . episodes_done if agent_context . is_train or agent_context . is_eval : xmin = 0 xmax = agent_context . train . episodes_done_in_training xlim = ( xmin , xmax ) if xlabel is None : xlabel = 'episodes' if agent_context . is_play : xlabel = 'episodes played' if agent_context . is_eval or agent_context . is_train : xlabel = 'episodes trained' self . clear_plot ( agent_context ) self . plot_axes ( xlim = xlim , xlabel = xlabel , ylabel = ylabel , yscale = yscale , ylim = ylim ) self . plot_values ( agent_context = agent_context , xvalues = xvalues , yvalues = yvalues , color = color ) def plot_values ( self , agent_context : core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], color : Optional [ str ] = 'blue' , marker : str = None , pause : bool = True ): \"\"\"Draws the graph given by xvalues, yvalues. Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) color: the graphs color (must be the name of a matplotlib color) or None marker: if None a marker is used if the graph has just a few points. pause: pause to redraw the plot. \"\"\" assert xvalues is not None assert yvalues is not None assert len ( xvalues ) == len ( yvalues ), \"xvalues do not match yvalues\" pyc : core . PyPlotContext = agent_context . pyplot # extract min / max and y values if yvalues is of the form [(min,y,max),...) yminvalues = None ymaxvalues = None if len ( yvalues ) > 0 and isinstance ( yvalues [ 0 ], tuple ): ymaxvalues = [ t [ 2 ] for t in yvalues ] yminvalues = [ t [ 0 ] for t in yvalues ] yvalues = [ t [ 1 ] for t in yvalues ] # plot values marker = marker if marker is None and len ( xvalues ) < 10 : marker = 'o' fill_alpha = 0.1 if pyc . is_jupyter_active : if yminvalues is not None : self . axes . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) self . axes . plot ( xvalues , yvalues , color = color , marker = marker ) else : if yminvalues is not None : plt . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) plt . plot ( xvalues , yvalues , color = color , marker = marker ) if pause : plt . pause ( 0.01 ) # noinspection DuplicatedCode class Actions ( _PlotCallback ): def __init__ ( self , num_steps_between_plot = 100 ): \"\"\"Plots a histogram of the actions taken during play or during the last evaluation period. Args: num_steps_between_plot: num of steps to play before plot is updated. \"\"\" super () . __init__ ( plot_type = core . PlotType . PLAY_STEP | core . PlotType . PLAY_EPISODE | core . PlotType . TRAIN_EVAL ) assert num_steps_between_plot > 0 self . _actions : List [ float ] = [] self . _num_steps_between_plot = num_steps_between_plot def _reset ( self ): self . _actions : List [ float ] = [] def on_play_begin ( self , agent_context : core . AgentContext ): super () . on_play_begin ( agent_context ) self . _reset () def on_play_end ( self , agent_context : core . AgentContext ): super () . on_play_end ( agent_context ) self . _reset () def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ): try : pc = agent_context . play is_plot = True xlabel = 'actions' if plot_type & core . PlotType . TRAIN_EVAL != core . PlotType . NONE : xlabel = 'actions taken during last evaluation period' if plot_type & core . PlotType . PLAY_STEP != core . PlotType . NONE : is_plot = ( pc . steps_done_in_episode % self . _num_steps_between_plot == 0 ) if is_plot : self . clear_plot ( agent_context ) self . plot_axes ( xlabel = xlabel , ylabel = 'count' ) self . axes . hist ( pc . actions . values ()) except : self . plot_text ( f 'Failed to create the actions histogram. \\n ' ) class Clear ( core . AgentCallback ): \"\"\"Configures the clearing of plots in the jupyter output cell after calls to train or play.\"\"\" def __init__ ( self , on_play : bool = True , on_train : bool = True ): \"\"\"Define the cell clearing behaviour after agent.train and agent.play. Args: on_play: if set the output cell is cleared after agent.play if a plot exists on_train: if set the output cell is cleared after agent.train if a plot exists \"\"\" self . _on_play : bool = on_play self . _on_train : bool = on_train on_play_end_clear_jupyter_display = self . _on_play on_train_end_clear_jupyter_display = self . _on_train def on_play_begin ( self , agent_context : core . AgentContext ): global on_play_end_clear_jupyter_display on_play_end_clear_jupyter_display = self . _on_play def on_train_begin ( self , agent_context : core . AgentContext ): global on_train_end_clear_jupyter_display on_train_end_clear_jupyter_display = self . _on_train class Loss ( _PlotCallback ): def __init__ ( self , yscale : str = 'symlog' , ylim : Optional [ Tuple [ float , float ]] = None ): \"\"\"Plots the loss resulting from each iterations policy training. Hints: o for actro-critic agents the loss from training the actor- and critic-networks are plotted along with the total loss. Args: yscale: scale of the y-axes ('linear', 'symlog',...) ylim: (min,max) for the y-axes \"\"\" super () . __init__ ( plot_type = core . PlotType . TRAIN_ITERATION ) self . ylim = ylim self . yscale = yscale def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ): ac = agent_context tc = ac . train xvalues = list ( tc . loss . keys ()) self . clear_plot ( agent_context ) lossvalues = list ( tc . loss . values ()) if self . _is_nan ( lossvalues ): self . plot_text ( 'no loss data available' ) else : self . plot_axes ( xlim = ( 0 , tc . episodes_done_in_training ), xlabel = 'episodes trained' , ylim = self . ylim , ylabel = 'loss' , yscale = self . yscale ) if isinstance ( tc , core . PpoTrainContext ): acc : core . PpoTrainContext = tc self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = lossvalues , color = 'indigo' , pause = False ) self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = list ( acc . actor_loss . values ()), color = 'g' , pause = False ) self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = list ( acc . critic_loss . values ()), color = 'b' ) self . axes . legend (( 'total' , 'actor' , 'critic' )) else : self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = lossvalues , color = 'indigo' ) class Rewards ( _PlotCallback ): def __init__ ( self , yscale : str = 'linear' , ylim : Optional [ Tuple [ float , float ]] = None ): \"\"\"Plots the sum of rewards observed during policy evaluation. Args: yscale: scale of the y-axes ('linear', 'symlog',...) ylim: (min,max) for the y-axes \"\"\" super () . __init__ ( core . PlotType . TRAIN_ITERATION | core . PlotType . TRAIN_EVAL | core . PlotType . PLAY_EPISODE ) self . ylim = ylim self . yscale = yscale def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ): xvalues = yvalues = [] ylabel = 'sum of rewards' if agent_context . is_train or agent_context . is_eval : tc = agent_context . train xvalues = list ( tc . eval_rewards . keys ()) yvalues = list ( tc . eval_rewards . values ()) ylabel = '\u00d8 sum of rewards' if agent_context . is_play : pc = agent_context . play xvalues = list ( pc . sum_of_rewards . keys ()) yvalues = list ( pc . sum_of_rewards . values ()) if xvalues : self . plot_subplot ( agent_context , color = 'green' , ylim = self . ylim , yscale = self . yscale , xvalues = xvalues , yvalues = yvalues , ylabel = ylabel ) class State ( _PlotCallback ): \"\"\"Renders the gym state as a plot to the pyplot figure using gym.render('rgb_array'). During training only the last state of the last game evaluation is plotted. During play all states are plotted. \"\"\" def __init__ ( self , mode = 'rgb_array' ): \"\"\" Args: mode: the render mode passed to gym.render(), yielding an rgb_array \"\"\" super () . __init__ ( plot_type = core . PlotType . PLAY_STEP | core . PlotType . TRAIN_EVAL ) self . _render_mode = mode def _plot_rgb_array ( self , agent_context : core . AgentContext , rgb_array : np . ndarray ): \"\"\"Renders rgb_array to the current subplot.\"\"\" assert rgb_array is not None ax = self . axes xlabel = '' if agent_context . is_eval : xlabel = \"'done state' of last evaluation episode\" ax . imshow ( rgb_array ) ax . set_xlabel ( xlabel ) ax . get_xaxis () . set_ticks ([]) ax . get_yaxis () . set_ticks ([]) axes_color = self . axes_color for spin in ax . spines : ax . spines [ spin ] . set_visible ( True ) ax . spines [ spin ] . set_color ( axes_color ) # noinspection PyArgumentList,DuplicatedCode def _render_to_rgb_array ( self , gym_env : gym . Env , mode : str ) -> np . ndarray : \"\"\" calls gym_env.render(mode) and validates the return value to be a numpy rgb array throws an exception if not an rgb array Returns: numpy rgb array \"\"\" result = gym_env . render ( mode = mode ) assert result is not None , f 'gym_env.render(mode= {mode} ) yielded None' assert isinstance ( result , np . ndarray ), f 'gym_env.render(mode= {mode} ) did not yield a numpy.ndarray.' assert result . min () >= 0 , f 'gym_env.render(mode= {mode} ) contains negative values => not an rgb array' assert result . max () <= 255 , f 'gym_env.render(mode= {mode} ) contains values > 255 => not an rgb array' assert len ( result . shape ) == 3 , f 'gym_env.render(mode= {mode} ) shape is not of the form (x,y,n)' assert result . shape [ 2 ] == 3 or result . shape [ 2 ] == 4 , \\ f 'gym_env.render(mode= {mode} ) shape is not of the form (x,y,3|4)' return result def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ): self . clear_plot ( agent_context ) try : rgb_array : np . ndarray = self . _render_to_rgb_array ( agent_context . play . gym_env , self . _render_mode ) self . _plot_rgb_array ( agent_context , rgb_array ) except : self . plot_text ( f 'gym.Env.render(mode=\" {self._render_mode} \") failed: \\n ' ) class Steps ( _PlotCallback ): def __init__ ( self , yscale : str = 'linear' , ylim : Optional [ Tuple [ float , float ]] = None ): \"\"\"Plots the step counts observed during policy evaluation. Args: yscale: scale of the y-axes ('linear','log') ylim: (min,max) for the y-axes \"\"\" super () . __init__ ( core . PlotType . TRAIN_ITERATION | core . PlotType . TRAIN_EVAL | core . PlotType . PLAY_EPISODE ) self . ylim = ylim self . yscale = yscale def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ): xvalues = yvalues = [] ylabel = 'steps' if agent_context . is_train or agent_context . is_eval : tc = agent_context . train xvalues = list ( tc . eval_steps . keys ()) yvalues = list ( tc . eval_steps . values ()) ylabel = '\u00d8 steps' if agent_context . is_play : pc = agent_context . play xvalues = list ( pc . actions . keys ()) yvalues = [ len ( pc . actions [ episode ]) for episode in pc . actions . keys ()] self . plot_subplot ( agent_context , color = 'blue' , ylim = self . ylim , yscale = self . yscale , xvalues = xvalues , yvalues = yvalues , ylabel = ylabel ) class StepRewards ( _PlotCallback ): def __init__ ( self , num_steps_between_plot = 100 ): \"\"\"Plots the sum of rewards up to the current step during play or at the end of an evaluation period. Args: num_steps_between_plot: num of steps to play before plot is updated. \"\"\" super () . __init__ ( core . PlotType . PLAY_STEP | core . PlotType . PLAY_EPISODE | core . PlotType . TRAIN_EVAL ) assert num_steps_between_plot > 0 self . _xy_values : Dict [ int , Tuple [ List [ int ], List [ float ]]] = dict () self . _xmax : int = 0 self . _num_steps_between_plot = num_steps_between_plot def _replot ( self , agent_context : core . AgentContext ): if self . _xmax >= 1 : self . clear_plot ( agent_context ) xlabel = 'steps played' if agent_context . is_eval : xlabel = 'steps taken during last evaluation period' self . plot_axes ( xlim = ( 1 , self . _xmax ), ylabel = 'sum of rewards' , xlabel = xlabel ) xy_values = list ( self . _xy_values . values ()) xlast , _ = xy_values [ - 1 ] for xvalues , yvalues in xy_values : pause = ( xvalues == xlast ) self . plot_values ( agent_context = agent_context , xvalues = xvalues , yvalues = yvalues , color = None , marker = '' , pause = pause ) def _reset ( self ): self . _xy_values = dict () self . _xmax = 0 def on_play_begin ( self , agent_context : core . AgentContext ): super () . on_play_begin ( agent_context ) self . _reset () def on_play_end ( self , agent_context : core . AgentContext ): super () . on_play_end ( agent_context ) self . _reset () def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ): pc = agent_context . play if plot_type & core . PlotType . PLAY_STEP != core . PlotType . NONE : episode = pc . episodes_done + 1 if episode not in self . _xy_values : self . _xy_values [ episode ] = ([], []) xvalues , yvalues = self . _xy_values [ episode ] xvalues . append ( pc . steps_done_in_episode ) yvalues . append ( pc . sum_of_rewards [ episode ]) if pc . steps_done_in_episode > self . _xmax : self . _xmax = pc . steps_done_in_episode if ( episode == 1 and pc . steps_done == 1 ) or ( pc . steps_done % self . _num_steps_between_plot ) == 0 : self . _replot ( agent_context ) if plot_type & core . PlotType . PLAY_EPISODE != core . PlotType . NONE : self . _replot ( agent_context ) if plot_type & core . PlotType . TRAIN_EVAL != core . PlotType . NONE : self . _xmax = max ([ len ( step_rewards ) for step_rewards in pc . rewards . values ()]) for episode in pc . rewards . keys (): step_rewards = pc . rewards [ episode ] xvalues = list ( range ( 1 , len ( step_rewards ) + 1 )) yvalues = [] for reward in step_rewards : old_sum = yvalues [ - 1 ] if len ( yvalues ) > 0 else 0 yvalues . append ( old_sum + reward ) self . _xy_values [ episode ] = ( xvalues , yvalues ) self . _replot ( agent_context ) class ToMovie ( core . _PostProcessCallback ): \"\"\"Plots the pyplot figure to an mp4 file Attributes: fps: frame per seconds filepath: the filepath of the mp4 file. \"\"\" def __init__ ( self , fps : Optional [ int ] = None , filepath : str = None ): \"\"\"Writes the ploted graphs and images to the mp4 / gif file given by filepath. if filepath ends in '.gif' an animated gif is created. Args: fps: frames per second filepath: the filepath of the mp4 or gif file file. If None the file is written to a temp file. \"\"\" super () . __init__ () self . fps = fps self . _is_filepath_set = filepath is not None self . filepath = filepath if not self . _is_filepath_set : self . filepath = easyagents . backends . core . _get_temp_path () if ( not self . _is_animated_gif ()) and ( not self . filepath . lower () . endswith ( '.mp4' )): self . filepath = self . filepath + '.mp4' self . _video = imageio . get_writer ( self . filepath , fps = fps ) if fps else imageio . get_writer ( self . filepath ) def _close ( self , agent_context : core . AgentContext ): \"\"\"closes the mp4 file and displays it in jupyter cell (if in a jupyter notebook)\"\"\" self . _video . close () self . _video = None if agent_context . pyplot . is_jupyter_active : with open ( self . filepath , 'rb' ) as f : video = f . read () b64 = base64 . b64encode ( video ) if not self . _is_filepath_set : os . remove ( self . filepath ) width = 640 height = 480 if self . _is_animated_gif (): result = ''' <img src=\"data:image/gif;base64, {2} \" alt=\"easyagents.plot\" width= {0} /> ''' . format ( width , height , b64 . decode ()) else : result = ''' <video width=\" {0} \" height=\" {1} \" controls> <source src=\"data:video/mp4;base64, {2} \" type=\"video/mp4\"> Your browser does not support the video tag. </video>''' . format ( width , height , b64 . decode ()) result = HTML ( result ) # noinspection PyTypeChecker clear_output ( wait = True ) # noinspection PyTypeChecker display ( result ) def _get_rgb_array ( self , agent_context : core . AgentContext ) -> np . ndarray : \"\"\"Yields an rgb array representing the current content of the subplots.\"\"\" pyc = agent_context . pyplot pyc . figure . canvas . draw () result = np . frombuffer ( pyc . figure . canvas . tostring_rgb (), dtype = 'uint8' ) result = result . reshape ( pyc . figure . canvas . get_width_height ()[:: - 1 ] + ( 3 ,)) return result def _is_animated_gif ( self ): return self . filepath . lower () . endswith ( '.gif' ) def _write_figure_to_video ( self , agent_context : core . AgentContext ): \"\"\"Appends the current pyplot figure to the video. if an exception occures no frame is added. \"\"\" try : rgb_array = self . _get_rgb_array ( agent_context ) self . _video . append_data ( rgb_array ) except : pass def on_play_episode_end ( self , agent_context : core . AgentContext ): if agent_context . _is_plot_ready ( core . PlotType . PLAY_EPISODE | core . PlotType . TRAIN_EVAL ): self . _write_figure_to_video ( agent_context ) def on_play_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): if agent_context . _is_plot_ready ( core . PlotType . PLAY_STEP ): self . _write_figure_to_video ( agent_context ) def on_train_iteration_end ( self , agent_context : core . AgentContext ): if agent_context . _is_plot_ready ( core . PlotType . TRAIN_ITERATION ): self . _write_figure_to_video ( agent_context ) def on_play_end ( self , agent_context : core . AgentContext ): if agent_context . _is_plot_ready ( core . PlotType . PLAY_EPISODE ): self . _close ( agent_context ) def on_train_end ( self , agent_context : core . AgentContext ): self . _close ( agent_context )","title":"Module easyagents.callbacks.plot"},{"location":"reference/easyagents/callbacks/plot/#variables","text":"on_play_end_clear_jupyter_display on_train_end_clear_jupyter_display shell","title":"Variables"},{"location":"reference/easyagents/callbacks/plot/#classes","text":"","title":"Classes"},{"location":"reference/easyagents/callbacks/plot/#actions","text":"class Actions ( num_steps_between_plot = 100 ) Base class of plyplot callbacks generating a plot after a trained iteration or an episode played. Attributes: axes: the subplot to plot onto View Source class Actions ( _PlotCallback ) : def __init__ ( self , num_steps_between_plot = 100 ) : \"\"\"Plots a histogram of the actions taken during play or during the last evaluation period. Args: num_steps_between_plot: num of steps to play before plot is updated. \"\"\" super (). __init__ ( plot_type = core . PlotType . PLAY_STEP | core . PlotType . PLAY_EPISODE | core . PlotType . TRAIN_EVAL ) assert num_steps_between_plot > 0 self . _actions : List [ float ] = [] self . _num_steps_between_plot = num_steps_between_plot def _reset ( self ) : self . _actions : List [ float ] = [] def on_play_begin ( self , agent_context : core . AgentContext ) : super (). on_play_begin ( agent_context ) self . _reset () def on_play_end ( self , agent_context : core . AgentContext ) : super (). on_play_end ( agent_context ) self . _reset () def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ) : try : pc = agent_context . play is_plot = True xlabel = 'actions' if plot_type & core . PlotType . TRAIN_EVAL != core . PlotType . NONE : xlabel = 'actions taken during last evaluation period' if plot_type & core . PlotType . PLAY_STEP != core . PlotType . NONE : is_plot = ( pc . steps_done_in_episode % self . _num_steps_between_plot == 0 ) if is_plot : self . clear_plot ( agent_context ) self . plot_axes ( xlabel = xlabel , ylabel = 'count' ) self . axes . hist ( pc . actions . values ()) except : self . plot_text ( f 'Failed to create the actions histogram.\\n' )","title":"Actions"},{"location":"reference/easyagents/callbacks/plot/#ancestors-in-mro","text":"easyagents.callbacks.plot._PlotCallback easyagents.core.AgentCallback abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/callbacks/plot/#methods","text":"","title":"Methods"},{"location":"reference/easyagents/callbacks/plot/#clear_plot","text":"def clear_plot ( self , agent_context : easyagents . core . AgentContext ) Clears the axes for this plot. Should be called by self.plot before replotting an axes. View Source def clear_plot ( self , agent_context : core . AgentContext ): \"\"\"Clears the axes for this plot. Should be called by self.plot before replotting an axes.\"\"\" assert self . axes is not None pyc = agent_context . pyplot if pyc . is_jupyter_active : self . axes . cla () else : plt . cla ()","title":"clear_plot"},{"location":"reference/easyagents/callbacks/plot/#on_api_log","text":"def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass","title":"on_api_log"},{"location":"reference/easyagents/callbacks/plot/#on_gym_init_begin","text":"def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\"","title":"on_gym_init_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_init_end","text":"def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass","title":"on_gym_init_end"},{"location":"reference/easyagents/callbacks/plot/#on_gym_reset_begin","text":"def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\"","title":"on_gym_reset_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_reset_end","text":"def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass","title":"on_gym_reset_end"},{"location":"reference/easyagents/callbacks/plot/#on_gym_step_begin","text":"def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass","title":"on_gym_step_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_step_end","text":"def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass","title":"on_gym_step_end"},{"location":"reference/easyagents/callbacks/plot/#on_log","text":"def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass","title":"on_log"},{"location":"reference/easyagents/callbacks/plot/#on_play_begin","text":"def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : core . AgentContext ): super (). on_play_begin ( agent_context ) self . _reset ()","title":"on_play_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_end","text":"def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : core . AgentContext ): super (). on_play_end ( agent_context ) self . _reset ()","title":"on_play_end"},{"location":"reference/easyagents/callbacks/plot/#on_play_episode_begin","text":"def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\"","title":"on_play_episode_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_episode_end","text":"def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_EPISODE )","title":"on_play_episode_end"},{"location":"reference/easyagents/callbacks/plot/#on_play_step_begin","text":"def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\"","title":"on_play_step_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_step_end","text":"def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_STEP )","title":"on_play_step_end"},{"location":"reference/easyagents/callbacks/plot/#on_train_begin","text":"def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : core . AgentContext ): if ( self . _plot_type & ( core . PlotType . TRAIN_EVAL | core . PlotType . TRAIN_ITERATION )) != core . PlotType . NONE : self . _create_subplot ( agent_context )","title":"on_train_begin"},{"location":"reference/easyagents/callbacks/plot/#on_train_end","text":"def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\"","title":"on_train_end"},{"location":"reference/easyagents/callbacks/plot/#on_train_iteration_begin","text":"def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\"","title":"on_train_iteration_begin"},{"location":"reference/easyagents/callbacks/plot/#on_train_iteration_end","text":"def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_ITERATION )","title":"on_train_iteration_end"},{"location":"reference/easyagents/callbacks/plot/#plot","text":"def plot ( self , agent_context : easyagents . core . AgentContext , plot_type : easyagents . core . PlotType ) Plots a graph on the self.axes object. Args: agent_context: the context containing the data to plot plot_type: the PlotType requesting this plot call. View Source def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ): try : pc = agent_context . play is_plot = True xlabel = 'actions' if plot_type & core . PlotType . TRAIN_EVAL != core . PlotType . NONE : xlabel = 'actions taken during last evaluation period' if plot_type & core . PlotType . PLAY_STEP != core . PlotType . NONE : is_plot = ( pc . steps_done_in_episode % self . _num_steps_between_plot == 0 ) if is_plot : self . clear_plot ( agent_context ) self . plot_axes ( xlabel = xlabel , ylabel = 'count' ) self . axes . hist ( pc . actions . values ()) except : self . plot_text ( f 'Failed to create the actions histogram.\\n' )","title":"plot"},{"location":"reference/easyagents/callbacks/plot/#plot_axes","text":"def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None ) Draws the x- and y-axes. Attributes: xlim: None or (min,max) for the x-axes xlabel: label of the x-axes ylim: None or (min,max) for the y-axes (or None) ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log', ...) View Source def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = 'linear' , ylim : Optional [ Tuple [ float , float ]] = None ): \"\"\"Draws the x- and y-axes. Attributes: xlim: None or (min,max) for the x-axes xlabel: label of the x-axes ylim: None or (min,max) for the y-axes (or None) ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log', ...) \"\"\" assert xlim is None or xlim [ 0 ] <= xlim [ 1 ] assert ylim is None or ylim [ 0 ] <= ylim [ 1 ] assert xlabel is not None assert ylabel is not None # setup subplot ( axes , labels , colors ) axes_color = self . axes_color self . axes . set_xlabel ( xlabel ) self . axes . set_ylabel ( ylabel ) self . axes . spines [ 'top' ]. set_visible ( False ) self . axes . spines [ 'right' ]. set_visible ( False ) self . axes . spines [ 'bottom' ]. set_color ( axes_color ) self . axes . spines [ 'left' ]. set_color ( axes_color ) self . axes . grid ( color = axes_color , linestyle = '-' , linewidth = 0 . 25 , alpha = 0 . 5 ) if xlim is not None : self . axes . set_xlim ( xlim ) if ylim is not None : self . axes . set_ylim ( ylim ) self . axes . set_yscale ( yscale )","title":"plot_axes"},{"location":"reference/easyagents/callbacks/plot/#plot_subplot","text":"def plot_subplot ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], ylabel : str , xlabel : str = None , xlim : Union [ Tuple [ float , float ], NoneType ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None , color : str = 'blue' ) Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) ylim: (min,max) for the x-axes xlabel: label of the x-axes ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log',...) ylim: (min,max) for the y-axes (or None) color: the graphs color (must be the name of a matplotlib color) View Source def plot_subplot ( self , agent_context : core . AgentContext , xvalues : List [ int ] , yvalues : List [ Union[float, Tuple[float, float, float ] ]] , ylabel : str , xlabel : str = None , xlim : Optional [ Tuple[float, float ] ] = None , yscale : str = 'linear' , ylim : Optional [ Tuple[float, float ] ] = None , color : str = 'blue' ) : \"\"\"Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) ylim: (min,max) for the x-axes xlabel: label of the x-axes ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log',...) ylim: (min,max) for the y-axes (or None) color: the graphs color (must be the name of a matplotlib color) \"\"\" if not xlim : xmin = 0 xmax = 1 if agent_context . is_play : xmin = 1 xmax = agent_context . play . episodes_done if agent_context . is_train or agent_context . is_eval : xmin = 0 xmax = agent_context . train . episodes_done_in_training xlim = ( xmin , xmax ) if xlabel is None : xlabel = 'episodes' if agent_context . is_play : xlabel = 'episodes played' if agent_context . is_eval or agent_context . is_train : xlabel = 'episodes trained' self . clear_plot ( agent_context ) self . plot_axes ( xlim = xlim , xlabel = xlabel , ylabel = ylabel , yscale = yscale , ylim = ylim ) self . plot_values ( agent_context = agent_context , xvalues = xvalues , yvalues = yvalues , color = color )","title":"plot_subplot"},{"location":"reference/easyagents/callbacks/plot/#plot_text","text":"def plot_text ( self , text : str ) View Source def plot_text ( self , text : str ) : if text : ax = self . axes ax . text ( 0.5 , 0.5 , text , horizontalalignment = 'center' , verticalalignment = 'center' , color = 'blue' , wrap = True ) ax . set_xlabel ( '' ) ax . get_xaxis (). set_ticks ( [] ) ax . get_yaxis (). set_ticks ( [] ) axes_color = self . axes_color for spin in ax . spines : ax . spines [ spin ] . set_visible ( True ) ax . spines [ spin ] . set_color ( axes_color )","title":"plot_text"},{"location":"reference/easyagents/callbacks/plot/#plot_values","text":"def plot_values ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], color : Union [ str , NoneType ] = 'blue' , marker : str = None , pause : bool = True ) Draws the graph given by xvalues, yvalues. Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) color: the graphs color (must be the name of a matplotlib color) or None marker: if None a marker is used if the graph has just a few points. pause: pause to redraw the plot. View Source def plot_values ( self , agent_context : core . AgentContext , xvalues : List [ int ] , yvalues : List [ Union[float, Tuple[float, float, float ] ]] , color : Optional [ str ] = 'blue' , marker : str = None , pause : bool = True ) : \"\"\"Draws the graph given by xvalues, yvalues. Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) color: the graphs color (must be the name of a matplotlib color) or None marker: if None a marker is used if the graph has just a few points. pause: pause to redraw the plot. \"\"\" assert xvalues is not None assert yvalues is not None assert len ( xvalues ) == len ( yvalues ), \"xvalues do not match yvalues\" pyc : core . PyPlotContext = agent_context . pyplot # extract min / max and y values if yvalues is of the form [ (min,y,max),...) yminvalues = None ymaxvalues = None if len(yvalues) > 0 and isinstance(yvalues[0 ] , tuple ) : ymaxvalues = [ t[2 ] for t in yvalues ] yminvalues = [ t[0 ] for t in yvalues ] yvalues = [ t[1 ] for t in yvalues ] # plot values marker = marker if marker is None and len ( xvalues ) < 10 : marker = 'o' fill_alpha = 0.1 if pyc . is_jupyter_active : if yminvalues is not None : self . axes . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) self . axes . plot ( xvalues , yvalues , color = color , marker = marker ) else : if yminvalues is not None : plt . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) plt . plot ( xvalues , yvalues , color = color , marker = marker ) if pause : plt . pause ( 0.01 )","title":"plot_values"},{"location":"reference/easyagents/callbacks/plot/#clear","text":"class Clear ( on_play : bool = True , on_train : bool = True ) Configures the clearing of plots in the jupyter output cell after calls to train or play. View Source class Clear ( core . AgentCallback ): \"\"\"Configures the clearing of plots in the jupyter output cell after calls to train or play.\"\"\" def __init__ ( self , on_play: bool = True , on_train: bool = True ): \"\"\"Define the cell clearing behaviour after agent.train and agent.play. Args: on_play: if set the output cell is cleared after agent.play if a plot exists on_train: if set the output cell is cleared after agent.train if a plot exists \"\"\" self . _on_play: bool = on_play self . _on_train: bool = on_train on_play_end_clear_jupyter_display = self . _on_play on_train_end_clear_jupyter_display = self . _on_train def on_play_begin ( self , agent_context: core . AgentContext ): global on_play_end_clear_jupyter_display on_play_end_clear_jupyter_display = self . _on_play def on_train_begin ( self , agent_context: core . AgentContext ): global on_train_end_clear_jupyter_display on_train_end_clear_jupyter_display = self . _on_train","title":"Clear"},{"location":"reference/easyagents/callbacks/plot/#ancestors-in-mro_1","text":"easyagents.core.AgentCallback abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/callbacks/plot/#methods_1","text":"","title":"Methods"},{"location":"reference/easyagents/callbacks/plot/#on_api_log_1","text":"def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass","title":"on_api_log"},{"location":"reference/easyagents/callbacks/plot/#on_gym_init_begin_1","text":"def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\"","title":"on_gym_init_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_init_end_1","text":"def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass","title":"on_gym_init_end"},{"location":"reference/easyagents/callbacks/plot/#on_gym_reset_begin_1","text":"def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\"","title":"on_gym_reset_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_reset_end_1","text":"def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass","title":"on_gym_reset_end"},{"location":"reference/easyagents/callbacks/plot/#on_gym_step_begin_1","text":"def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass","title":"on_gym_step_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_step_end_1","text":"def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass","title":"on_gym_step_end"},{"location":"reference/easyagents/callbacks/plot/#on_log_1","text":"def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass","title":"on_log"},{"location":"reference/easyagents/callbacks/plot/#on_play_begin_1","text":"def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : core . AgentContext ): global on_play_end_clear_jupyter_display on_play_end_clear_jupyter_display = self . _on_play","title":"on_play_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_end_1","text":"def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.play() call (during play or eval, but not during train)\"\"\"","title":"on_play_end"},{"location":"reference/easyagents/callbacks/plot/#on_play_episode_begin_1","text":"def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\"","title":"on_play_episode_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_episode_end_1","text":"def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : AgentContext ): \"\"\"Called once after an episode is done or stopped (during play or eval, but not during train).\"\"\"","title":"on_play_episode_end"},{"location":"reference/easyagents/callbacks/plot/#on_play_step_begin_1","text":"def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\"","title":"on_play_step_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_step_end_1","text":"def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"Called once after a step is completed in the current episode (during play or eval, but not during train).\"\"\"","title":"on_play_step_end"},{"location":"reference/easyagents/callbacks/plot/#on_train_begin_1","text":"def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : core . AgentContext ): global on_train_end_clear_jupyter_display on_train_end_clear_jupyter_display = self . _on_train","title":"on_train_begin"},{"location":"reference/easyagents/callbacks/plot/#on_train_end_1","text":"def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\"","title":"on_train_end"},{"location":"reference/easyagents/callbacks/plot/#on_train_iteration_begin_1","text":"def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\"","title":"on_train_iteration_begin"},{"location":"reference/easyagents/callbacks/plot/#on_train_iteration_end_1","text":"def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : AgentContext ): \"\"\"Called once after the current iteration is completed\"\"\"","title":"on_train_iteration_end"},{"location":"reference/easyagents/callbacks/plot/#loss","text":"class Loss ( yscale : str = 'symlog' , ylim : Union [ Tuple [ float , float ], NoneType ] = None ) Base class of plyplot callbacks generating a plot after a trained iteration or an episode played. Attributes: axes: the subplot to plot onto View Source class Loss ( _PlotCallback ): def __init__ ( self , yscale: str = 'symlog' , ylim: Optional [ Tuple [ float , float ]] = None ): \"\"\"Plots the loss resulting from each iterations policy training. Hints: o for actro-critic agents the loss from training the actor- and critic-networks are plotted along with the total loss. Args: yscale: scale of the y-axes ('linear', 'symlog',...) ylim: (min,max) for the y-axes \"\"\" super (). __init__ ( plot_type = core . PlotType . TRAIN_ITERATION ) self . ylim = ylim self . yscale = yscale def plot ( self , agent_context: core . AgentContext , plot_type: core . PlotType ): ac = agent_context tc = ac . train xvalues = list ( tc . loss . keys ()) self . clear_plot ( agent_context ) lossvalues = list ( tc . loss . values ()) if self . _is_nan ( lossvalues ): self . plot_text ( 'no loss data available' ) else: self . plot_axes ( xlim =( 0 , tc . episodes_done_in_training ), xlabel = 'episodes trained' , ylim = self . ylim , ylabel = 'loss' , yscale = self . yscale ) if isinstance ( tc , core . PpoTrainContext ): acc: core . PpoTrainContext = tc self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = lossvalues , color = 'indigo' , pause = False ) self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = list ( acc . actor_loss . values ()), color = 'g' , pause = False ) self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = list ( acc . critic_loss . values ()), color = 'b' ) self . axes . legend (( 'total' , 'actor' , 'critic' )) else: self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = lossvalues , color = 'indigo' )","title":"Loss"},{"location":"reference/easyagents/callbacks/plot/#ancestors-in-mro_2","text":"easyagents.callbacks.plot._PlotCallback easyagents.core.AgentCallback abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/callbacks/plot/#methods_2","text":"","title":"Methods"},{"location":"reference/easyagents/callbacks/plot/#clear_plot_1","text":"def clear_plot ( self , agent_context : easyagents . core . AgentContext ) Clears the axes for this plot. Should be called by self.plot before replotting an axes. View Source def clear_plot ( self , agent_context : core . AgentContext ): \"\"\"Clears the axes for this plot. Should be called by self.plot before replotting an axes.\"\"\" assert self . axes is not None pyc = agent_context . pyplot if pyc . is_jupyter_active : self . axes . cla () else : plt . cla ()","title":"clear_plot"},{"location":"reference/easyagents/callbacks/plot/#on_api_log_2","text":"def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass","title":"on_api_log"},{"location":"reference/easyagents/callbacks/plot/#on_gym_init_begin_2","text":"def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\"","title":"on_gym_init_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_init_end_2","text":"def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass","title":"on_gym_init_end"},{"location":"reference/easyagents/callbacks/plot/#on_gym_reset_begin_2","text":"def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\"","title":"on_gym_reset_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_reset_end_2","text":"def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass","title":"on_gym_reset_end"},{"location":"reference/easyagents/callbacks/plot/#on_gym_step_begin_2","text":"def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass","title":"on_gym_step_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_step_end_2","text":"def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass","title":"on_gym_step_end"},{"location":"reference/easyagents/callbacks/plot/#on_log_2","text":"def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass","title":"on_log"},{"location":"reference/easyagents/callbacks/plot/#on_play_begin_2","text":"def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : core . AgentContext ): if ( self . _plot_type & ( core . PlotType . PLAY_EPISODE | core . PlotType . PLAY_STEP )) != core . PlotType . NONE : self . _create_subplot ( agent_context )","title":"on_play_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_end_2","text":"def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_EVAL )","title":"on_play_end"},{"location":"reference/easyagents/callbacks/plot/#on_play_episode_begin_2","text":"def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\"","title":"on_play_episode_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_episode_end_2","text":"def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_EPISODE )","title":"on_play_episode_end"},{"location":"reference/easyagents/callbacks/plot/#on_play_step_begin_2","text":"def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\"","title":"on_play_step_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_step_end_2","text":"def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_STEP )","title":"on_play_step_end"},{"location":"reference/easyagents/callbacks/plot/#on_train_begin_2","text":"def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : core . AgentContext ): if ( self . _plot_type & ( core . PlotType . TRAIN_EVAL | core . PlotType . TRAIN_ITERATION )) != core . PlotType . NONE : self . _create_subplot ( agent_context )","title":"on_train_begin"},{"location":"reference/easyagents/callbacks/plot/#on_train_end_2","text":"def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\"","title":"on_train_end"},{"location":"reference/easyagents/callbacks/plot/#on_train_iteration_begin_2","text":"def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\"","title":"on_train_iteration_begin"},{"location":"reference/easyagents/callbacks/plot/#on_train_iteration_end_2","text":"def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_ITERATION )","title":"on_train_iteration_end"},{"location":"reference/easyagents/callbacks/plot/#plot_1","text":"def plot ( self , agent_context : easyagents . core . AgentContext , plot_type : easyagents . core . PlotType ) Plots a graph on the self.axes object. Args: agent_context: the context containing the data to plot plot_type: the PlotType requesting this plot call. View Source def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ): ac = agent_context tc = ac . train xvalues = list ( tc . loss . keys ()) self . clear_plot ( agent_context ) lossvalues = list ( tc . loss . values ()) if self . _is_nan ( lossvalues ): self . plot_text ( 'no loss data available' ) else : self . plot_axes ( xlim = ( 0 , tc . episodes_done_in_training ), xlabel = 'episodes trained' , ylim = self . ylim , ylabel = 'loss' , yscale = self . yscale ) if isinstance ( tc , core . PpoTrainContext ): acc : core . PpoTrainContext = tc self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = lossvalues , color = 'indigo' , pause = False ) self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = list ( acc . actor_loss . values ()), color = 'g' , pause = False ) self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = list ( acc . critic_loss . values ()), color = 'b' ) self . axes . legend (( 'total' , 'actor' , 'critic' )) else : self . plot_values ( agent_context = ac , xvalues = xvalues , yvalues = lossvalues , color = 'indigo' )","title":"plot"},{"location":"reference/easyagents/callbacks/plot/#plot_axes_1","text":"def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None ) Draws the x- and y-axes. Attributes: xlim: None or (min,max) for the x-axes xlabel: label of the x-axes ylim: None or (min,max) for the y-axes (or None) ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log', ...) View Source def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = 'linear' , ylim : Optional [ Tuple [ float , float ]] = None ): \"\"\"Draws the x- and y-axes. Attributes: xlim: None or (min,max) for the x-axes xlabel: label of the x-axes ylim: None or (min,max) for the y-axes (or None) ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log', ...) \"\"\" assert xlim is None or xlim [ 0 ] <= xlim [ 1 ] assert ylim is None or ylim [ 0 ] <= ylim [ 1 ] assert xlabel is not None assert ylabel is not None # setup subplot ( axes , labels , colors ) axes_color = self . axes_color self . axes . set_xlabel ( xlabel ) self . axes . set_ylabel ( ylabel ) self . axes . spines [ 'top' ]. set_visible ( False ) self . axes . spines [ 'right' ]. set_visible ( False ) self . axes . spines [ 'bottom' ]. set_color ( axes_color ) self . axes . spines [ 'left' ]. set_color ( axes_color ) self . axes . grid ( color = axes_color , linestyle = '-' , linewidth = 0 . 25 , alpha = 0 . 5 ) if xlim is not None : self . axes . set_xlim ( xlim ) if ylim is not None : self . axes . set_ylim ( ylim ) self . axes . set_yscale ( yscale )","title":"plot_axes"},{"location":"reference/easyagents/callbacks/plot/#plot_subplot_1","text":"def plot_subplot ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], ylabel : str , xlabel : str = None , xlim : Union [ Tuple [ float , float ], NoneType ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None , color : str = 'blue' ) Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) ylim: (min,max) for the x-axes xlabel: label of the x-axes ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log',...) ylim: (min,max) for the y-axes (or None) color: the graphs color (must be the name of a matplotlib color) View Source def plot_subplot ( self , agent_context : core . AgentContext , xvalues : List [ int ] , yvalues : List [ Union[float, Tuple[float, float, float ] ]] , ylabel : str , xlabel : str = None , xlim : Optional [ Tuple[float, float ] ] = None , yscale : str = 'linear' , ylim : Optional [ Tuple[float, float ] ] = None , color : str = 'blue' ) : \"\"\"Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) ylim: (min,max) for the x-axes xlabel: label of the x-axes ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log',...) ylim: (min,max) for the y-axes (or None) color: the graphs color (must be the name of a matplotlib color) \"\"\" if not xlim : xmin = 0 xmax = 1 if agent_context . is_play : xmin = 1 xmax = agent_context . play . episodes_done if agent_context . is_train or agent_context . is_eval : xmin = 0 xmax = agent_context . train . episodes_done_in_training xlim = ( xmin , xmax ) if xlabel is None : xlabel = 'episodes' if agent_context . is_play : xlabel = 'episodes played' if agent_context . is_eval or agent_context . is_train : xlabel = 'episodes trained' self . clear_plot ( agent_context ) self . plot_axes ( xlim = xlim , xlabel = xlabel , ylabel = ylabel , yscale = yscale , ylim = ylim ) self . plot_values ( agent_context = agent_context , xvalues = xvalues , yvalues = yvalues , color = color )","title":"plot_subplot"},{"location":"reference/easyagents/callbacks/plot/#plot_text_1","text":"def plot_text ( self , text : str ) View Source def plot_text ( self , text : str ) : if text : ax = self . axes ax . text ( 0.5 , 0.5 , text , horizontalalignment = 'center' , verticalalignment = 'center' , color = 'blue' , wrap = True ) ax . set_xlabel ( '' ) ax . get_xaxis (). set_ticks ( [] ) ax . get_yaxis (). set_ticks ( [] ) axes_color = self . axes_color for spin in ax . spines : ax . spines [ spin ] . set_visible ( True ) ax . spines [ spin ] . set_color ( axes_color )","title":"plot_text"},{"location":"reference/easyagents/callbacks/plot/#plot_values_1","text":"def plot_values ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], color : Union [ str , NoneType ] = 'blue' , marker : str = None , pause : bool = True ) Draws the graph given by xvalues, yvalues. Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) color: the graphs color (must be the name of a matplotlib color) or None marker: if None a marker is used if the graph has just a few points. pause: pause to redraw the plot. View Source def plot_values ( self , agent_context : core . AgentContext , xvalues : List [ int ] , yvalues : List [ Union[float, Tuple[float, float, float ] ]] , color : Optional [ str ] = 'blue' , marker : str = None , pause : bool = True ) : \"\"\"Draws the graph given by xvalues, yvalues. Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) color: the graphs color (must be the name of a matplotlib color) or None marker: if None a marker is used if the graph has just a few points. pause: pause to redraw the plot. \"\"\" assert xvalues is not None assert yvalues is not None assert len ( xvalues ) == len ( yvalues ), \"xvalues do not match yvalues\" pyc : core . PyPlotContext = agent_context . pyplot # extract min / max and y values if yvalues is of the form [ (min,y,max),...) yminvalues = None ymaxvalues = None if len(yvalues) > 0 and isinstance(yvalues[0 ] , tuple ) : ymaxvalues = [ t[2 ] for t in yvalues ] yminvalues = [ t[0 ] for t in yvalues ] yvalues = [ t[1 ] for t in yvalues ] # plot values marker = marker if marker is None and len ( xvalues ) < 10 : marker = 'o' fill_alpha = 0.1 if pyc . is_jupyter_active : if yminvalues is not None : self . axes . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) self . axes . plot ( xvalues , yvalues , color = color , marker = marker ) else : if yminvalues is not None : plt . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) plt . plot ( xvalues , yvalues , color = color , marker = marker ) if pause : plt . pause ( 0.01 )","title":"plot_values"},{"location":"reference/easyagents/callbacks/plot/#rewards","text":"class Rewards ( yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None ) Base class of plyplot callbacks generating a plot after a trained iteration or an episode played. Attributes: axes: the subplot to plot onto View Source class Rewards ( _PlotCallback ): def __init__ ( self , yscale: str = 'linear' , ylim: Optional [ Tuple [ float , float ]] = None ): \"\"\"Plots the sum of rewards observed during policy evaluation. Args: yscale: scale of the y-axes ('linear', 'symlog',...) ylim: (min,max) for the y-axes \"\"\" super (). __init__ ( core . PlotType . TRAIN_ITERATION | core . PlotType . TRAIN_EVAL | core . PlotType . PLAY_EPISODE ) self . ylim = ylim self . yscale = yscale def plot ( self , agent_context: core . AgentContext , plot_type: core . PlotType ): xvalues = yvalues = [] ylabel = 'sum of rewards' if agent_context . is_train or agent_context . is_eval: tc = agent_context . train xvalues = list ( tc . eval_rewards . keys ()) yvalues = list ( tc . eval_rewards . values ()) ylabel = '\u00d8 sum of rewards' if agent_context . is_play: pc = agent_context . play xvalues = list ( pc . sum_of_rewards . keys ()) yvalues = list ( pc . sum_of_rewards . values ()) if xvalues: self . plot_subplot ( agent_context , color = 'green' , ylim = self . ylim , yscale = self . yscale , xvalues = xvalues , yvalues = yvalues , ylabel = ylabel )","title":"Rewards"},{"location":"reference/easyagents/callbacks/plot/#ancestors-in-mro_3","text":"easyagents.callbacks.plot._PlotCallback easyagents.core.AgentCallback abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/callbacks/plot/#methods_3","text":"","title":"Methods"},{"location":"reference/easyagents/callbacks/plot/#clear_plot_2","text":"def clear_plot ( self , agent_context : easyagents . core . AgentContext ) Clears the axes for this plot. Should be called by self.plot before replotting an axes. View Source def clear_plot ( self , agent_context : core . AgentContext ): \"\"\"Clears the axes for this plot. Should be called by self.plot before replotting an axes.\"\"\" assert self . axes is not None pyc = agent_context . pyplot if pyc . is_jupyter_active : self . axes . cla () else : plt . cla ()","title":"clear_plot"},{"location":"reference/easyagents/callbacks/plot/#on_api_log_3","text":"def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass","title":"on_api_log"},{"location":"reference/easyagents/callbacks/plot/#on_gym_init_begin_3","text":"def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\"","title":"on_gym_init_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_init_end_3","text":"def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass","title":"on_gym_init_end"},{"location":"reference/easyagents/callbacks/plot/#on_gym_reset_begin_3","text":"def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\"","title":"on_gym_reset_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_reset_end_3","text":"def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass","title":"on_gym_reset_end"},{"location":"reference/easyagents/callbacks/plot/#on_gym_step_begin_3","text":"def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass","title":"on_gym_step_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_step_end_3","text":"def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass","title":"on_gym_step_end"},{"location":"reference/easyagents/callbacks/plot/#on_log_3","text":"def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass","title":"on_log"},{"location":"reference/easyagents/callbacks/plot/#on_play_begin_3","text":"def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : core . AgentContext ): if ( self . _plot_type & ( core . PlotType . PLAY_EPISODE | core . PlotType . PLAY_STEP )) != core . PlotType . NONE : self . _create_subplot ( agent_context )","title":"on_play_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_end_3","text":"def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_EVAL )","title":"on_play_end"},{"location":"reference/easyagents/callbacks/plot/#on_play_episode_begin_3","text":"def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\"","title":"on_play_episode_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_episode_end_3","text":"def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_EPISODE )","title":"on_play_episode_end"},{"location":"reference/easyagents/callbacks/plot/#on_play_step_begin_3","text":"def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\"","title":"on_play_step_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_step_end_3","text":"def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_STEP )","title":"on_play_step_end"},{"location":"reference/easyagents/callbacks/plot/#on_train_begin_3","text":"def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : core . AgentContext ): if ( self . _plot_type & ( core . PlotType . TRAIN_EVAL | core . PlotType . TRAIN_ITERATION )) != core . PlotType . NONE : self . _create_subplot ( agent_context )","title":"on_train_begin"},{"location":"reference/easyagents/callbacks/plot/#on_train_end_3","text":"def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\"","title":"on_train_end"},{"location":"reference/easyagents/callbacks/plot/#on_train_iteration_begin_3","text":"def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\"","title":"on_train_iteration_begin"},{"location":"reference/easyagents/callbacks/plot/#on_train_iteration_end_3","text":"def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_ITERATION )","title":"on_train_iteration_end"},{"location":"reference/easyagents/callbacks/plot/#plot_2","text":"def plot ( self , agent_context : easyagents . core . AgentContext , plot_type : easyagents . core . PlotType ) Plots a graph on the self.axes object. Args: agent_context: the context containing the data to plot plot_type: the PlotType requesting this plot call. View Source def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ): xvalues = yvalues = [] ylabel = 'sum of rewards' if agent_context . is_train or agent_context . is_eval : tc = agent_context . train xvalues = list ( tc . eval_rewards . keys ()) yvalues = list ( tc . eval_rewards . values ()) ylabel = '\u00d8 sum of rewards' if agent_context . is_play : pc = agent_context . play xvalues = list ( pc . sum_of_rewards . keys ()) yvalues = list ( pc . sum_of_rewards . values ()) if xvalues : self . plot_subplot ( agent_context , color = 'green' , ylim = self . ylim , yscale = self . yscale , xvalues = xvalues , yvalues = yvalues , ylabel = ylabel )","title":"plot"},{"location":"reference/easyagents/callbacks/plot/#plot_axes_2","text":"def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None ) Draws the x- and y-axes. Attributes: xlim: None or (min,max) for the x-axes xlabel: label of the x-axes ylim: None or (min,max) for the y-axes (or None) ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log', ...) View Source def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = 'linear' , ylim : Optional [ Tuple [ float , float ]] = None ): \"\"\"Draws the x- and y-axes. Attributes: xlim: None or (min,max) for the x-axes xlabel: label of the x-axes ylim: None or (min,max) for the y-axes (or None) ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log', ...) \"\"\" assert xlim is None or xlim [ 0 ] <= xlim [ 1 ] assert ylim is None or ylim [ 0 ] <= ylim [ 1 ] assert xlabel is not None assert ylabel is not None # setup subplot ( axes , labels , colors ) axes_color = self . axes_color self . axes . set_xlabel ( xlabel ) self . axes . set_ylabel ( ylabel ) self . axes . spines [ 'top' ]. set_visible ( False ) self . axes . spines [ 'right' ]. set_visible ( False ) self . axes . spines [ 'bottom' ]. set_color ( axes_color ) self . axes . spines [ 'left' ]. set_color ( axes_color ) self . axes . grid ( color = axes_color , linestyle = '-' , linewidth = 0 . 25 , alpha = 0 . 5 ) if xlim is not None : self . axes . set_xlim ( xlim ) if ylim is not None : self . axes . set_ylim ( ylim ) self . axes . set_yscale ( yscale )","title":"plot_axes"},{"location":"reference/easyagents/callbacks/plot/#plot_subplot_2","text":"def plot_subplot ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], ylabel : str , xlabel : str = None , xlim : Union [ Tuple [ float , float ], NoneType ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None , color : str = 'blue' ) Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) ylim: (min,max) for the x-axes xlabel: label of the x-axes ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log',...) ylim: (min,max) for the y-axes (or None) color: the graphs color (must be the name of a matplotlib color) View Source def plot_subplot ( self , agent_context : core . AgentContext , xvalues : List [ int ] , yvalues : List [ Union[float, Tuple[float, float, float ] ]] , ylabel : str , xlabel : str = None , xlim : Optional [ Tuple[float, float ] ] = None , yscale : str = 'linear' , ylim : Optional [ Tuple[float, float ] ] = None , color : str = 'blue' ) : \"\"\"Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) ylim: (min,max) for the x-axes xlabel: label of the x-axes ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log',...) ylim: (min,max) for the y-axes (or None) color: the graphs color (must be the name of a matplotlib color) \"\"\" if not xlim : xmin = 0 xmax = 1 if agent_context . is_play : xmin = 1 xmax = agent_context . play . episodes_done if agent_context . is_train or agent_context . is_eval : xmin = 0 xmax = agent_context . train . episodes_done_in_training xlim = ( xmin , xmax ) if xlabel is None : xlabel = 'episodes' if agent_context . is_play : xlabel = 'episodes played' if agent_context . is_eval or agent_context . is_train : xlabel = 'episodes trained' self . clear_plot ( agent_context ) self . plot_axes ( xlim = xlim , xlabel = xlabel , ylabel = ylabel , yscale = yscale , ylim = ylim ) self . plot_values ( agent_context = agent_context , xvalues = xvalues , yvalues = yvalues , color = color )","title":"plot_subplot"},{"location":"reference/easyagents/callbacks/plot/#plot_text_2","text":"def plot_text ( self , text : str ) View Source def plot_text ( self , text : str ) : if text : ax = self . axes ax . text ( 0.5 , 0.5 , text , horizontalalignment = 'center' , verticalalignment = 'center' , color = 'blue' , wrap = True ) ax . set_xlabel ( '' ) ax . get_xaxis (). set_ticks ( [] ) ax . get_yaxis (). set_ticks ( [] ) axes_color = self . axes_color for spin in ax . spines : ax . spines [ spin ] . set_visible ( True ) ax . spines [ spin ] . set_color ( axes_color )","title":"plot_text"},{"location":"reference/easyagents/callbacks/plot/#plot_values_2","text":"def plot_values ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], color : Union [ str , NoneType ] = 'blue' , marker : str = None , pause : bool = True ) Draws the graph given by xvalues, yvalues. Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) color: the graphs color (must be the name of a matplotlib color) or None marker: if None a marker is used if the graph has just a few points. pause: pause to redraw the plot. View Source def plot_values ( self , agent_context : core . AgentContext , xvalues : List [ int ] , yvalues : List [ Union[float, Tuple[float, float, float ] ]] , color : Optional [ str ] = 'blue' , marker : str = None , pause : bool = True ) : \"\"\"Draws the graph given by xvalues, yvalues. Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) color: the graphs color (must be the name of a matplotlib color) or None marker: if None a marker is used if the graph has just a few points. pause: pause to redraw the plot. \"\"\" assert xvalues is not None assert yvalues is not None assert len ( xvalues ) == len ( yvalues ), \"xvalues do not match yvalues\" pyc : core . PyPlotContext = agent_context . pyplot # extract min / max and y values if yvalues is of the form [ (min,y,max),...) yminvalues = None ymaxvalues = None if len(yvalues) > 0 and isinstance(yvalues[0 ] , tuple ) : ymaxvalues = [ t[2 ] for t in yvalues ] yminvalues = [ t[0 ] for t in yvalues ] yvalues = [ t[1 ] for t in yvalues ] # plot values marker = marker if marker is None and len ( xvalues ) < 10 : marker = 'o' fill_alpha = 0.1 if pyc . is_jupyter_active : if yminvalues is not None : self . axes . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) self . axes . plot ( xvalues , yvalues , color = color , marker = marker ) else : if yminvalues is not None : plt . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) plt . plot ( xvalues , yvalues , color = color , marker = marker ) if pause : plt . pause ( 0.01 )","title":"plot_values"},{"location":"reference/easyagents/callbacks/plot/#state","text":"class State ( mode = 'rgb_array' ) Renders the gym state as a plot to the pyplot figure using gym.render('rgb_array'). During training only the last state of the last game evaluation is plotted. During play all states are plotted. View Source class State ( _PlotCallback ) : \"\"\"Renders the gym state as a plot to the pyplot figure using gym.render('rgb_array'). During training only the last state of the last game evaluation is plotted. During play all states are plotted. \"\"\" def __init__ ( self , mode = 'rgb_array' ) : \"\"\" Args: mode: the render mode passed to gym.render(), yielding an rgb_array \"\"\" super (). __init__ ( plot_type = core . PlotType . PLAY_STEP | core . PlotType . TRAIN_EVAL ) self . _render_mode = mode def _plot_rgb_array ( self , agent_context : core . AgentContext , rgb_array : np . ndarray ) : \"\"\"Renders rgb_array to the current subplot.\"\"\" assert rgb_array is not None ax = self . axes xlabel = '' if agent_context . is_eval : xlabel = \"'done state' of last evaluation episode\" ax . imshow ( rgb_array ) ax . set_xlabel ( xlabel ) ax . get_xaxis (). set_ticks ( [] ) ax . get_yaxis (). set_ticks ( [] ) axes_color = self . axes_color for spin in ax . spines : ax . spines [ spin ] . set_visible ( True ) ax . spines [ spin ] . set_color ( axes_color ) # noinspection PyArgumentList , DuplicatedCode def _render_to_rgb_array ( self , gym_env : gym . Env , mode : str ) -> np . ndarray : \"\"\" calls gym_env.render(mode) and validates the return value to be a numpy rgb array throws an exception if not an rgb array Returns: numpy rgb array \"\"\" result = gym_env . render ( mode = mode ) assert result is not None , f 'gym_env.render(mode={mode}) yielded None' assert isinstance ( result , np . ndarray ), f 'gym_env.render(mode={mode}) did not yield a numpy.ndarray.' assert result . min () >= 0 , f 'gym_env.render(mode={mode}) contains negative values => not an rgb array' assert result . max () <= 255 , f 'gym_env.render(mode={mode}) contains values > 255 => not an rgb array' assert len ( result . shape ) == 3 , f 'gym_env.render(mode={mode}) shape is not of the form (x,y,n)' assert result . shape [ 2 ] == 3 or result . shape [ 2 ] == 4 , \\ f 'gym_env.render(mode={mode}) shape is not of the form (x,y,3|4)' return result def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ) : self . clear_plot ( agent_context ) try : rgb_array : np . ndarray = self . _render_to_rgb_array ( agent_context . play . gym_env , self . _render_mode ) self . _plot_rgb_array ( agent_context , rgb_array ) except : self . plot_text ( f 'gym.Env.render(mode=\"{self._render_mode}\") failed:\\n' )","title":"State"},{"location":"reference/easyagents/callbacks/plot/#ancestors-in-mro_4","text":"easyagents.callbacks.plot._PlotCallback easyagents.core.AgentCallback abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/callbacks/plot/#methods_4","text":"","title":"Methods"},{"location":"reference/easyagents/callbacks/plot/#clear_plot_3","text":"def clear_plot ( self , agent_context : easyagents . core . AgentContext ) Clears the axes for this plot. Should be called by self.plot before replotting an axes. View Source def clear_plot ( self , agent_context : core . AgentContext ): \"\"\"Clears the axes for this plot. Should be called by self.plot before replotting an axes.\"\"\" assert self . axes is not None pyc = agent_context . pyplot if pyc . is_jupyter_active : self . axes . cla () else : plt . cla ()","title":"clear_plot"},{"location":"reference/easyagents/callbacks/plot/#on_api_log_4","text":"def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass","title":"on_api_log"},{"location":"reference/easyagents/callbacks/plot/#on_gym_init_begin_4","text":"def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\"","title":"on_gym_init_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_init_end_4","text":"def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass","title":"on_gym_init_end"},{"location":"reference/easyagents/callbacks/plot/#on_gym_reset_begin_4","text":"def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\"","title":"on_gym_reset_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_reset_end_4","text":"def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass","title":"on_gym_reset_end"},{"location":"reference/easyagents/callbacks/plot/#on_gym_step_begin_4","text":"def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass","title":"on_gym_step_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_step_end_4","text":"def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass","title":"on_gym_step_end"},{"location":"reference/easyagents/callbacks/plot/#on_log_4","text":"def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass","title":"on_log"},{"location":"reference/easyagents/callbacks/plot/#on_play_begin_4","text":"def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : core . AgentContext ): if ( self . _plot_type & ( core . PlotType . PLAY_EPISODE | core . PlotType . PLAY_STEP )) != core . PlotType . NONE : self . _create_subplot ( agent_context )","title":"on_play_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_end_4","text":"def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_EVAL )","title":"on_play_end"},{"location":"reference/easyagents/callbacks/plot/#on_play_episode_begin_4","text":"def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\"","title":"on_play_episode_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_episode_end_4","text":"def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_EPISODE )","title":"on_play_episode_end"},{"location":"reference/easyagents/callbacks/plot/#on_play_step_begin_4","text":"def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\"","title":"on_play_step_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_step_end_4","text":"def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_STEP )","title":"on_play_step_end"},{"location":"reference/easyagents/callbacks/plot/#on_train_begin_4","text":"def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : core . AgentContext ): if ( self . _plot_type & ( core . PlotType . TRAIN_EVAL | core . PlotType . TRAIN_ITERATION )) != core . PlotType . NONE : self . _create_subplot ( agent_context )","title":"on_train_begin"},{"location":"reference/easyagents/callbacks/plot/#on_train_end_4","text":"def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\"","title":"on_train_end"},{"location":"reference/easyagents/callbacks/plot/#on_train_iteration_begin_4","text":"def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\"","title":"on_train_iteration_begin"},{"location":"reference/easyagents/callbacks/plot/#on_train_iteration_end_4","text":"def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_ITERATION )","title":"on_train_iteration_end"},{"location":"reference/easyagents/callbacks/plot/#plot_3","text":"def plot ( self , agent_context : easyagents . core . AgentContext , plot_type : easyagents . core . PlotType ) Plots a graph on the self.axes object. Args: agent_context: the context containing the data to plot plot_type: the PlotType requesting this plot call. View Source def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ): self . clear_plot ( agent_context ) try : rgb_array : np . ndarray = self . _render_to_rgb_array ( agent_context . play . gym_env , self . _render_mode ) self . _plot_rgb_array ( agent_context , rgb_array ) except : self . plot_text ( f 'gym.Env.render(mode=\"{self._render_mode}\") failed:\\n' )","title":"plot"},{"location":"reference/easyagents/callbacks/plot/#plot_axes_3","text":"def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None ) Draws the x- and y-axes. Attributes: xlim: None or (min,max) for the x-axes xlabel: label of the x-axes ylim: None or (min,max) for the y-axes (or None) ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log', ...) View Source def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = 'linear' , ylim : Optional [ Tuple [ float , float ]] = None ): \"\"\"Draws the x- and y-axes. Attributes: xlim: None or (min,max) for the x-axes xlabel: label of the x-axes ylim: None or (min,max) for the y-axes (or None) ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log', ...) \"\"\" assert xlim is None or xlim [ 0 ] <= xlim [ 1 ] assert ylim is None or ylim [ 0 ] <= ylim [ 1 ] assert xlabel is not None assert ylabel is not None # setup subplot ( axes , labels , colors ) axes_color = self . axes_color self . axes . set_xlabel ( xlabel ) self . axes . set_ylabel ( ylabel ) self . axes . spines [ 'top' ]. set_visible ( False ) self . axes . spines [ 'right' ]. set_visible ( False ) self . axes . spines [ 'bottom' ]. set_color ( axes_color ) self . axes . spines [ 'left' ]. set_color ( axes_color ) self . axes . grid ( color = axes_color , linestyle = '-' , linewidth = 0 . 25 , alpha = 0 . 5 ) if xlim is not None : self . axes . set_xlim ( xlim ) if ylim is not None : self . axes . set_ylim ( ylim ) self . axes . set_yscale ( yscale )","title":"plot_axes"},{"location":"reference/easyagents/callbacks/plot/#plot_subplot_3","text":"def plot_subplot ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], ylabel : str , xlabel : str = None , xlim : Union [ Tuple [ float , float ], NoneType ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None , color : str = 'blue' ) Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) ylim: (min,max) for the x-axes xlabel: label of the x-axes ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log',...) ylim: (min,max) for the y-axes (or None) color: the graphs color (must be the name of a matplotlib color) View Source def plot_subplot ( self , agent_context : core . AgentContext , xvalues : List [ int ] , yvalues : List [ Union[float, Tuple[float, float, float ] ]] , ylabel : str , xlabel : str = None , xlim : Optional [ Tuple[float, float ] ] = None , yscale : str = 'linear' , ylim : Optional [ Tuple[float, float ] ] = None , color : str = 'blue' ) : \"\"\"Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) ylim: (min,max) for the x-axes xlabel: label of the x-axes ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log',...) ylim: (min,max) for the y-axes (or None) color: the graphs color (must be the name of a matplotlib color) \"\"\" if not xlim : xmin = 0 xmax = 1 if agent_context . is_play : xmin = 1 xmax = agent_context . play . episodes_done if agent_context . is_train or agent_context . is_eval : xmin = 0 xmax = agent_context . train . episodes_done_in_training xlim = ( xmin , xmax ) if xlabel is None : xlabel = 'episodes' if agent_context . is_play : xlabel = 'episodes played' if agent_context . is_eval or agent_context . is_train : xlabel = 'episodes trained' self . clear_plot ( agent_context ) self . plot_axes ( xlim = xlim , xlabel = xlabel , ylabel = ylabel , yscale = yscale , ylim = ylim ) self . plot_values ( agent_context = agent_context , xvalues = xvalues , yvalues = yvalues , color = color )","title":"plot_subplot"},{"location":"reference/easyagents/callbacks/plot/#plot_text_3","text":"def plot_text ( self , text : str ) View Source def plot_text ( self , text : str ) : if text : ax = self . axes ax . text ( 0.5 , 0.5 , text , horizontalalignment = 'center' , verticalalignment = 'center' , color = 'blue' , wrap = True ) ax . set_xlabel ( '' ) ax . get_xaxis (). set_ticks ( [] ) ax . get_yaxis (). set_ticks ( [] ) axes_color = self . axes_color for spin in ax . spines : ax . spines [ spin ] . set_visible ( True ) ax . spines [ spin ] . set_color ( axes_color )","title":"plot_text"},{"location":"reference/easyagents/callbacks/plot/#plot_values_3","text":"def plot_values ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], color : Union [ str , NoneType ] = 'blue' , marker : str = None , pause : bool = True ) Draws the graph given by xvalues, yvalues. Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) color: the graphs color (must be the name of a matplotlib color) or None marker: if None a marker is used if the graph has just a few points. pause: pause to redraw the plot. View Source def plot_values ( self , agent_context : core . AgentContext , xvalues : List [ int ] , yvalues : List [ Union[float, Tuple[float, float, float ] ]] , color : Optional [ str ] = 'blue' , marker : str = None , pause : bool = True ) : \"\"\"Draws the graph given by xvalues, yvalues. Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) color: the graphs color (must be the name of a matplotlib color) or None marker: if None a marker is used if the graph has just a few points. pause: pause to redraw the plot. \"\"\" assert xvalues is not None assert yvalues is not None assert len ( xvalues ) == len ( yvalues ), \"xvalues do not match yvalues\" pyc : core . PyPlotContext = agent_context . pyplot # extract min / max and y values if yvalues is of the form [ (min,y,max),...) yminvalues = None ymaxvalues = None if len(yvalues) > 0 and isinstance(yvalues[0 ] , tuple ) : ymaxvalues = [ t[2 ] for t in yvalues ] yminvalues = [ t[0 ] for t in yvalues ] yvalues = [ t[1 ] for t in yvalues ] # plot values marker = marker if marker is None and len ( xvalues ) < 10 : marker = 'o' fill_alpha = 0.1 if pyc . is_jupyter_active : if yminvalues is not None : self . axes . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) self . axes . plot ( xvalues , yvalues , color = color , marker = marker ) else : if yminvalues is not None : plt . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) plt . plot ( xvalues , yvalues , color = color , marker = marker ) if pause : plt . pause ( 0.01 )","title":"plot_values"},{"location":"reference/easyagents/callbacks/plot/#steprewards","text":"class StepRewards ( num_steps_between_plot = 100 ) Base class of plyplot callbacks generating a plot after a trained iteration or an episode played. Attributes: axes: the subplot to plot onto View Source class StepRewards ( _PlotCallback ) : def __init__ ( self , num_steps_between_plot = 100 ) : \"\"\"Plots the sum of rewards up to the current step during play or at the end of an evaluation period. Args: num_steps_between_plot: num of steps to play before plot is updated. \"\"\" super (). __init__ ( core . PlotType . PLAY_STEP | core . PlotType . PLAY_EPISODE | core . PlotType . TRAIN_EVAL ) assert num_steps_between_plot > 0 self . _xy_values : Dict [ int, Tuple[List[int ] , List [ float ] ]] = dict () self . _xmax : int = 0 self . _num_steps_between_plot = num_steps_between_plot def _replot ( self , agent_context : core . AgentContext ) : if self . _xmax >= 1 : self . clear_plot ( agent_context ) xlabel = 'steps played' if agent_context . is_eval : xlabel = 'steps taken during last evaluation period' self . plot_axes ( xlim = ( 1 , self . _xmax ), ylabel = 'sum of rewards' , xlabel = xlabel ) xy_values = list ( self . _xy_values . values ()) xlast , _ = xy_values [ -1 ] for xvalues , yvalues in xy_values : pause = ( xvalues == xlast ) self . plot_values ( agent_context = agent_context , xvalues = xvalues , yvalues = yvalues , color = None , marker = '' , pause = pause ) def _reset ( self ) : self . _xy_values = dict () self . _xmax = 0 def on_play_begin ( self , agent_context : core . AgentContext ) : super (). on_play_begin ( agent_context ) self . _reset () def on_play_end ( self , agent_context : core . AgentContext ) : super (). on_play_end ( agent_context ) self . _reset () def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ) : pc = agent_context . play if plot_type & core . PlotType . PLAY_STEP != core . PlotType . NONE : episode = pc . episodes_done + 1 if episode not in self . _xy_values : self . _xy_values [ episode ] = ( [] , [] ) xvalues , yvalues = self . _xy_values [ episode ] xvalues . append ( pc . steps_done_in_episode ) yvalues . append ( pc . sum_of_rewards [ episode ] ) if pc . steps_done_in_episode > self . _xmax : self . _xmax = pc . steps_done_in_episode if ( episode == 1 and pc . steps_done == 1 ) or ( pc . steps_done % self . _num_steps_between_plot ) == 0 : self . _replot ( agent_context ) if plot_type & core . PlotType . PLAY_EPISODE != core . PlotType . NONE : self . _replot ( agent_context ) if plot_type & core . PlotType . TRAIN_EVAL != core . PlotType . NONE : self . _xmax = max ( [ len(step_rewards) for step_rewards in pc.rewards.values() ] ) for episode in pc . rewards . keys () : step_rewards = pc . rewards [ episode ] xvalues = list ( range ( 1 , len ( step_rewards ) + 1 )) yvalues = [] for reward in step_rewards : old_sum = yvalues [ -1 ] if len ( yvalues ) > 0 else 0 yvalues . append ( old_sum + reward ) self . _xy_values [ episode ] = ( xvalues , yvalues ) self . _replot ( agent_context )","title":"StepRewards"},{"location":"reference/easyagents/callbacks/plot/#ancestors-in-mro_5","text":"easyagents.callbacks.plot._PlotCallback easyagents.core.AgentCallback abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/callbacks/plot/#methods_5","text":"","title":"Methods"},{"location":"reference/easyagents/callbacks/plot/#clear_plot_4","text":"def clear_plot ( self , agent_context : easyagents . core . AgentContext ) Clears the axes for this plot. Should be called by self.plot before replotting an axes. View Source def clear_plot ( self , agent_context : core . AgentContext ): \"\"\"Clears the axes for this plot. Should be called by self.plot before replotting an axes.\"\"\" assert self . axes is not None pyc = agent_context . pyplot if pyc . is_jupyter_active : self . axes . cla () else : plt . cla ()","title":"clear_plot"},{"location":"reference/easyagents/callbacks/plot/#on_api_log_5","text":"def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass","title":"on_api_log"},{"location":"reference/easyagents/callbacks/plot/#on_gym_init_begin_5","text":"def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\"","title":"on_gym_init_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_init_end_5","text":"def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass","title":"on_gym_init_end"},{"location":"reference/easyagents/callbacks/plot/#on_gym_reset_begin_5","text":"def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\"","title":"on_gym_reset_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_reset_end_5","text":"def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass","title":"on_gym_reset_end"},{"location":"reference/easyagents/callbacks/plot/#on_gym_step_begin_5","text":"def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass","title":"on_gym_step_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_step_end_5","text":"def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass","title":"on_gym_step_end"},{"location":"reference/easyagents/callbacks/plot/#on_log_5","text":"def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass","title":"on_log"},{"location":"reference/easyagents/callbacks/plot/#on_play_begin_5","text":"def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : core . AgentContext ): super (). on_play_begin ( agent_context ) self . _reset ()","title":"on_play_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_end_5","text":"def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : core . AgentContext ): super (). on_play_end ( agent_context ) self . _reset ()","title":"on_play_end"},{"location":"reference/easyagents/callbacks/plot/#on_play_episode_begin_5","text":"def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\"","title":"on_play_episode_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_episode_end_5","text":"def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_EPISODE )","title":"on_play_episode_end"},{"location":"reference/easyagents/callbacks/plot/#on_play_step_begin_5","text":"def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\"","title":"on_play_step_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_step_end_5","text":"def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_STEP )","title":"on_play_step_end"},{"location":"reference/easyagents/callbacks/plot/#on_train_begin_5","text":"def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : core . AgentContext ): if ( self . _plot_type & ( core . PlotType . TRAIN_EVAL | core . PlotType . TRAIN_ITERATION )) != core . PlotType . NONE : self . _create_subplot ( agent_context )","title":"on_train_begin"},{"location":"reference/easyagents/callbacks/plot/#on_train_end_5","text":"def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\"","title":"on_train_end"},{"location":"reference/easyagents/callbacks/plot/#on_train_iteration_begin_5","text":"def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\"","title":"on_train_iteration_begin"},{"location":"reference/easyagents/callbacks/plot/#on_train_iteration_end_5","text":"def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_ITERATION )","title":"on_train_iteration_end"},{"location":"reference/easyagents/callbacks/plot/#plot_4","text":"def plot ( self , agent_context : easyagents . core . AgentContext , plot_type : easyagents . core . PlotType ) Plots a graph on the self.axes object. Args: agent_context: the context containing the data to plot plot_type: the PlotType requesting this plot call. View Source def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ) : pc = agent_context . play if plot_type & core . PlotType . PLAY_STEP != core . PlotType . NONE : episode = pc . episodes_done + 1 if episode not in self . _xy_values : self . _xy_values [ episode ] = ( [] , [] ) xvalues , yvalues = self . _xy_values [ episode ] xvalues . append ( pc . steps_done_in_episode ) yvalues . append ( pc . sum_of_rewards [ episode ] ) if pc . steps_done_in_episode > self . _xmax : self . _xmax = pc . steps_done_in_episode if ( episode == 1 and pc . steps_done == 1 ) or ( pc . steps_done % self . _num_steps_between_plot ) == 0 : self . _replot ( agent_context ) if plot_type & core . PlotType . PLAY_EPISODE != core . PlotType . NONE : self . _replot ( agent_context ) if plot_type & core . PlotType . TRAIN_EVAL != core . PlotType . NONE : self . _xmax = max ( [ len(step_rewards) for step_rewards in pc.rewards.values() ] ) for episode in pc . rewards . keys () : step_rewards = pc . rewards [ episode ] xvalues = list ( range ( 1 , len ( step_rewards ) + 1 )) yvalues = [] for reward in step_rewards : old_sum = yvalues [ -1 ] if len ( yvalues ) > 0 else 0 yvalues . append ( old_sum + reward ) self . _xy_values [ episode ] = ( xvalues , yvalues ) self . _replot ( agent_context )","title":"plot"},{"location":"reference/easyagents/callbacks/plot/#plot_axes_4","text":"def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None ) Draws the x- and y-axes. Attributes: xlim: None or (min,max) for the x-axes xlabel: label of the x-axes ylim: None or (min,max) for the y-axes (or None) ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log', ...) View Source def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = 'linear' , ylim : Optional [ Tuple [ float , float ]] = None ): \"\"\"Draws the x- and y-axes. Attributes: xlim: None or (min,max) for the x-axes xlabel: label of the x-axes ylim: None or (min,max) for the y-axes (or None) ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log', ...) \"\"\" assert xlim is None or xlim [ 0 ] <= xlim [ 1 ] assert ylim is None or ylim [ 0 ] <= ylim [ 1 ] assert xlabel is not None assert ylabel is not None # setup subplot ( axes , labels , colors ) axes_color = self . axes_color self . axes . set_xlabel ( xlabel ) self . axes . set_ylabel ( ylabel ) self . axes . spines [ 'top' ]. set_visible ( False ) self . axes . spines [ 'right' ]. set_visible ( False ) self . axes . spines [ 'bottom' ]. set_color ( axes_color ) self . axes . spines [ 'left' ]. set_color ( axes_color ) self . axes . grid ( color = axes_color , linestyle = '-' , linewidth = 0 . 25 , alpha = 0 . 5 ) if xlim is not None : self . axes . set_xlim ( xlim ) if ylim is not None : self . axes . set_ylim ( ylim ) self . axes . set_yscale ( yscale )","title":"plot_axes"},{"location":"reference/easyagents/callbacks/plot/#plot_subplot_4","text":"def plot_subplot ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], ylabel : str , xlabel : str = None , xlim : Union [ Tuple [ float , float ], NoneType ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None , color : str = 'blue' ) Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) ylim: (min,max) for the x-axes xlabel: label of the x-axes ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log',...) ylim: (min,max) for the y-axes (or None) color: the graphs color (must be the name of a matplotlib color) View Source def plot_subplot ( self , agent_context : core . AgentContext , xvalues : List [ int ] , yvalues : List [ Union[float, Tuple[float, float, float ] ]] , ylabel : str , xlabel : str = None , xlim : Optional [ Tuple[float, float ] ] = None , yscale : str = 'linear' , ylim : Optional [ Tuple[float, float ] ] = None , color : str = 'blue' ) : \"\"\"Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) ylim: (min,max) for the x-axes xlabel: label of the x-axes ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log',...) ylim: (min,max) for the y-axes (or None) color: the graphs color (must be the name of a matplotlib color) \"\"\" if not xlim : xmin = 0 xmax = 1 if agent_context . is_play : xmin = 1 xmax = agent_context . play . episodes_done if agent_context . is_train or agent_context . is_eval : xmin = 0 xmax = agent_context . train . episodes_done_in_training xlim = ( xmin , xmax ) if xlabel is None : xlabel = 'episodes' if agent_context . is_play : xlabel = 'episodes played' if agent_context . is_eval or agent_context . is_train : xlabel = 'episodes trained' self . clear_plot ( agent_context ) self . plot_axes ( xlim = xlim , xlabel = xlabel , ylabel = ylabel , yscale = yscale , ylim = ylim ) self . plot_values ( agent_context = agent_context , xvalues = xvalues , yvalues = yvalues , color = color )","title":"plot_subplot"},{"location":"reference/easyagents/callbacks/plot/#plot_text_4","text":"def plot_text ( self , text : str ) View Source def plot_text ( self , text : str ) : if text : ax = self . axes ax . text ( 0.5 , 0.5 , text , horizontalalignment = 'center' , verticalalignment = 'center' , color = 'blue' , wrap = True ) ax . set_xlabel ( '' ) ax . get_xaxis (). set_ticks ( [] ) ax . get_yaxis (). set_ticks ( [] ) axes_color = self . axes_color for spin in ax . spines : ax . spines [ spin ] . set_visible ( True ) ax . spines [ spin ] . set_color ( axes_color )","title":"plot_text"},{"location":"reference/easyagents/callbacks/plot/#plot_values_4","text":"def plot_values ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], color : Union [ str , NoneType ] = 'blue' , marker : str = None , pause : bool = True ) Draws the graph given by xvalues, yvalues. Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) color: the graphs color (must be the name of a matplotlib color) or None marker: if None a marker is used if the graph has just a few points. pause: pause to redraw the plot. View Source def plot_values ( self , agent_context : core . AgentContext , xvalues : List [ int ] , yvalues : List [ Union[float, Tuple[float, float, float ] ]] , color : Optional [ str ] = 'blue' , marker : str = None , pause : bool = True ) : \"\"\"Draws the graph given by xvalues, yvalues. Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) color: the graphs color (must be the name of a matplotlib color) or None marker: if None a marker is used if the graph has just a few points. pause: pause to redraw the plot. \"\"\" assert xvalues is not None assert yvalues is not None assert len ( xvalues ) == len ( yvalues ), \"xvalues do not match yvalues\" pyc : core . PyPlotContext = agent_context . pyplot # extract min / max and y values if yvalues is of the form [ (min,y,max),...) yminvalues = None ymaxvalues = None if len(yvalues) > 0 and isinstance(yvalues[0 ] , tuple ) : ymaxvalues = [ t[2 ] for t in yvalues ] yminvalues = [ t[0 ] for t in yvalues ] yvalues = [ t[1 ] for t in yvalues ] # plot values marker = marker if marker is None and len ( xvalues ) < 10 : marker = 'o' fill_alpha = 0.1 if pyc . is_jupyter_active : if yminvalues is not None : self . axes . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) self . axes . plot ( xvalues , yvalues , color = color , marker = marker ) else : if yminvalues is not None : plt . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) plt . plot ( xvalues , yvalues , color = color , marker = marker ) if pause : plt . pause ( 0.01 )","title":"plot_values"},{"location":"reference/easyagents/callbacks/plot/#steps","text":"class Steps ( yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None ) Base class of plyplot callbacks generating a plot after a trained iteration or an episode played. Attributes: axes: the subplot to plot onto View Source class Steps ( _PlotCallback ) : def __init__ ( self , yscale : str = 'linear' , ylim : Optional [ Tuple[float, float ] ] = None ) : \"\"\"Plots the step counts observed during policy evaluation. Args: yscale: scale of the y-axes ('linear','log') ylim: (min,max) for the y-axes \"\"\" super (). __init__ ( core . PlotType . TRAIN_ITERATION | core . PlotType . TRAIN_EVAL | core . PlotType . PLAY_EPISODE ) self . ylim = ylim self . yscale = yscale def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ) : xvalues = yvalues = [] ylabel = 'steps' if agent_context . is_train or agent_context . is_eval : tc = agent_context . train xvalues = list ( tc . eval_steps . keys ()) yvalues = list ( tc . eval_steps . values ()) ylabel = '\u00d8 steps' if agent_context . is_play : pc = agent_context . play xvalues = list ( pc . actions . keys ()) yvalues = [ len(pc.actions[episode ] ) for episode in pc . actions . keys () ] self . plot_subplot ( agent_context , color = 'blue' , ylim = self . ylim , yscale = self . yscale , xvalues = xvalues , yvalues = yvalues , ylabel = ylabel )","title":"Steps"},{"location":"reference/easyagents/callbacks/plot/#ancestors-in-mro_6","text":"easyagents.callbacks.plot._PlotCallback easyagents.core.AgentCallback abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/callbacks/plot/#methods_6","text":"","title":"Methods"},{"location":"reference/easyagents/callbacks/plot/#clear_plot_5","text":"def clear_plot ( self , agent_context : easyagents . core . AgentContext ) Clears the axes for this plot. Should be called by self.plot before replotting an axes. View Source def clear_plot ( self , agent_context : core . AgentContext ): \"\"\"Clears the axes for this plot. Should be called by self.plot before replotting an axes.\"\"\" assert self . axes is not None pyc = agent_context . pyplot if pyc . is_jupyter_active : self . axes . cla () else : plt . cla ()","title":"clear_plot"},{"location":"reference/easyagents/callbacks/plot/#on_api_log_6","text":"def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass","title":"on_api_log"},{"location":"reference/easyagents/callbacks/plot/#on_gym_init_begin_6","text":"def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\"","title":"on_gym_init_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_init_end_6","text":"def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass","title":"on_gym_init_end"},{"location":"reference/easyagents/callbacks/plot/#on_gym_reset_begin_6","text":"def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\"","title":"on_gym_reset_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_reset_end_6","text":"def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass","title":"on_gym_reset_end"},{"location":"reference/easyagents/callbacks/plot/#on_gym_step_begin_6","text":"def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass","title":"on_gym_step_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_step_end_6","text":"def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass","title":"on_gym_step_end"},{"location":"reference/easyagents/callbacks/plot/#on_log_6","text":"def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass","title":"on_log"},{"location":"reference/easyagents/callbacks/plot/#on_play_begin_6","text":"def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : core . AgentContext ): if ( self . _plot_type & ( core . PlotType . PLAY_EPISODE | core . PlotType . PLAY_STEP )) != core . PlotType . NONE : self . _create_subplot ( agent_context )","title":"on_play_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_end_6","text":"def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_EVAL )","title":"on_play_end"},{"location":"reference/easyagents/callbacks/plot/#on_play_episode_begin_6","text":"def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\"","title":"on_play_episode_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_episode_end_6","text":"def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_EPISODE )","title":"on_play_episode_end"},{"location":"reference/easyagents/callbacks/plot/#on_play_step_begin_6","text":"def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\"","title":"on_play_step_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_step_end_6","text":"def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): self . _refresh_subplot ( agent_context , core . PlotType . PLAY_STEP )","title":"on_play_step_end"},{"location":"reference/easyagents/callbacks/plot/#on_train_begin_6","text":"def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : core . AgentContext ): if ( self . _plot_type & ( core . PlotType . TRAIN_EVAL | core . PlotType . TRAIN_ITERATION )) != core . PlotType . NONE : self . _create_subplot ( agent_context )","title":"on_train_begin"},{"location":"reference/easyagents/callbacks/plot/#on_train_end_6","text":"def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\"","title":"on_train_end"},{"location":"reference/easyagents/callbacks/plot/#on_train_iteration_begin_6","text":"def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\"","title":"on_train_iteration_begin"},{"location":"reference/easyagents/callbacks/plot/#on_train_iteration_end_6","text":"def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : core . AgentContext ): self . _refresh_subplot ( agent_context , core . PlotType . TRAIN_ITERATION )","title":"on_train_iteration_end"},{"location":"reference/easyagents/callbacks/plot/#plot_5","text":"def plot ( self , agent_context : easyagents . core . AgentContext , plot_type : easyagents . core . PlotType ) Plots a graph on the self.axes object. Args: agent_context: the context containing the data to plot plot_type: the PlotType requesting this plot call. View Source def plot ( self , agent_context : core . AgentContext , plot_type : core . PlotType ) : xvalues = yvalues = [] ylabel = 'steps' if agent_context . is_train or agent_context . is_eval : tc = agent_context . train xvalues = list ( tc . eval_steps . keys ()) yvalues = list ( tc . eval_steps . values ()) ylabel = '\u00d8 steps' if agent_context . is_play : pc = agent_context . play xvalues = list ( pc . actions . keys ()) yvalues = [ len(pc.actions[episode ] ) for episode in pc . actions . keys () ] self . plot_subplot ( agent_context , color = 'blue' , ylim = self . ylim , yscale = self . yscale , xvalues = xvalues , yvalues = yvalues , ylabel = ylabel )","title":"plot"},{"location":"reference/easyagents/callbacks/plot/#plot_axes_5","text":"def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None ) Draws the x- and y-axes. Attributes: xlim: None or (min,max) for the x-axes xlabel: label of the x-axes ylim: None or (min,max) for the y-axes (or None) ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log', ...) View Source def plot_axes ( self , xlabel : str , ylabel : str , xlim : Tuple [ float , float ] = None , yscale : str = 'linear' , ylim : Optional [ Tuple [ float , float ]] = None ): \"\"\"Draws the x- and y-axes. Attributes: xlim: None or (min,max) for the x-axes xlabel: label of the x-axes ylim: None or (min,max) for the y-axes (or None) ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log', ...) \"\"\" assert xlim is None or xlim [ 0 ] <= xlim [ 1 ] assert ylim is None or ylim [ 0 ] <= ylim [ 1 ] assert xlabel is not None assert ylabel is not None # setup subplot ( axes , labels , colors ) axes_color = self . axes_color self . axes . set_xlabel ( xlabel ) self . axes . set_ylabel ( ylabel ) self . axes . spines [ 'top' ]. set_visible ( False ) self . axes . spines [ 'right' ]. set_visible ( False ) self . axes . spines [ 'bottom' ]. set_color ( axes_color ) self . axes . spines [ 'left' ]. set_color ( axes_color ) self . axes . grid ( color = axes_color , linestyle = '-' , linewidth = 0 . 25 , alpha = 0 . 5 ) if xlim is not None : self . axes . set_xlim ( xlim ) if ylim is not None : self . axes . set_ylim ( ylim ) self . axes . set_yscale ( yscale )","title":"plot_axes"},{"location":"reference/easyagents/callbacks/plot/#plot_subplot_5","text":"def plot_subplot ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], ylabel : str , xlabel : str = None , xlim : Union [ Tuple [ float , float ], NoneType ] = None , yscale : str = 'linear' , ylim : Union [ Tuple [ float , float ], NoneType ] = None , color : str = 'blue' ) Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) ylim: (min,max) for the x-axes xlabel: label of the x-axes ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log',...) ylim: (min,max) for the y-axes (or None) color: the graphs color (must be the name of a matplotlib color) View Source def plot_subplot ( self , agent_context : core . AgentContext , xvalues : List [ int ] , yvalues : List [ Union[float, Tuple[float, float, float ] ]] , ylabel : str , xlabel : str = None , xlim : Optional [ Tuple[float, float ] ] = None , yscale : str = 'linear' , ylim : Optional [ Tuple[float, float ] ] = None , color : str = 'blue' ) : \"\"\"Draws the graph given by xvalues, yvalues (including x- & y-axes) . Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) ylim: (min,max) for the x-axes xlabel: label of the x-axes ylabel: label of the y-axes yscale: scale of the y-axes ('linear', 'log',...) ylim: (min,max) for the y-axes (or None) color: the graphs color (must be the name of a matplotlib color) \"\"\" if not xlim : xmin = 0 xmax = 1 if agent_context . is_play : xmin = 1 xmax = agent_context . play . episodes_done if agent_context . is_train or agent_context . is_eval : xmin = 0 xmax = agent_context . train . episodes_done_in_training xlim = ( xmin , xmax ) if xlabel is None : xlabel = 'episodes' if agent_context . is_play : xlabel = 'episodes played' if agent_context . is_eval or agent_context . is_train : xlabel = 'episodes trained' self . clear_plot ( agent_context ) self . plot_axes ( xlim = xlim , xlabel = xlabel , ylabel = ylabel , yscale = yscale , ylim = ylim ) self . plot_values ( agent_context = agent_context , xvalues = xvalues , yvalues = yvalues , color = color )","title":"plot_subplot"},{"location":"reference/easyagents/callbacks/plot/#plot_text_5","text":"def plot_text ( self , text : str ) View Source def plot_text ( self , text : str ) : if text : ax = self . axes ax . text ( 0.5 , 0.5 , text , horizontalalignment = 'center' , verticalalignment = 'center' , color = 'blue' , wrap = True ) ax . set_xlabel ( '' ) ax . get_xaxis (). set_ticks ( [] ) ax . get_yaxis (). set_ticks ( [] ) axes_color = self . axes_color for spin in ax . spines : ax . spines [ spin ] . set_visible ( True ) ax . spines [ spin ] . set_color ( axes_color )","title":"plot_text"},{"location":"reference/easyagents/callbacks/plot/#plot_values_5","text":"def plot_values ( self , agent_context : easyagents . core . AgentContext , xvalues : List [ int ], yvalues : List [ Union [ float , Tuple [ float , float , float ]]], color : Union [ str , NoneType ] = 'blue' , marker : str = None , pause : bool = True ) Draws the graph given by xvalues, yvalues. Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) color: the graphs color (must be the name of a matplotlib color) or None marker: if None a marker is used if the graph has just a few points. pause: pause to redraw the plot. View Source def plot_values ( self , agent_context : core . AgentContext , xvalues : List [ int ] , yvalues : List [ Union[float, Tuple[float, float, float ] ]] , color : Optional [ str ] = 'blue' , marker : str = None , pause : bool = True ) : \"\"\"Draws the graph given by xvalues, yvalues. Attributes: agent_context: context containing the figure to plot to xvalues: the graphs x-values (must have same length as y-values) yvalues: the graphs y-values or (min,y,max)-tuples (must have same length as x-values) color: the graphs color (must be the name of a matplotlib color) or None marker: if None a marker is used if the graph has just a few points. pause: pause to redraw the plot. \"\"\" assert xvalues is not None assert yvalues is not None assert len ( xvalues ) == len ( yvalues ), \"xvalues do not match yvalues\" pyc : core . PyPlotContext = agent_context . pyplot # extract min / max and y values if yvalues is of the form [ (min,y,max),...) yminvalues = None ymaxvalues = None if len(yvalues) > 0 and isinstance(yvalues[0 ] , tuple ) : ymaxvalues = [ t[2 ] for t in yvalues ] yminvalues = [ t[0 ] for t in yvalues ] yvalues = [ t[1 ] for t in yvalues ] # plot values marker = marker if marker is None and len ( xvalues ) < 10 : marker = 'o' fill_alpha = 0.1 if pyc . is_jupyter_active : if yminvalues is not None : self . axes . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) self . axes . plot ( xvalues , yvalues , color = color , marker = marker ) else : if yminvalues is not None : plt . fill_between ( xvalues , yminvalues , ymaxvalues , color = color , alpha = fill_alpha ) plt . plot ( xvalues , yvalues , color = color , marker = marker ) if pause : plt . pause ( 0.01 )","title":"plot_values"},{"location":"reference/easyagents/callbacks/plot/#tomovie","text":"class ToMovie ( fps : Union [ int , NoneType ] = None , filepath : str = None ) Plots the pyplot figure to an mp4 file Attributes: fps: frame per seconds filepath: the filepath of the mp4 file. View Source class ToMovie ( core . _ PostProcessCallback ) : \"\"\"Plots the pyplot figure to an mp4 file Attributes: fps: frame per seconds filepath: the filepath of the mp4 file. \"\"\" def __ init__ ( self , fps : Optional [ int ] = None , filepath : str = None ) : \"\"\"Writes the ploted graphs and images to the mp4 / gif file given by filepath. if filepath ends in '.gif' an animated gif is created. Args: fps: frames per second filepath: the filepath of the mp4 or gif file file. If None the file is written to a temp file. \"\"\" super (). __ init__ () self . fps = fps self . _ is_filepath_set = filepath is not None self . filepath = filepath if not self . _ is_filepath_set: self . filepath = easyagents . backends . core . _ get_temp_path () if ( not self . _ is_animated_gif ()) and ( not self . filepath . lower (). endswith ( '.mp4' )) : self . filepath = self . filepath + '.mp4' self . _ video = imageio . get_writer ( self . filepath , fps = fps ) if fps else imageio . get_writer ( self . filepath ) def _ close ( self , agent_context: core . AgentContext ) : \"\"\"closes the mp4 file and displays it in jupyter cell (if in a jupyter notebook)\"\"\" self . _ video . close () self . _ video = None if agent_context . pyplot . is_jupyter_active: with open ( self . filepath , 'rb' ) as f : video = f . read () b64 = base64 . b64encode ( video ) if not self . _ is_filepath_set: os . remove ( self . filepath ) width = 640 height = 480 if self . _ is_animated_gif () : result = ''' <img src=\"data:image/gif;base64,{2}\" alt=\"easyagents.plot\" width={0}/> ''' . format ( width , height , b64 . decode ()) else : result = ''' <video width=\"{0}\" height=\"{1}\" controls> <source src=\"data:video/mp4;base64,{2}\" type=\"video/mp4\"> Your browser does not support the video tag. </video>''' . format ( width , height , b64 . decode ()) result = HTML ( result ) # noinspection PyTypeChecker clear_output ( wait = True ) # noinspection PyTypeChecker display ( result ) def _ get_rgb_array ( self , agent_context: core . AgentContext ) -> np . ndarray : \"\"\"Yields an rgb array representing the current content of the subplots.\"\"\" pyc = agent_context . pyplot pyc . figure . canvas . draw () result = np . frombuffer ( pyc . figure . canvas . tostring_rgb (), dtype='uint8' ) result = result . reshape ( pyc . figure . canvas . get_width_height ()[ ::- 1 ] + ( 3 ,)) return result def _ is_animated_gif ( self ) : return self . filepath . lower (). endswith ( '.gif' ) def _ write_figure_to_video ( self , agent_context: core . AgentContext ) : \"\"\"Appends the current pyplot figure to the video. if an exception occures no frame is added. \"\"\" try : rgb_array = self . _ get_rgb_array ( agent_context ) self . _ video . append_data ( rgb_array ) except : pass def on_play_episode_end ( self , agent_context: core . AgentContext ) : if agent_context . _ is_plot_ready ( core . PlotType . PLAY_EPISODE | core . PlotType . TRAIN_EVAL ) : self . _ write_figure_to_video ( agent_context ) def on_play_step_end ( self , agent_context: core . AgentContext , action , step_result: Tuple ) : if agent_context . _ is_plot_ready ( core . PlotType . PLAY_STEP ) : self . _ write_figure_to_video ( agent_context ) def on_train_iteration_end ( self , agent_context: core . AgentContext ) : if agent_context . _ is_plot_ready ( core . PlotType . TRAIN_ITERATION ) : self . _ write_figure_to_video ( agent_context ) def on_play_end ( self , agent_context: core . AgentContext ) : if agent_context . _ is_plot_ready ( core . PlotType . PLAY_EPISODE ) : self . _ close ( agent_context ) def on_train_end ( self , agent_context: core . AgentContext ) : self . _ close ( agent_context )","title":"ToMovie"},{"location":"reference/easyagents/callbacks/plot/#ancestors-in-mro_7","text":"easyagents.core._PostProcessCallback easyagents.core.AgentCallback abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/callbacks/plot/#methods_7","text":"","title":"Methods"},{"location":"reference/easyagents/callbacks/plot/#on_api_log_7","text":"def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass","title":"on_api_log"},{"location":"reference/easyagents/callbacks/plot/#on_gym_init_begin_7","text":"def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\"","title":"on_gym_init_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_init_end_7","text":"def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass","title":"on_gym_init_end"},{"location":"reference/easyagents/callbacks/plot/#on_gym_reset_begin_7","text":"def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\"","title":"on_gym_reset_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_reset_end_7","text":"def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass","title":"on_gym_reset_end"},{"location":"reference/easyagents/callbacks/plot/#on_gym_step_begin_7","text":"def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass","title":"on_gym_step_begin"},{"location":"reference/easyagents/callbacks/plot/#on_gym_step_end_7","text":"def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass","title":"on_gym_step_end"},{"location":"reference/easyagents/callbacks/plot/#on_log_7","text":"def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass","title":"on_log"},{"location":"reference/easyagents/callbacks/plot/#on_play_begin_7","text":"def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.play() call (during play or eval, but not during train). \"\"\"","title":"on_play_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_end_7","text":"def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : core . AgentContext ): if agent_context . _is_plot_ready ( core . PlotType . PLAY_EPISODE ): self . _close ( agent_context )","title":"on_play_end"},{"location":"reference/easyagents/callbacks/plot/#on_play_episode_begin_7","text":"def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\"","title":"on_play_episode_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_episode_end_7","text":"def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : core . AgentContext ): if agent_context . _is_plot_ready ( core . PlotType . PLAY_EPISODE | core . PlotType . TRAIN_EVAL ): self . _write_figure_to_video ( agent_context )","title":"on_play_episode_end"},{"location":"reference/easyagents/callbacks/plot/#on_play_step_begin_7","text":"def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\"","title":"on_play_step_begin"},{"location":"reference/easyagents/callbacks/plot/#on_play_step_end_7","text":"def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : core . AgentContext , action , step_result : Tuple ): if agent_context . _is_plot_ready ( core . PlotType . PLAY_STEP ): self . _write_figure_to_video ( agent_context )","title":"on_play_step_end"},{"location":"reference/easyagents/callbacks/plot/#on_train_begin_7","text":"def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.train() call. \"\"\"","title":"on_train_begin"},{"location":"reference/easyagents/callbacks/plot/#on_train_end_7","text":"def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : core . AgentContext ): self . _close ( agent_context )","title":"on_train_end"},{"location":"reference/easyagents/callbacks/plot/#on_train_iteration_begin_7","text":"def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\"","title":"on_train_iteration_begin"},{"location":"reference/easyagents/callbacks/plot/#on_train_iteration_end_7","text":"def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : core . AgentContext ): if agent_context . _is_plot_ready ( core . PlotType . TRAIN_ITERATION ): self . _write_figure_to_video ( agent_context )","title":"on_train_iteration_end"},{"location":"reference/easyagents/callbacks/save/","text":"Module easyagents.callbacks.save View Source import os import sys from typing import List , Tuple import easyagents.core as core import easyagents.backends.core as bcore class _SaveCallback ( core . AgentCallback ): \"\"\"Base class for all agent savent callbacks. Attributes: directory: the absolute path of the directory containing the persisted policies. saved_agents: list of tuples (episode, avg_rewards, directory) for each saved agent \"\"\" def __init__ ( self , directory : str = None ): \"\"\"Saves the best policies (along with the agent definition) in directory. If directory is None the policies are written in a temp directory. Args: directory: the directory to save to, if None a temp directory is created. \"\"\" directory = directory if directory else bcore . _get_temp_path () self . directory : str = bcore . _mkdir ( directory ) self . saved_agents : List [ Tuple [ int , float , str ]] = [] def __str__ ( self ): return self . directory def _save ( self , agent_context : core . AgentContext ): \"\"\"Saves the current policy in directory.\"\"\" assert agent_context assert agent_context . train , \"TrainContext not set.\" tc = agent_context . train min_rewards , avg_reward , max_rewards = tc . eval_rewards [ tc . episodes_done_in_training ] current_dir = f 'episode_{tc.episodes_done_in_training}-avg_reward_{avg_reward}' current_dir = os . path . join ( self . directory , current_dir ) agent_context . _agent_saver ( directory = current_dir ) self . saved_agents . append (( tc . episodes_done_in_training , avg_reward , current_dir )) class Best ( _SaveCallback ): \"\"\"After each policy evaluation the policy is saved if average reward is larger than all previous average rewards. The policies can then be loaded using agents.load() Attributes: directory: the absolute path of the directory containing the persisted policies. saved_agents: list of tuples (episode, avg_rewards, directory) for each saved agent \"\"\" def __init__ ( self , directory : str = None ): \"\"\"Saves the best policies (along with the agent definition) in directory. If directory is None the policies are written in a temp directory. Args: directory: the directory to save to, if None a temp directory is created. \"\"\" super () . __init__ ( directory = directory ) self . _best_avg_reward = - sys . float_info . max def on_play_end ( self , agent_context : core . AgentContext ): if agent_context . is_eval : tc = agent_context . train min_rewards , avg_reward , max_rewards = tc . eval_rewards [ tc . episodes_done_in_training ] if avg_reward > self . _best_avg_reward : self . _save ( agent_context = agent_context ) self . _best_avg_reward = avg_reward class Every ( _SaveCallback ): \"\"\"Saves the current policy every n evaluations. Attributes: directory: the absolute path of the directory containing the persisted policies. saved_agents: list of tuples (episode, avg_rewards, directory) for each saved agent \"\"\" def __init__ ( self , num_evals_between_save : int = 1 , directory : str = None ): \"\"\"Saves the current policy every n evaluations. In terms of episodes: the policy is saved every num_eval_between_saves * num_iterations_between_eval * num_episodes_per_iteration Args: num_evals_between_save: the number of evaluations between saves. \"\"\" assert num_evals_between_save > 0 super () . __init__ ( directory = directory ) self . num_evals_between_save : int = num_evals_between_save def on_play_end ( self , agent_context : core . AgentContext ): if agent_context . is_eval : tc = agent_context . train if tc . num_iterations % tc . num_iterations_between_eval == 0 : evals = tc . num_iterations / tc . num_iterations_between_eval if evals % self . num_evals_between_save == 0 : self . _save ( agent_context = agent_context ) Classes Best class Best ( directory : str = None ) After each policy evaluation the policy is saved if average reward is larger than all previous average rewards. The policies can then be loaded using agents.load() Attributes: directory: the absolute path of the directory containing the persisted policies. saved_agents: list of tuples (episode, avg_rewards, directory) for each saved agent View Source class Best ( _SaveCallback ): \"\"\"After each policy evaluation the policy is saved if average reward is larger than all previous average rewards. The policies can then be loaded using agents.load() Attributes: directory: the absolute path of the directory containing the persisted policies. saved_agents: list of tuples (episode, avg_rewards, directory) for each saved agent \"\"\" def __init__ ( self , directory: str = None ): \"\"\"Saves the best policies (along with the agent definition) in directory. If directory is None the policies are written in a temp directory. Args: directory: the directory to save to, if None a temp directory is created. \"\"\" super (). __init__ ( directory = directory ) self . _best_avg_reward = - sys . float_info . max def on_play_end ( self , agent_context: core . AgentContext ): if agent_context . is_eval: tc = agent_context . train min_rewards , avg_reward , max_rewards = tc . eval_rewards [ tc . episodes_done_in_training ] if avg_reward > self . _best_avg_reward: self . _save ( agent_context = agent_context ) self . _best_avg_reward = avg_reward Ancestors (in MRO) easyagents.callbacks.save._SaveCallback easyagents.core.AgentCallback abc.ABC Methods on_api_log def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass on_gym_init_begin def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" on_gym_init_end def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass on_gym_reset_begin def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\" on_gym_reset_end def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass on_gym_step_begin def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass on_gym_step_end def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass on_log def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass on_play_begin def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.play() call (during play or eval, but not during train). \"\"\" on_play_end def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : core . AgentContext ): if agent_context . is_eval : tc = agent_context . train min_rewards , avg_reward , max_rewards = tc . eval_rewards [ tc . episodes_done_in_training ] if avg_reward > self . _best_avg_reward : self . _save ( agent_context = agent_context ) self . _best_avg_reward = avg_reward on_play_episode_begin def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\" on_play_episode_end def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : AgentContext ): \"\"\"Called once after an episode is done or stopped (during play or eval, but not during train).\"\"\" on_play_step_begin def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\" on_play_step_end def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"Called once after a step is completed in the current episode (during play or eval, but not during train).\"\"\" on_train_begin def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.train() call. \"\"\" on_train_end def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\" on_train_iteration_begin def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\" on_train_iteration_end def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : AgentContext ): \"\"\"Called once after the current iteration is completed\"\"\" Every class Every ( num_evals_between_save : int = 1 , directory : str = None ) Saves the current policy every n evaluations. Attributes: directory: the absolute path of the directory containing the persisted policies. saved_agents: list of tuples (episode, avg_rewards, directory) for each saved agent View Source class Every ( _SaveCallback ): \"\"\"Saves the current policy every n evaluations. Attributes: directory: the absolute path of the directory containing the persisted policies. saved_agents: list of tuples (episode, avg_rewards, directory) for each saved agent \"\"\" def __init__ ( self , num_evals_between_save: int = 1 , directory: str = None ): \"\"\"Saves the current policy every n evaluations. In terms of episodes: the policy is saved every num_eval_between_saves * num_iterations_between_eval * num_episodes_per_iteration Args: num_evals_between_save: the number of evaluations between saves. \"\"\" assert num_evals_between_save > 0 super (). __init__ ( directory = directory ) self . num_evals_between_save: int = num_evals_between_save def on_play_end ( self , agent_context: core . AgentContext ): if agent_context . is_eval: tc = agent_context . train if tc . num_iterations % tc . num_iterations_between_eval == 0 : evals = tc . num_iterations / tc . num_iterations_between_eval if evals % self . num_evals_between_save == 0 : self . _save ( agent_context = agent_context ) Ancestors (in MRO) easyagents.callbacks.save._SaveCallback easyagents.core.AgentCallback abc.ABC Methods on_api_log def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass on_gym_init_begin def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" on_gym_init_end def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass on_gym_reset_begin def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\" on_gym_reset_end def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass on_gym_step_begin def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass on_gym_step_end def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass on_log def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass on_play_begin def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.play() call (during play or eval, but not during train). \"\"\" on_play_end def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : core . AgentContext ): if agent_context . is_eval : tc = agent_context . train if tc . num_iterations % tc . num_iterations_between_eval == 0 : evals = tc . num_iterations / tc . num_iterations_between_eval if evals % self . num_evals_between_save == 0 : self . _save ( agent_context = agent_context ) on_play_episode_begin def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\" on_play_episode_end def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : AgentContext ): \"\"\"Called once after an episode is done or stopped (during play or eval, but not during train).\"\"\" on_play_step_begin def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\" on_play_step_end def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"Called once after a step is completed in the current episode (during play or eval, but not during train).\"\"\" on_train_begin def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.train() call. \"\"\" on_train_end def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\" on_train_iteration_begin def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\" on_train_iteration_end def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : AgentContext ): \"\"\"Called once after the current iteration is completed\"\"\"","title":"Save"},{"location":"reference/easyagents/callbacks/save/#module-easyagentscallbackssave","text":"View Source import os import sys from typing import List , Tuple import easyagents.core as core import easyagents.backends.core as bcore class _SaveCallback ( core . AgentCallback ): \"\"\"Base class for all agent savent callbacks. Attributes: directory: the absolute path of the directory containing the persisted policies. saved_agents: list of tuples (episode, avg_rewards, directory) for each saved agent \"\"\" def __init__ ( self , directory : str = None ): \"\"\"Saves the best policies (along with the agent definition) in directory. If directory is None the policies are written in a temp directory. Args: directory: the directory to save to, if None a temp directory is created. \"\"\" directory = directory if directory else bcore . _get_temp_path () self . directory : str = bcore . _mkdir ( directory ) self . saved_agents : List [ Tuple [ int , float , str ]] = [] def __str__ ( self ): return self . directory def _save ( self , agent_context : core . AgentContext ): \"\"\"Saves the current policy in directory.\"\"\" assert agent_context assert agent_context . train , \"TrainContext not set.\" tc = agent_context . train min_rewards , avg_reward , max_rewards = tc . eval_rewards [ tc . episodes_done_in_training ] current_dir = f 'episode_{tc.episodes_done_in_training}-avg_reward_{avg_reward}' current_dir = os . path . join ( self . directory , current_dir ) agent_context . _agent_saver ( directory = current_dir ) self . saved_agents . append (( tc . episodes_done_in_training , avg_reward , current_dir )) class Best ( _SaveCallback ): \"\"\"After each policy evaluation the policy is saved if average reward is larger than all previous average rewards. The policies can then be loaded using agents.load() Attributes: directory: the absolute path of the directory containing the persisted policies. saved_agents: list of tuples (episode, avg_rewards, directory) for each saved agent \"\"\" def __init__ ( self , directory : str = None ): \"\"\"Saves the best policies (along with the agent definition) in directory. If directory is None the policies are written in a temp directory. Args: directory: the directory to save to, if None a temp directory is created. \"\"\" super () . __init__ ( directory = directory ) self . _best_avg_reward = - sys . float_info . max def on_play_end ( self , agent_context : core . AgentContext ): if agent_context . is_eval : tc = agent_context . train min_rewards , avg_reward , max_rewards = tc . eval_rewards [ tc . episodes_done_in_training ] if avg_reward > self . _best_avg_reward : self . _save ( agent_context = agent_context ) self . _best_avg_reward = avg_reward class Every ( _SaveCallback ): \"\"\"Saves the current policy every n evaluations. Attributes: directory: the absolute path of the directory containing the persisted policies. saved_agents: list of tuples (episode, avg_rewards, directory) for each saved agent \"\"\" def __init__ ( self , num_evals_between_save : int = 1 , directory : str = None ): \"\"\"Saves the current policy every n evaluations. In terms of episodes: the policy is saved every num_eval_between_saves * num_iterations_between_eval * num_episodes_per_iteration Args: num_evals_between_save: the number of evaluations between saves. \"\"\" assert num_evals_between_save > 0 super () . __init__ ( directory = directory ) self . num_evals_between_save : int = num_evals_between_save def on_play_end ( self , agent_context : core . AgentContext ): if agent_context . is_eval : tc = agent_context . train if tc . num_iterations % tc . num_iterations_between_eval == 0 : evals = tc . num_iterations / tc . num_iterations_between_eval if evals % self . num_evals_between_save == 0 : self . _save ( agent_context = agent_context )","title":"Module easyagents.callbacks.save"},{"location":"reference/easyagents/callbacks/save/#classes","text":"","title":"Classes"},{"location":"reference/easyagents/callbacks/save/#best","text":"class Best ( directory : str = None ) After each policy evaluation the policy is saved if average reward is larger than all previous average rewards. The policies can then be loaded using agents.load() Attributes: directory: the absolute path of the directory containing the persisted policies. saved_agents: list of tuples (episode, avg_rewards, directory) for each saved agent View Source class Best ( _SaveCallback ): \"\"\"After each policy evaluation the policy is saved if average reward is larger than all previous average rewards. The policies can then be loaded using agents.load() Attributes: directory: the absolute path of the directory containing the persisted policies. saved_agents: list of tuples (episode, avg_rewards, directory) for each saved agent \"\"\" def __init__ ( self , directory: str = None ): \"\"\"Saves the best policies (along with the agent definition) in directory. If directory is None the policies are written in a temp directory. Args: directory: the directory to save to, if None a temp directory is created. \"\"\" super (). __init__ ( directory = directory ) self . _best_avg_reward = - sys . float_info . max def on_play_end ( self , agent_context: core . AgentContext ): if agent_context . is_eval: tc = agent_context . train min_rewards , avg_reward , max_rewards = tc . eval_rewards [ tc . episodes_done_in_training ] if avg_reward > self . _best_avg_reward: self . _save ( agent_context = agent_context ) self . _best_avg_reward = avg_reward","title":"Best"},{"location":"reference/easyagents/callbacks/save/#ancestors-in-mro","text":"easyagents.callbacks.save._SaveCallback easyagents.core.AgentCallback abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/callbacks/save/#methods","text":"","title":"Methods"},{"location":"reference/easyagents/callbacks/save/#on_api_log","text":"def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass","title":"on_api_log"},{"location":"reference/easyagents/callbacks/save/#on_gym_init_begin","text":"def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\"","title":"on_gym_init_begin"},{"location":"reference/easyagents/callbacks/save/#on_gym_init_end","text":"def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass","title":"on_gym_init_end"},{"location":"reference/easyagents/callbacks/save/#on_gym_reset_begin","text":"def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\"","title":"on_gym_reset_begin"},{"location":"reference/easyagents/callbacks/save/#on_gym_reset_end","text":"def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass","title":"on_gym_reset_end"},{"location":"reference/easyagents/callbacks/save/#on_gym_step_begin","text":"def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass","title":"on_gym_step_begin"},{"location":"reference/easyagents/callbacks/save/#on_gym_step_end","text":"def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass","title":"on_gym_step_end"},{"location":"reference/easyagents/callbacks/save/#on_log","text":"def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass","title":"on_log"},{"location":"reference/easyagents/callbacks/save/#on_play_begin","text":"def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.play() call (during play or eval, but not during train). \"\"\"","title":"on_play_begin"},{"location":"reference/easyagents/callbacks/save/#on_play_end","text":"def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : core . AgentContext ): if agent_context . is_eval : tc = agent_context . train min_rewards , avg_reward , max_rewards = tc . eval_rewards [ tc . episodes_done_in_training ] if avg_reward > self . _best_avg_reward : self . _save ( agent_context = agent_context ) self . _best_avg_reward = avg_reward","title":"on_play_end"},{"location":"reference/easyagents/callbacks/save/#on_play_episode_begin","text":"def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\"","title":"on_play_episode_begin"},{"location":"reference/easyagents/callbacks/save/#on_play_episode_end","text":"def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : AgentContext ): \"\"\"Called once after an episode is done or stopped (during play or eval, but not during train).\"\"\"","title":"on_play_episode_end"},{"location":"reference/easyagents/callbacks/save/#on_play_step_begin","text":"def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\"","title":"on_play_step_begin"},{"location":"reference/easyagents/callbacks/save/#on_play_step_end","text":"def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"Called once after a step is completed in the current episode (during play or eval, but not during train).\"\"\"","title":"on_play_step_end"},{"location":"reference/easyagents/callbacks/save/#on_train_begin","text":"def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.train() call. \"\"\"","title":"on_train_begin"},{"location":"reference/easyagents/callbacks/save/#on_train_end","text":"def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\"","title":"on_train_end"},{"location":"reference/easyagents/callbacks/save/#on_train_iteration_begin","text":"def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\"","title":"on_train_iteration_begin"},{"location":"reference/easyagents/callbacks/save/#on_train_iteration_end","text":"def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : AgentContext ): \"\"\"Called once after the current iteration is completed\"\"\"","title":"on_train_iteration_end"},{"location":"reference/easyagents/callbacks/save/#every","text":"class Every ( num_evals_between_save : int = 1 , directory : str = None ) Saves the current policy every n evaluations. Attributes: directory: the absolute path of the directory containing the persisted policies. saved_agents: list of tuples (episode, avg_rewards, directory) for each saved agent View Source class Every ( _SaveCallback ): \"\"\"Saves the current policy every n evaluations. Attributes: directory: the absolute path of the directory containing the persisted policies. saved_agents: list of tuples (episode, avg_rewards, directory) for each saved agent \"\"\" def __init__ ( self , num_evals_between_save: int = 1 , directory: str = None ): \"\"\"Saves the current policy every n evaluations. In terms of episodes: the policy is saved every num_eval_between_saves * num_iterations_between_eval * num_episodes_per_iteration Args: num_evals_between_save: the number of evaluations between saves. \"\"\" assert num_evals_between_save > 0 super (). __init__ ( directory = directory ) self . num_evals_between_save: int = num_evals_between_save def on_play_end ( self , agent_context: core . AgentContext ): if agent_context . is_eval: tc = agent_context . train if tc . num_iterations % tc . num_iterations_between_eval == 0 : evals = tc . num_iterations / tc . num_iterations_between_eval if evals % self . num_evals_between_save == 0 : self . _save ( agent_context = agent_context )","title":"Every"},{"location":"reference/easyagents/callbacks/save/#ancestors-in-mro_1","text":"easyagents.callbacks.save._SaveCallback easyagents.core.AgentCallback abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/easyagents/callbacks/save/#methods_1","text":"","title":"Methods"},{"location":"reference/easyagents/callbacks/save/#on_api_log_1","text":"def on_api_log ( self , agent_context : easyagents . core . AgentContext , api_target : str , log_msg : str ) Logs a call to the api of the agents implementation library / framework. View Source def on_api_log ( self , agent_context : AgentContext , api_target : str , log_msg : str ): \"\"\"Logs a call to the api of the agents implementation library / framework.\"\"\" pass","title":"on_api_log"},{"location":"reference/easyagents/callbacks/save/#on_gym_init_begin_1","text":"def on_gym_init_begin ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_begin ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment begins the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\"","title":"on_gym_init_begin"},{"location":"reference/easyagents/callbacks/save/#on_gym_init_end_1","text":"def on_gym_init_end ( self , agent_context : easyagents . core . AgentContext ) called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent View Source def on_gym_init_end ( self , agent_context : AgentContext ): \"\"\"called when the monitored environment completed the instantiation of a new gym environment. Args: agent_context: api_context passed to calling agent \"\"\" pass","title":"on_gym_init_end"},{"location":"reference/easyagents/callbacks/save/#on_gym_reset_begin_1","text":"def on_gym_reset_begin ( self , agent_context : easyagents . core . AgentContext , ** kwargs ) Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment View Source def on_gym_reset_begin ( self , agent_context : AgentContext , ** kwargs ): \"\"\"Before a call to gym.reset Args: agent_context: api_context passed to calling agent kwargs: the args to be passed to the underlying environment \"\"\"","title":"on_gym_reset_begin"},{"location":"reference/easyagents/callbacks/save/#on_gym_reset_end_1","text":"def on_gym_reset_end ( self , agent_context : easyagents . core . AgentContext , reset_result : Tuple , ** kwargs ) After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset View Source def on_gym_reset_end ( self , agent_context : AgentContext , reset_result : Tuple , ** kwargs ): \"\"\"After a call to gym.reset was completed Args: agent_context: api_context passed to calling agent reset_result: object returned by gym.reset kwargs: args passed to gym.reset \"\"\" pass","title":"on_gym_reset_end"},{"location":"reference/easyagents/callbacks/save/#on_gym_step_begin_1","text":"def on_gym_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment View Source def on_gym_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Before a call to gym.step Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment \"\"\" pass","title":"on_gym_step_begin"},{"location":"reference/easyagents/callbacks/save/#on_gym_step_end_1","text":"def on_gym_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step View Source def on_gym_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"After a call to gym.step was completed Args: agent_context: api_context passed to calling agent action: the action to be passed to the underlying environment step_result: (observation,reward,done,info) tuple returned by gym.step \"\"\" pass","title":"on_gym_step_end"},{"location":"reference/easyagents/callbacks/save/#on_log_1","text":"def on_log ( self , agent_context : easyagents . core . AgentContext , log_msg : str ) Logs a general message View Source def on_log ( self , agent_context : AgentContext , log_msg : str ): \"\"\"Logs a general message\"\"\" pass","title":"on_log"},{"location":"reference/easyagents/callbacks/save/#on_play_begin_1","text":"def on_play_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.play() call (during play or eval, but not during train). View Source def on_play_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.play() call (during play or eval, but not during train). \"\"\"","title":"on_play_begin"},{"location":"reference/easyagents/callbacks/save/#on_play_end_1","text":"def on_play_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.play() call (during play or eval, but not during train) View Source def on_play_end ( self , agent_context : core . AgentContext ): if agent_context . is_eval : tc = agent_context . train if tc . num_iterations % tc . num_iterations_between_eval == 0 : evals = tc . num_iterations / tc . num_iterations_between_eval if evals % self . num_evals_between_save == 0 : self . _save ( agent_context = agent_context )","title":"on_play_end"},{"location":"reference/easyagents/callbacks/save/#on_play_episode_begin_1","text":"def on_play_episode_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of new episode to be played (during play or eval, but not during train). View Source def on_play_episode_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of new episode to be played (during play or eval, but not during train). \"\"\"","title":"on_play_episode_begin"},{"location":"reference/easyagents/callbacks/save/#on_play_episode_end_1","text":"def on_play_episode_end ( self , agent_context : easyagents . core . AgentContext ) Called once after an episode is done or stopped (during play or eval, but not during train). View Source def on_play_episode_end ( self , agent_context : AgentContext ): \"\"\"Called once after an episode is done or stopped (during play or eval, but not during train).\"\"\"","title":"on_play_episode_end"},{"location":"reference/easyagents/callbacks/save/#on_play_step_begin_1","text":"def on_play_step_begin ( self , agent_context : easyagents . core . AgentContext , action ) Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call View Source def on_play_step_begin ( self , agent_context : AgentContext , action ): \"\"\"Called once before a new step is taken in the current episode (during play or eval, but not during train). Args: agent_context: the context describing the agents current configuration action: the action to be passed to the upcoming gym_env.step call \"\"\"","title":"on_play_step_begin"},{"location":"reference/easyagents/callbacks/save/#on_play_step_end_1","text":"def on_play_step_end ( self , agent_context : easyagents . core . AgentContext , action , step_result : Tuple ) Called once after a step is completed in the current episode (during play or eval, but not during train). View Source def on_play_step_end ( self , agent_context : AgentContext , action , step_result : Tuple ): \"\"\"Called once after a step is completed in the current episode (during play or eval, but not during train).\"\"\"","title":"on_play_step_end"},{"location":"reference/easyagents/callbacks/save/#on_train_begin_1","text":"def on_train_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the entry of an agent.train() call. View Source def on_train_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the entry of an agent.train() call. \"\"\"","title":"on_train_begin"},{"location":"reference/easyagents/callbacks/save/#on_train_end_1","text":"def on_train_end ( self , agent_context : easyagents . core . AgentContext ) Called once before exiting an agent.train() call View Source def on_train_end ( self , agent_context : AgentContext ): \"\"\"Called once before exiting an agent.train() call\"\"\"","title":"on_train_end"},{"location":"reference/easyagents/callbacks/save/#on_train_iteration_begin_1","text":"def on_train_iteration_begin ( self , agent_context : easyagents . core . AgentContext ) Called once at the start of a new iteration. View Source def on_train_iteration_begin ( self , agent_context : AgentContext ): \"\"\"Called once at the start of a new iteration. \"\"\"","title":"on_train_iteration_begin"},{"location":"reference/easyagents/callbacks/save/#on_train_iteration_end_1","text":"def on_train_iteration_end ( self , agent_context : easyagents . core . AgentContext ) Called once after the current iteration is completed View Source def on_train_iteration_end ( self , agent_context : AgentContext ): \"\"\"Called once after the current iteration is completed\"\"\"","title":"on_train_iteration_end"}]}